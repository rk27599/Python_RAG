# PyTorch documentation (Part 1)

PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.

Features described in this documentation are classified by release status:

Stable (API-Stable):
These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).

Unstable (API-Unstable):
Encompasses all features that are under active development where APIs may change based on user feedback, requisite performance improvements or because coverage across operators is not yet complete.
The APIs and performance characteristics of these features may change.

List:
Python API


torch.nn.functional
torch.Tensor
Tensor Attributes
Tensor Views

torch.autograd
torch.library
torch.accelerator

torch.cuda
torch.cuda.memory

================================================================================

# PyTorch documentation (Part 2)


torch.mtia
torch.mtia.memory
Meta device
torch.backends
torch.export
torch.distributed
torch.distributed.tensor
torch.distributed.algorithms.join
torch.distributed.elastic
torch.distributed.fsdp
torch.distributed.fsdp.fully_shard
torch.distributed.tensor.parallel
torch.distributed.optim
torch.distributed.pipelining
torch.distributed.checkpoint
torch.distributions
torch.compiler

torch.func
torch.futures

torch.fx.experimental


torch.linalg
torch.monitor
torch.signal
torch.special
torch.overrides
torch.package
torch.profiler
torch.nn.init
torch.nn.attention
torch.onnx
torch.optim
Complex Numbers
DDP Communication Hooks
Quantization
Distributed RPC Framework
torch.random
torch.masked
torch.nested
torch.Size
torch.sparse
torch.Storage
torch.testing
torch.utils
torch.utils.benchmark
torch.utils.bottleneck
torch.utils.checkpoint
torch.utils.cpp_extension
torch.utils.data
torch.utils.deterministic
torch.utils.jit
torch.utils.dlpack
torch.utils.mobile_optimizer
torch.utils.model_zoo
torch.utils.tensorboard
torch.utils.module_tracker

Named Tensors
Named Tensors operator coverage
torch.__config__
torch.__future__
torch._logging
Torch Environment Variables

================================================================================

# PyTorch documentation (Part 3)


Developer Notes
Automatic Mixed Precision examples
Autograd mechanics
Broadcasting semantics
CPU threading and TorchScript inference
CUDA semantics
PyTorch Custom Operators Landing Page
Distributed Data Parallel
Extending PyTorch
Extending torch.func with autograd.Function
Frequently Asked Questions
FSDP Notes
Getting Started on Intel GPU
Gradcheck mechanics
HIP (ROCm) semantics
Features for large-scale deployments
LibTorch Stable ABI

MPS backend
Multiprocessing best practices
Numerical accuracy

Reproducibility
Serialization semantics
Windows FAQ

List:
torch.nn.functional
torch.Tensor
Tensor Attributes
Tensor Views

torch.autograd
torch.library
torch.accelerator

torch.cuda
torch.cuda.memory


torch.mtia
torch.mtia.memory
Meta device
torch.backends
torch.export
torch.distributed
torch.distributed.tensor
torch.distributed.algorithms.join
torch.distributed.elastic
torch.distributed.fsdp
torch.distributed.fsdp.fully_shard
torch.distributed.tensor.parallel
torch.distributed.optim
torch.distributed.pipelining
torch.distributed.checkpoint
torch.distributions
torch.compiler

torch.func
torch.futures

torch.fx.experimental

================================================================================

# PyTorch documentation (Part 4)


torch.linalg
torch.monitor
torch.signal
torch.special
torch.overrides
torch.package
torch.profiler
torch.nn.init
torch.nn.attention
torch.onnx
torch.optim
Complex Numbers
DDP Communication Hooks
Quantization
Distributed RPC Framework
torch.random
torch.masked
torch.nested
torch.Size
torch.sparse
torch.Storage
torch.testing
torch.utils
torch.utils.benchmark
torch.utils.bottleneck
torch.utils.checkpoint
torch.utils.cpp_extension
torch.utils.data
torch.utils.deterministic
torch.utils.jit
torch.utils.dlpack
torch.utils.mobile_optimizer
torch.utils.model_zoo
torch.utils.tensorboard
torch.utils.module_tracker

Named Tensors
Named Tensors operator coverage
torch.__config__
torch.__future__
torch._logging
Torch Environment Variables

List:
Automatic Mixed Precision examples
Autograd mechanics
Broadcasting semantics
CPU threading and TorchScript inference
CUDA semantics
PyTorch Custom Operators Landing Page
Distributed Data Parallel
Extending PyTorch
Extending torch.func with autograd.Function
Frequently Asked Questions
FSDP Notes
Getting Started on Intel GPU
Gradcheck mechanics
HIP (ROCm) semantics
Features for large-scale deployments
LibTorch Stable ABI

================================================================================

# PyTorch documentation (Part 5)

MPS backend
Multiprocessing best practices
Numerical accuracy

Reproducibility
Serialization semantics
Windows FAQ

================================================================================

# PyTorch documentation - Indices and tables

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Get Started

Choose Your Path: Install PyTorch Locally or Launch Instantly on Supported Cloud Platforms

================================================================================

# Get Started - Join PyTorch Foundation

As a member of the PyTorch Foundation, you’ll have access to resources that allow you to be stewards of stable, secure, and long-lasting codebases. You can collaborate on training, local and regional events, open-source developer tooling, academic research, and guides to help new users and contributors have a productive experience.

================================================================================

# Get Started - Production Ready

Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe.

================================================================================

# Get Started - Distributed Training

Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend.

================================================================================

# Get Started - Robust Ecosystem

A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more.

================================================================================

# Get Started - Cloud Support

PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling.

================================================================================

# Get Started - Install PyTorch

Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users. Preview is available if you want the latest, not fully tested and supported, builds that are generated nightly. Please ensure that you have met the prerequisites below (e.g., numpy), depending on your package manager. Anaconda is our recommended package manager since it installs all dependencies. You can also install previous versions of PyTorch. Note that LibTorch is only available for C++.

Latest Stable PyTorch requires Python 3.9 or later. Latest Preview (Nightly) PyTorch  requires Python 3.10 or later.

Code example:
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

================================================================================

# Get Started - Quick Start With Cloud Partners

Get up and running with PyTorch quickly through popular cloud platforms and machine learning services.

================================================================================

# Get Started - Amazon Web Services

List:
PyTorch on AWS
Amazon SageMaker
AWS Deep Learning Containers
AWS Deep Learning AMIs

================================================================================

# Get Started - Google Cloud Platform

List:
Cloud Deep Learning VM Image
Deep Learning Containers

================================================================================

# Get Started - Microsoft Azure

List:
PyTorch on Azure
Azure Machine Learning
Azure Functions

================================================================================

# Get Started - Featured Projects

Explore a rich ecosystem of libraries, tools, and more to support development.

================================================================================

# Get Started - Captum

Captum (“comprehension” in Latin) is an open source, extensible library for model interpretability built on PyTorch.

================================================================================

# Get Started - PyTorch Geometric

PyTorch Geometric is a library for deep learning on irregular input data such as graphs, point clouds, and manifolds.

================================================================================

# Get Started - skorch

skorch is a high-level library for PyTorch that provides full scikit-learn compatibility.

================================================================================

# Get Started - Amazon Advertising

Reduce inference costs by 71% and scale out using PyTorch, TorchServe, and AWS Inferentia.

================================================================================

# Get Started - Salesforce

Pushing the state of the art in NLP and Multi-task learning.

================================================================================

# Get Started - Stanford University

Using PyTorch’s flexibility to efficiently research new algorithmic approaches.

================================================================================

# Welcome to PyTorch Tutorials

What’s new in PyTorch tutorials?

List:
Integrating Custom Operators with SYCL for Intel GPU
Supporting Custom C++ Classes in torch.compile/torch.export
Accelerating torch.save and torch.load with GPUDirect Storage
Getting Started with Fully Sharded Data Parallel (FSDP2)

Integrating Custom Operators with SYCL for Intel GPU

Supporting Custom C++ Classes in torch.compile/torch.export

Accelerating torch.save and torch.load with GPUDirect Storage

Getting Started with Fully Sharded Data Parallel (FSDP2)

Learn the Basics
Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.
Get started with PyTorch

================================================================================

# Welcome to PyTorch Tutorials - Learn the Basics

Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.

PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples.
Explore Recipes

================================================================================

# Welcome to PyTorch Tutorials - PyTorch Recipes

Bite-size, ready-to-deploy PyTorch code examples.

Learn the Basics

A step-by-step guide to building a complete ML workflow with PyTorch.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Learn the Basics

A step-by-step guide to building a complete ML workflow with PyTorch.

Introduction to PyTorch on YouTube

An introduction to building a complete ML workflow with PyTorch. Follows the PyTorch Beginner Series on YouTube.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Introduction to PyTorch on YouTube

An introduction to building a complete ML workflow with PyTorch. Follows the PyTorch Beginner Series on YouTube.

Learning PyTorch with Examples

This tutorial introduces the fundamental concepts of PyTorch through self-contained examples.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Learning PyTorch with Examples

This tutorial introduces the fundamental concepts of PyTorch through self-contained examples.

What is torch.nn really?

Use torch.nn to create and train a neural network.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - What is torch.nn really?

Use torch.nn to create and train a neural network.

Visualizing Models, Data, and Training with TensorBoard

Learn to use TensorBoard to visualize data and model training.
Interpretability,Getting-Started,TensorBoard

================================================================================

# Welcome to PyTorch Tutorials - Visualizing Models, Data, and Training with TensorBoard

Learn to use TensorBoard to visualize data and model training.

Interpretability,Getting-Started,TensorBoard

Good usage of `non_blocking` and `pin_memory()` in PyTorch

A guide on best practices to copy data from CPU to GPU.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Good usage of `non_blocking` and `pin_memory()` in PyTorch

A guide on best practices to copy data from CPU to GPU.

Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors

Learn the subtleties of requires_grad, retain_grad, leaf, and non-leaf tensors
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors

Learn the subtleties of requires_grad, retain_grad, leaf, and non-leaf tensors

Visualizing Gradients in PyTorch

Visualize the gradient flow of a network.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Visualizing Gradients in PyTorch

Visualize the gradient flow of a network.

TorchVision Object Detection Finetuning Tutorial

Finetune a pre-trained Mask R-CNN model.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - TorchVision Object Detection Finetuning Tutorial

Finetune a pre-trained Mask R-CNN model.

Transfer Learning for Computer Vision Tutorial

Train a convolutional neural network for image classification using transfer learning.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Transfer Learning for Computer Vision Tutorial

Train a convolutional neural network for image classification using transfer learning.

Adversarial Example Generation

Train a convolutional neural network for image classification using transfer learning.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Adversarial Example Generation

Train a convolutional neural network for image classification using transfer learning.

DCGAN Tutorial

Train a generative adversarial network (GAN) to generate new celebrities.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - DCGAN Tutorial

Train a generative adversarial network (GAN) to generate new celebrities.

Spatial Transformer Networks Tutorial

Learn how to augment your network using a visual attention mechanism.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Spatial Transformer Networks Tutorial

Learn how to augment your network using a visual attention mechanism.

Inference on Whole Slide Images with TIAToolbox

Learn how to use the TIAToolbox to perform inference on whole slide images.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Inference on Whole Slide Images with TIAToolbox

Learn how to use the TIAToolbox to perform inference on whole slide images.

Semi-Supervised Learning Tutorial Based on USB

Learn how to train semi-supervised learning algorithms (on custom data) using USB and PyTorch.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Semi-Supervised Learning Tutorial Based on USB

Learn how to train semi-supervised learning algorithms (on custom data) using USB and PyTorch.

Audio IO

Learn to load data with torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio IO

Learn to load data with torchaudio.

Audio Resampling

Learn to resample audio waveforms using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Resampling

Learn to resample audio waveforms using torchaudio.

Audio Data Augmentation

Learn to apply data augmentations using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Data Augmentation

Learn to apply data augmentations using torchaudio.

Audio Feature Extractions

Learn to extract features using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Feature Extractions

Learn to extract features using torchaudio.

Audio Feature Augmentation

Learn to augment features using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Feature Augmentation

Learn to augment features using torchaudio.

Audio Datasets

Learn to use torchaudio datasets.

================================================================================

# Welcome to PyTorch Tutorials - Audio Datasets

Learn to use torchaudio datasets.

Automatic Speech Recognition with Wav2Vec2 in torchaudio

Learn how to use torchaudio's pretrained models for building a speech recognition application.

================================================================================

# Welcome to PyTorch Tutorials - Automatic Speech Recognition with Wav2Vec2 in torchaudio

Learn how to use torchaudio's pretrained models for building a speech recognition application.

Speech Command Classification

Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset.

================================================================================

# Welcome to PyTorch Tutorials - Speech Command Classification

Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset.

Text-to-Speech with torchaudio

Learn how to use torchaudio's pretrained models for building a text-to-speech application.

================================================================================

# Welcome to PyTorch Tutorials - Text-to-Speech with torchaudio

Learn how to use torchaudio's pretrained models for building a text-to-speech application.

Forced Alignment with Wav2Vec2 in torchaudio

Learn how to use torchaudio's Wav2Vec2 pretrained models for aligning text to speech

================================================================================

# Welcome to PyTorch Tutorials - Forced Alignment with Wav2Vec2 in torchaudio

Learn how to use torchaudio's Wav2Vec2 pretrained models for aligning text to speech

NLP from Scratch: Classifying Names with a Character-level RNN

Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials.

================================================================================

# Welcome to PyTorch Tutorials - NLP from Scratch: Classifying Names with a Character-level RNN

Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials.

NLP from Scratch: Generating Names with a Character-level RNN

After using character-level RNN to classify names, learn how to generate names from languages. Second in a series of three tutorials.

================================================================================

# Welcome to PyTorch Tutorials - NLP from Scratch: Generating Names with a Character-level RNN

After using character-level RNN to classify names, learn how to generate names from languages. Second in a series of three tutorials.

NLP from Scratch: Translation with a Sequence-to-sequence Network and Attention

This is the third and final tutorial on doing “NLP From Scratch”, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks.

================================================================================

# Welcome to PyTorch Tutorials - NLP from Scratch: Translation with a Sequence-to-sequence Network and Attention

This is the third and final tutorial on doing “NLP From Scratch”, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks.

Exporting a PyTorch model to ONNX using TorchDynamo backend and Running it using ONNX Runtime

Build a image classifier model in PyTorch and convert it to ONNX before deploying it with ONNX Runtime.
Production,ONNX,Backends

================================================================================

# Welcome to PyTorch Tutorials - Exporting a PyTorch model to ONNX using TorchDynamo backend and Running it using ONNX Runtime

Build a image classifier model in PyTorch and convert it to ONNX before deploying it with ONNX Runtime.

Production,ONNX,Backends

Extending the ONNX exporter operator support

Demonstrate end-to-end how to address unsupported operators in ONNX.
Production,ONNX,Backends

================================================================================

# Welcome to PyTorch Tutorials - Extending the ONNX exporter operator support

Demonstrate end-to-end how to address unsupported operators in ONNX.

Production,ONNX,Backends

Exporting a model with control flow to ONNX

Demonstrate how to handle control flow logic while exporting a PyTorch model to ONNX.
Production,ONNX,Backends

================================================================================

# Welcome to PyTorch Tutorials - Exporting a model with control flow to ONNX

Demonstrate how to handle control flow logic while exporting a PyTorch model to ONNX.

Production,ONNX,Backends

Reinforcement Learning (DQN)

Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym.
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Reinforcement Learning (DQN)

Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym.

Reinforcement-Learning

Reinforcement Learning (PPO) with TorchRL

Learn how to use PyTorch and TorchRL to train a Proximal Policy Optimization agent on the Inverted Pendulum task from Gym.
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Reinforcement Learning (PPO) with TorchRL

Learn how to use PyTorch and TorchRL to train a Proximal Policy Optimization agent on the Inverted Pendulum task from Gym.

Reinforcement-Learning

Train a Mario-playing RL Agent

Use PyTorch to train a Double Q-learning agent to play Mario.
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Train a Mario-playing RL Agent

Use PyTorch to train a Double Q-learning agent to play Mario.

Reinforcement-Learning

Recurrent DQN

Use TorchRL to train recurrent policies
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Recurrent DQN

Use TorchRL to train recurrent policies

Reinforcement-Learning

Code a DDPG Loss

Use TorchRL to code a DDPG Loss
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Code a DDPG Loss

Use TorchRL to code a DDPG Loss

Reinforcement-Learning

Writing your environment and transforms

Use TorchRL to code a Pendulum
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Writing your environment and transforms

Use TorchRL to code a Pendulum

Reinforcement-Learning

Profiling PyTorch

Learn how to profile a PyTorch application

================================================================================

# Welcome to PyTorch Tutorials - Profiling PyTorch

Learn how to profile a PyTorch application

Profiling PyTorch

Introduction to Holistic Trace Analysis

_static/img/thumbnails/default.png

================================================================================

# Welcome to PyTorch Tutorials - Profiling PyTorch

Introduction to Holistic Trace Analysis

Profiling PyTorch

Trace Diff using Holistic Trace Analysis

_static/img/thumbnails/default.png

================================================================================

# Welcome to PyTorch Tutorials - Profiling PyTorch

Trace Diff using Holistic Trace Analysis

Building a Simple Performance Profiler with FX

Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics

================================================================================

# Welcome to PyTorch Tutorials - Building a Simple Performance Profiler with FX

Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics

(beta) Channels Last Memory Format in PyTorch

Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions.
Memory-Format,Best-Practice,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - (beta) Channels Last Memory Format in PyTorch

Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions.

Memory-Format,Best-Practice,Frontend-APIs

Using the PyTorch C++ Frontend

Walk through an end-to-end example of training a model with the C++ frontend by training a DCGAN – a kind of generative model – to generate images of MNIST digits.
Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Using the PyTorch C++ Frontend

Walk through an end-to-end example of training a model with the C++ frontend by training a DCGAN – a kind of generative model – to generate images of MNIST digits.

PyTorch Custom Operators Landing Page

This is the landing page for all things related to custom operators in PyTorch.
Extending-PyTorch,Frontend-APIs,C++,CUDA

================================================================================

# Welcome to PyTorch Tutorials - PyTorch Custom Operators Landing Page

This is the landing page for all things related to custom operators in PyTorch.

Extending-PyTorch,Frontend-APIs,C++,CUDA

Custom Python Operators

Create Custom Operators in Python. Useful for black-boxing a Python function for use with torch.compile.
Extending-PyTorch,Frontend-APIs,C++,CUDA

================================================================================

# Welcome to PyTorch Tutorials - Custom Python Operators

Create Custom Operators in Python. Useful for black-boxing a Python function for use with torch.compile.

Extending-PyTorch,Frontend-APIs,C++,CUDA

Compiled Autograd: Capturing a larger backward graph for ``torch.compile``

Learn how to use compiled autograd to capture a larger backward graph.
Model-Optimization,CUDA

================================================================================

# Welcome to PyTorch Tutorials - Compiled Autograd: Capturing a larger backward graph for ``torch.compile``

Learn how to use compiled autograd to capture a larger backward graph.

Model-Optimization,CUDA

Custom C++ and CUDA Operators

How to extend PyTorch with custom C++ and CUDA operators.
Extending-PyTorch,Frontend-APIs,C++,CUDA

================================================================================

# Welcome to PyTorch Tutorials - Custom C++ and CUDA Operators

How to extend PyTorch with custom C++ and CUDA operators.

Extending-PyTorch,Frontend-APIs,C++,CUDA

Autograd in C++ Frontend

The autograd package helps build flexible and dynamic neural netorks. In this tutorial, explore several examples of doing autograd in PyTorch C++ frontend
Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Autograd in C++ Frontend

The autograd package helps build flexible and dynamic neural netorks. In this tutorial, explore several examples of doing autograd in PyTorch C++ frontend

Registering a Dispatched Operator in C++

The dispatcher is an internal component of PyTorch which is responsible for figuring out what code should actually get run when you call a function like torch::add.
Extending-PyTorch,Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Registering a Dispatched Operator in C++

The dispatcher is an internal component of PyTorch which is responsible for figuring out what code should actually get run when you call a function like torch::add.

Extending-PyTorch,Frontend-APIs,C++

Extending Dispatcher For a New Backend in C++

Learn how to extend the dispatcher to add a new device living outside of the pytorch/pytorch repo and maintain it to keep in sync with native PyTorch devices.
Extending-PyTorch,Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Extending Dispatcher For a New Backend in C++

Learn how to extend the dispatcher to add a new device living outside of the pytorch/pytorch repo and maintain it to keep in sync with native PyTorch devices.

Extending-PyTorch,Frontend-APIs,C++

Facilitating New Backend Integration by PrivateUse1

Learn how to integrate a new backend living outside of the pytorch/pytorch repo and maintain it to keep in sync with the native PyTorch backend.
Extending-PyTorch,Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Facilitating New Backend Integration by PrivateUse1

Learn how to integrate a new backend living outside of the pytorch/pytorch repo and maintain it to keep in sync with the native PyTorch backend.

Extending-PyTorch,Frontend-APIs,C++

Custom Function Tutorial: Double Backward

Learn how to write a custom autograd Function that supports double backward.
Extending-PyTorch,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Custom Function Tutorial: Double Backward

Learn how to write a custom autograd Function that supports double backward.

Extending-PyTorch,Frontend-APIs

Custom Function Tutorial: Fusing Convolution and Batch Norm

Learn how to create a custom autograd Function that fuses batch norm into a convolution to improve memory usage.
Extending-PyTorch,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Custom Function Tutorial: Fusing Convolution and Batch Norm

Learn how to create a custom autograd Function that fuses batch norm into a convolution to improve memory usage.

Extending-PyTorch,Frontend-APIs

Forward-mode Automatic Differentiation

Learn how to use forward-mode automatic differentiation.
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Forward-mode Automatic Differentiation

Learn how to use forward-mode automatic differentiation.

Jacobians, Hessians, hvp, vhp, and more

Learn how to compute advanced autodiff quantities using torch.func
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Jacobians, Hessians, hvp, vhp, and more

Learn how to compute advanced autodiff quantities using torch.func

Model Ensembling

Learn how to ensemble models using torch.vmap
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Model Ensembling

Learn how to ensemble models using torch.vmap

Per-Sample-Gradients

Learn how to compute per-sample-gradients using torch.func
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Per-Sample-Gradients

Learn how to compute per-sample-gradients using torch.func

Neural Tangent Kernels

Learn how to compute neural tangent kernels using torch.func
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Neural Tangent Kernels

Learn how to compute neural tangent kernels using torch.func

Performance Profiling in PyTorch

Learn how to use the PyTorch Profiler to benchmark your module's performance.
Model-Optimization,Best-Practice,Profiling

================================================================================

# Welcome to PyTorch Tutorials - Performance Profiling in PyTorch

Learn how to use the PyTorch Profiler to benchmark your module's performance.

Model-Optimization,Best-Practice,Profiling

Performance Profiling in TensorBoard

Learn how to use the TensorBoard plugin to profile and analyze your model's performance.
Model-Optimization,Best-Practice,Profiling,TensorBoard

================================================================================

# Welcome to PyTorch Tutorials - Performance Profiling in TensorBoard

Learn how to use the TensorBoard plugin to profile and analyze your model's performance.

Model-Optimization,Best-Practice,Profiling,TensorBoard

Hyperparameter Tuning Tutorial

Learn how to use Ray Tune to find the best performing set of hyperparameters for your model.
Model-Optimization,Best-Practice

================================================================================

# Welcome to PyTorch Tutorials - Hyperparameter Tuning Tutorial

Learn how to use Ray Tune to find the best performing set of hyperparameters for your model.

Model-Optimization,Best-Practice

Parametrizations Tutorial

Learn how to use torch.nn.utils.parametrize to put constraints on your parameters (e.g. make them orthogonal, symmetric positive definite, low-rank...)
Model-Optimization,Best-Practice

================================================================================

# Welcome to PyTorch Tutorials - Parametrizations Tutorial

Learn how to use torch.nn.utils.parametrize to put constraints on your parameters (e.g. make them orthogonal, symmetric positive definite, low-rank...)

Model-Optimization,Best-Practice

Pruning Tutorial

Learn how to use torch.nn.utils.prune to sparsify your neural networks, and how to extend it to implement your own custom pruning technique.
Model-Optimization,Best-Practice

================================================================================

# Welcome to PyTorch Tutorials - Pruning Tutorial

Learn how to use torch.nn.utils.prune to sparsify your neural networks, and how to extend it to implement your own custom pruning technique.

Model-Optimization,Best-Practice

How to save memory by fusing the optimizer step into the backward pass

Learn a memory-saving technique through fusing the optimizer step into the backward pass using memory snapshots.
Model-Optimization,Best-Practice,CUDA,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - How to save memory by fusing the optimizer step into the backward pass

Learn a memory-saving technique through fusing the optimizer step into the backward pass using memory snapshots.

Model-Optimization,Best-Practice,CUDA,Frontend-APIs

(beta) Accelerating BERT with semi-structured sparsity

Train BERT, prune it to be 2:4 sparse, and then accelerate it to achieve 2x inference speedups with semi-structured sparsity and torch.compile.
Text,Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - (beta) Accelerating BERT with semi-structured sparsity

Train BERT, prune it to be 2:4 sparse, and then accelerate it to achieve 2x inference speedups with semi-structured sparsity and torch.compile.

Text,Model-Optimization

Multi-Objective Neural Architecture Search with Ax

Learn how to use Ax to search over architectures find optimal tradeoffs between accuracy and latency.
Model-Optimization,Best-Practice,Ax,TorchX

================================================================================

# Welcome to PyTorch Tutorials - Multi-Objective Neural Architecture Search with Ax

Learn how to use Ax to search over architectures find optimal tradeoffs between accuracy and latency.

Model-Optimization,Best-Practice,Ax,TorchX

torch.compile Tutorial

Speed up your models with minimal code changes using torch.compile, the latest PyTorch compiler solution.
Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - torch.compile Tutorial

Speed up your models with minimal code changes using torch.compile, the latest PyTorch compiler solution.

Building a Convolution/Batch Norm fuser in torch.compile

Build a simple pattern matcher pass that fuses batch norm into convolution to improve performance during inference.
Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - Building a Convolution/Batch Norm fuser in torch.compile

Build a simple pattern matcher pass that fuses batch norm into convolution to improve performance during inference.

Inductor CPU Backend Debugging and Profiling

Learn the usage, debugging and performance profiling for ``torch.compile`` with Inductor CPU backend.
Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - Inductor CPU Backend Debugging and Profiling

Learn the usage, debugging and performance profiling for ``torch.compile`` with Inductor CPU backend.

(beta) Implementing High-Performance Transformers with SCALED DOT PRODUCT ATTENTION

This tutorial explores the new torch.nn.functional.scaled_dot_product_attention and how it can be used to construct Transformer components.
Model-Optimization,Attention,Transformer

================================================================================

# Welcome to PyTorch Tutorials - (beta) Implementing High-Performance Transformers with SCALED DOT PRODUCT ATTENTION

This tutorial explores the new torch.nn.functional.scaled_dot_product_attention and how it can be used to construct Transformer components.

Model-Optimization,Attention,Transformer

Knowledge Distillation in Convolutional Neural Networks

Learn how to improve the accuracy of lightweight models using more powerful models as teachers.
Model-Optimization,Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Knowledge Distillation in Convolutional Neural Networks

Learn how to improve the accuracy of lightweight models using more powerful models as teachers.

Model-Optimization,Image/Video

Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()

This tutorial goes over recommended best practices for implementing Transformers with native PyTorch.
Transformer

================================================================================

# Welcome to PyTorch Tutorials - Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()

This tutorial goes over recommended best practices for implementing Transformers with native PyTorch.

PyTorch Distributed Overview

Briefly go over all concepts and features in the distributed package. Use this document to find the distributed training technology that can best serve your application.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - PyTorch Distributed Overview

Briefly go over all concepts and features in the distributed package. Use this document to find the distributed training technology that can best serve your application.

Parallel-and-Distributed-Training

Distributed Data Parallel in PyTorch - Video Tutorials

This series of video tutorials walks you through distributed training in PyTorch via DDP.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Distributed Data Parallel in PyTorch - Video Tutorials

This series of video tutorials walks you through distributed training in PyTorch via DDP.

Parallel-and-Distributed-Training

Single-Machine Model Parallel Best Practices

Learn how to implement model parallel, a distributed training technique which splits a single model onto different GPUs, rather than replicating the entire model on each GPU
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Single-Machine Model Parallel Best Practices

Learn how to implement model parallel, a distributed training technique which splits a single model onto different GPUs, rather than replicating the entire model on each GPU

Parallel-and-Distributed-Training

Getting Started with Distributed Data Parallel

Learn the basics of when to use distributed data paralle versus data parallel and work through an example to set it up.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Getting Started with Distributed Data Parallel

Learn the basics of when to use distributed data paralle versus data parallel and work through an example to set it up.

Parallel-and-Distributed-Training

Writing Distributed Applications with PyTorch

Set up the distributed package of PyTorch, use the different communication strategies, and go over some the internals of the package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Writing Distributed Applications with PyTorch

Set up the distributed package of PyTorch, use the different communication strategies, and go over some the internals of the package.

Parallel-and-Distributed-Training

Large Scale Transformer model training with Tensor Parallel

Learn how to train large models with Tensor Parallel package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Large Scale Transformer model training with Tensor Parallel

Learn how to train large models with Tensor Parallel package.

Parallel-and-Distributed-Training

Customize Process Group Backends Using Cpp Extensions

Extend ProcessGroup with custom collective communication implementations.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Customize Process Group Backends Using Cpp Extensions

Extend ProcessGroup with custom collective communication implementations.

Parallel-and-Distributed-Training

Getting Started with Distributed RPC Framework

Learn how to build distributed training using the torch.distributed.rpc package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Getting Started with Distributed RPC Framework

Learn how to build distributed training using the torch.distributed.rpc package.

Parallel-and-Distributed-Training

Implementing a Parameter Server Using Distributed RPC Framework

Walk through a through a simple example of implementing a parameter server using PyTorch’s Distributed RPC framework.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Implementing a Parameter Server Using Distributed RPC Framework

Walk through a through a simple example of implementing a parameter server using PyTorch’s Distributed RPC framework.

Parallel-and-Distributed-Training

Introduction to Distributed Pipeline Parallelism

Demonstrate how to implement pipeline parallelism using torch.distributed.pipelining
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Introduction to Distributed Pipeline Parallelism

Demonstrate how to implement pipeline parallelism using torch.distributed.pipelining

Parallel-and-Distributed-Training

Implementing Batch RPC Processing Using Asynchronous Executions

Learn how to use rpc.functions.async_execution to implement batch RPC
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Implementing Batch RPC Processing Using Asynchronous Executions

Learn how to use rpc.functions.async_execution to implement batch RPC

Parallel-and-Distributed-Training

Combining Distributed DataParallel with Distributed RPC Framework

Walk through a through a simple example of how to combine distributed data parallelism with distributed model parallelism.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Combining Distributed DataParallel with Distributed RPC Framework

Walk through a through a simple example of how to combine distributed data parallelism with distributed model parallelism.

Parallel-and-Distributed-Training

Getting Started with Fully Sharded Data Parallel (FSDP2)

Learn how to train models with Fully Sharded Data Parallel (fully_shard) package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Getting Started with Fully Sharded Data Parallel (FSDP2)

Learn how to train models with Fully Sharded Data Parallel (fully_shard) package.

Parallel-and-Distributed-Training

Introduction to Libuv TCPStore Backend

TCPStore now uses a new server backend for faster connection and better scalability.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Introduction to Libuv TCPStore Backend

TCPStore now uses a new server backend for faster connection and better scalability.

Parallel-and-Distributed-Training

Exporting to ExecuTorch Tutorial

Learn about how to use ExecuTorch, a unified ML stack for lowering PyTorch models to edge devices.

================================================================================

# Welcome to PyTorch Tutorials - Exporting to ExecuTorch Tutorial

Learn about how to use ExecuTorch, a unified ML stack for lowering PyTorch models to edge devices.

Running an ExecuTorch Model in C++ Tutorial

Learn how to load and execute an ExecuTorch model in C++

================================================================================

# Welcome to PyTorch Tutorials - Running an ExecuTorch Model in C++ Tutorial

Learn how to load and execute an ExecuTorch model in C++

Using the ExecuTorch SDK to Profile a Model

Explore how to use the ExecuTorch SDK to profile, debug, and visualize ExecuTorch models

================================================================================

# Welcome to PyTorch Tutorials - Using the ExecuTorch SDK to Profile a Model

Explore how to use the ExecuTorch SDK to profile, debug, and visualize ExecuTorch models

Building an ExecuTorch iOS Demo App

Explore how to set up the ExecuTorch iOS Demo App, which uses the MobileNet v3 model to process live camera images leveraging three different backends: XNNPACK, Core ML, and Metal Performance Shaders (MPS).

================================================================================

# Welcome to PyTorch Tutorials - Building an ExecuTorch iOS Demo App

Explore how to set up the ExecuTorch iOS Demo App, which uses the MobileNet v3 model to process live camera images leveraging three different backends: XNNPACK, Core ML, and Metal Performance Shaders (MPS).

Building an ExecuTorch Android Demo App

Learn how to set up the ExecuTorch Android Demo App for image segmentation tasks using the DeepLab v3 model and XNNPACK FP32 backend.

================================================================================

# Welcome to PyTorch Tutorials - Building an ExecuTorch Android Demo App

Learn how to set up the ExecuTorch Android Demo App for image segmentation tasks using the DeepLab v3 model and XNNPACK FP32 backend.

Lowering a Model as a Delegate

Learn to accelerate your program using ExecuTorch by applying delegates through three methods: lowering the whole module, composing it with another module, and partitioning parts of a module.

================================================================================

# Welcome to PyTorch Tutorials - Lowering a Model as a Delegate

Learn to accelerate your program using ExecuTorch by applying delegates through three methods: lowering the whole module, composing it with another module, and partitioning parts of a module.

Introduction to TorchRec

TorchRec is a PyTorch domain library built to provide common sparsity & parallelism primitives needed for large-scale recommender systems.
TorchRec,Recommender

================================================================================

# Welcome to PyTorch Tutorials - Introduction to TorchRec

TorchRec is a PyTorch domain library built to provide common sparsity & parallelism primitives needed for large-scale recommender systems.

Exploring TorchRec sharding

This tutorial covers the sharding schemes of embedding tables by using EmbeddingPlanner and DistributedModelParallel API.
TorchRec,Recommender

================================================================================

# Welcome to PyTorch Tutorials - Exploring TorchRec sharding

This tutorial covers the sharding schemes of embedding tables by using EmbeddingPlanner and DistributedModelParallel API.

================================================================================

# Welcome to PyTorch Tutorials - Additional Resources

Examples of PyTorch
A set of examples around PyTorch in Vision, Text, Reinforcement Learning that you can incorporate in your existing work.
Check Out Examples

================================================================================

# Welcome to PyTorch Tutorials - Examples of PyTorch

A set of examples around PyTorch in Vision, Text, Reinforcement Learning that you can incorporate in your existing work.

Run Tutorials on Google Colab
Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.

================================================================================

# Welcome to PyTorch Tutorials - Run Tutorials on Google Colab

Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Learn the Basics

Created On: Feb 09, 2021 | Last Updated: Jul 07, 2025 | Last Verified: Nov 05, 2024

Authors:
Suraj Subramanian,
Seth Juarez,
Cassie Breviu,
Dmitry Soshnikov,
Ari Bornstein

Most machine learning workflows involve working with data, creating models, optimizing model
parameters, and saving the trained models. This tutorial introduces you to a complete ML workflow
implemented in PyTorch, with links to learn more about each of these concepts.

We’ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs
to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,
Bag, or Ankle boot.

This tutorial assumes a basic familiarity with Python and Deep Learning concepts.

================================================================================

# Learn the Basics - Running the Tutorial Code

You can run this tutorial in a couple of ways:

List:
In the cloud: This is the easiest way to get started! Each section has a “Run in Microsoft Learn” and “Run in Google Colab” link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.
: This option requires you to setup PyTorch and TorchVision first on your local machine (installation instructions). Download the notebook or copy the code into your favorite IDE.

In the cloud: This is the easiest way to get started! Each section has a “Run in Microsoft Learn” and “Run in Google Colab” link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.

: This option requires you to setup PyTorch and TorchVision first on your local machine (installation instructions). Download the notebook or copy the code into your favorite IDE.

================================================================================

# Learn the Basics - How to Use this Guide

If you’re familiar with other deep learning frameworks, check out the 0. Quickstart first
to quickly familiarize yourself with PyTorch’s API.

If you’re new to deep learning frameworks, head right into the first section of our step-by-step guide: 1. Tensors.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Introduction to PyTorch - YouTube Series

Created On: Nov 30, 2021 | Last Updated: Nov 04, 2024 | Last Verified: Nov 05, 2024

This page has been moved.

================================================================================

# Join the PyTorch Ecosystem - The PyTorch Ecosystem is made up of innovative open source AI projects that extend, integrate with, or build upon PyTorch.

If you’re developing a project that supports the PyTorch community, you’re welcome to apply for inclusion in the Ecosystem. Please review the PyTorch Ecosystem review process to ensure that you meet the minimum expectations before applying.

================================================================================

# Join the PyTorch Ecosystem - Application Process

Applying to join the PyTorch Ecosystem is simple:

List:
Open a new application using the Ecosystem Application GitHub issue form.
Complete all required sections.
Submit the issue.

Once submitted, your application enters the review pipeline managed by the PyTorch Ecosystem Working Group.

================================================================================

# Join the PyTorch Ecosystem - Review Workflow

All applications are tracked via the PyTorch Ecosystem Project Board.

Applications are reviewed on a first-come, first-served basis.

The Working Group reviews approximately 7–10 projects per session, depending on availability.

Projects are moved to “In Progress” roughly two weeks before a scheduled review session.

During review, Working Group members assess your application based on technical merit, alignment with the PyTorch mission, and community readiness. Questions and requests for clarification may be posted directly in your GitHub issue.

================================================================================

# Community Hub - Blog

Catch up on the latest happenings and technical insights.

================================================================================

# Community Hub - Community Events

Meet us at events events around the world.

================================================================================

# Community Hub - Case Studies

How our community solves real, everyday ML problems with PyTorch.

================================================================================

# Community Hub - PyTorch

Browse and join discussions on deep learning with PyTorch.

================================================================================

# Community Hub - Slack

Discuss advanced topics. Request access: https://bit.ly/ptslack

================================================================================

# Community Hub - 中文文档

Docs and tutorials in Chinese, translated by the community.

================================================================================

# Community Hub - 파이토치

Tutorials in Korean, translated by the community.

================================================================================

# Community Hub - 日本語

Tutorials in Japanese, translated by the community.

================================================================================

# Community Hub - Maintainers

Learn about the PyTorch core and module maintainers.

================================================================================

# Community Hub - Contribution

Learn how you can contribute to PyTorch code and documentation.

================================================================================

# Community Hub - DesignPhilosophy

PyTorch design principles for contributors and maintainers.

================================================================================

# Community Hub - Governance

Learn about the PyTorch governance hierarchy.

================================================================================

# Community Hub - Contributors

Stay up to date with the codebase and discover RFCs, PRs and more.

================================================================================

# Community Hub - PyTorch

Further your education and career goals.

================================================================================

# Community Hub - Newsletter

Stay up in touch for the latest updates, event info, and news.

================================================================================

# Developer Resources

Explore resources, get your questions answered, and join the discussion with other PyTorch developers.

================================================================================

# Developer Resources - PyTorch

Access comprehensive developer documentation.

================================================================================

# Developer Resources - PyTorch

Browse and join discussions on deep learning with PyTorch.

================================================================================

# Developer Resources - Tutorials

Get in-depth tutorials for beginners and advanced developers.

================================================================================

# Developer Resources - 中文文档

Docs and tutorials in Chinese, translated by the community.

================================================================================

# Developer Resources - GitHub

Report bugs, request features, discuss issues, and more.

================================================================================

# Developer Resources - 파이토치

Tutorials in Korean, translated by the community.

================================================================================

# Developer Resources - 日本語

Tutorials in Japanese, translated by the community.

================================================================================

# Developer Resources - Examples

View example projects for vision, text, RL, and more.

================================================================================

# Developer Resources - Maintainers

Learn about the PyTorch core and module maintainers.

================================================================================

# Developer Resources - Contribution

Learn how you can contribute to PyTorch code and documentation.

================================================================================

# Developer Resources - DesignPhilosophy

PyTorch design principles for contributors and maintainers.

================================================================================

# Developer Resources - PyTorch Dev Discussions

Forums used by PyTorch core developers and contributors to the PyTorch codebase for technical discussions.

================================================================================

# Developer Resources - Governance

Learn about the PyTorch governance hierarchy.

================================================================================

# Developer Resources - Mobile

Check out the PyTorch Mobile demo app for iOS and Android.

================================================================================

# Developer Resources - PyTorch

Further your education and career goals.

================================================================================

# Developer Resources - Newsletter

Stay update with the latest updates.

================================================================================

# PyTorch Contributor Awards - Call for Nominations: 2025 PyTorch Contributor Awards

Nominations for the 2025 PyTorch Contributor Awards are now .

Nomination Window
July 31 – August 22, 2025

Next Steps
Thank you to everyone who submitted! Please check back here for updates on candidate notifications and the award timeline.

================================================================================

# PyTorch Contributor Awards - PyTorch 2024 Awardees

Aaron Gokasian
PyTorch Review Powerhouse

Natalia Gimelshein
PyTorch Problem Solver

Kaichao You
PyTorch Innovator

Andrew Hoblitzell
PyTorch Trail Blazer

Stefano Fabri
PyTorch Rock Turner

Phillipe Tillet
PyTorch Ecosystem Champion

Jiong Gong
PyTorchbearer

================================================================================

# PyTorch Contributor Awards - PyTorch 2024 Nominees

Eddie Yan
Jithun Nair
Masahiro Hiramori
Jonathan Chuang

Salman Mohammadi
Davis Wertheimer
Xuehai Pan
Sunita Nadampalli

Leslie Fang
Sahdev Zala
Thien Tran
Pritam Damania

================================================================================

# PyTorch Contributor Awards - 2023 Awardees

SJeff Daily and Jack Cao
Pytorchbearer
Excellence in Long-Term Contributions Across All Modalities

James Keeble
PyTorch Pace-Setter
Excellence in High-Level Activity and Contributions

Tim Dettmers and Srishti Gureja
PyTorch Newcomer
Excellence in New Contributions

Mazen Alotaibi (Ma7dev), Stas Bekman, and Junghwan Park
PyTorch Ambassador
Excellence in Bringing New Users to the Community

Piotr Bialecki (ptrblck)
PyTorch Superhero
Excellence in All Aspects of Community Contributions

Aaron Gokaslan
Pytorch Review Powerhouse
Excellence in Code Review

Vadim Kantorov
PyTorch Problem-Solver
 Excellence in Uncovering or Resolving Bugs

Emcastillo
PyTorch Innovator
Excellence in Innovative New Features or Approaches

José Luis Castro Garcia
PyTorch Trail-blazer
Excellence in Documentation and Knowledge Sharing

================================================================================

# PyTorch Contributor Awards - 2023 Nominees

Bowen Bao
Yuanyuan Chen
Justin Chu
K. Frank
Jeremian Johnson
Jiawei Li

Li-Huai (Allan) Lin
Li Ning
Muhammad Qasim Khan
Fritz Obermeyer
Kulin Seth
ShiBo

Thomas Viehmann
Antoni Viros-i-Martin
Christian von der Weth
Ivan Yashchuk
Xiaobing Zhang

================================================================================

# Community Events - 

Sep 24–25, 2025				



					Montreal, Canada

================================================================================

# PyTorch Ambassadors

The PyTorch Ambassador Program highlights and supports passionate community leaders who educate, advocate for, and build with PyTorch in meaningful ways.

Ambassadors help grow PyTorch usage around the world by organizing events, creating educational content, mentoring new users, and contributing to the open-source ecosystem.

The first official cohort of PyTorch Ambassadors launched in September 2025.

Interested in becoming an Ambassador?

================================================================================

# PyTorch - PyTorch is an open source deep learning framework built to be flexible and modular for research, with the stability and support needed for production deployment.

PyTorch is an open source machine learning framework that accelerates the path from research prototyping to production deployment. Built to offer maximum flexibility and speed, PyTorch supports dynamic computation graphs, enabling researchers and developers to iterate quickly and intuitively. Its Pythonic design and deep integration with native Python tools make it an accessible and powerful platform for building and training deep learning models at scale.

Widely adopted across academia and industry, PyTorch has become the framework of choice for cutting-edge research and commercial AI applications. It supports a broad range of use cases—from natural language processing and computer vision to reinforcement learning and generative AI—through a robust ecosystem of libraries, tools, and integrations. PyTorch is also optimized for performance across CPUs, GPUs, and custom hardware accelerators, including support for distributed training and deployment on cloud platforms and mobile devices.

Whether you’re advancing academic research or deploying AI solutions at enterprise scale, PyTorch delivers the speed, flexibility, and ecosystem support needed to transform ideas into impact.

================================================================================

# vLLM - vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.

vLLM is an open source library for fast, easy-to-use LLM inference and serving. It optimizes hundreds of language models across diverse data-center hardware—NVIDIA and AMD GPUs, Google TPUs, AWS Trainium, Intel CPUs—using innovations such as PagedAttention, chunked prefill, multi-LoRA and automatic prefix caching. It is designed to serve large scale production traffic with OpenAI compatible server and offline batch inference, scalable to multi-node inference. As a community-driven project, vLLM collaborates with foundation model labs, hardware vendors and AI infrastructure companies to develop cutting-edge features.

The University of California – Berkeley contributed vLLM to the Linux Foundation in July 2024.

================================================================================

# DeepSpeed - DeepSpeed empowers developers to streamline distributed training and inference, making it easier to scale AI models efficiently while minimizing costs and operational complexity.

Training advanced deep learning models is challenging. Beyond model design, model scientists also need to set up the state-of-the-art training techniques such as distributed training, mixed precision, gradient accumulation, and checkpointing. Yet still, scientists may not achieve the desired system performance and convergence rate. Large model sizes are even more challenging: a large model easily runs out of memory with pure data parallelism, and it is difficult to use model parallelism. DeepSpeed addresses these challenges to accelerate model development and training. DeepSpeed enables the world’s most powerful language models like MT-530B and BLOOM. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference.

DeepSpeed was contributed by Microsoft to the Linux Foundation in January 2025.

================================================================================

# Host Your Project - Have an open source project that accelerates open source AI? The PyTorch Foundation offers a neutral, community-driven home for open source AI projects that share our commitment to open and accessible tools that solve problems and provide value in the AI lifecycle.

By hosting your project with us, you’ll gain support from a global network of developers, researchers, and industry leaders—plus access to events, training, and infrastructure that help your project grow and thrive.

Ready to take the next step? Submit your project for consideration to become a hosted project at the PyTorch Foundation.

================================================================================

# PyTorch documentation (Part 1)

PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.

Features described in this documentation are classified by release status:

Stable (API-Stable):
These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).

Unstable (API-Unstable):
Encompasses all features that are under active development where APIs may change based on user feedback, requisite performance improvements or because coverage across operators is not yet complete.
The APIs and performance characteristics of these features may change.

List:
Python API


torch.nn.functional
torch.Tensor
Tensor Attributes
Tensor Views

torch.autograd
torch.library
torch.accelerator

torch.cuda
torch.cuda.memory

================================================================================

# PyTorch documentation (Part 2)


torch.mtia
torch.mtia.memory
Meta device
torch.backends
torch.export
torch.distributed
torch.distributed.tensor
torch.distributed.algorithms.join
torch.distributed.elastic
torch.distributed.fsdp
torch.distributed.fsdp.fully_shard
torch.distributed.tensor.parallel
torch.distributed.optim
torch.distributed.pipelining
torch.distributed.checkpoint
torch.distributions
torch.compiler

torch.func
torch.futures

torch.fx.experimental


torch.linalg
torch.monitor
torch.signal
torch.special
torch.overrides
torch.package
torch.profiler
torch.nn.init
torch.nn.attention
torch.onnx
torch.optim
Complex Numbers
DDP Communication Hooks
Quantization
Distributed RPC Framework
torch.random
torch.masked
torch.nested
torch.Size
torch.sparse
torch.Storage
torch.testing
torch.utils
torch.utils.benchmark
torch.utils.bottleneck
torch.utils.checkpoint
torch.utils.cpp_extension
torch.utils.data
torch.utils.deterministic
torch.utils.jit
torch.utils.dlpack
torch.utils.mobile_optimizer
torch.utils.model_zoo
torch.utils.tensorboard
torch.utils.module_tracker

Named Tensors
Named Tensors operator coverage
torch.__config__
torch.__future__
torch._logging
Torch Environment Variables

================================================================================

# PyTorch documentation (Part 3)


Developer Notes
Automatic Mixed Precision examples
Autograd mechanics
Broadcasting semantics
CPU threading and TorchScript inference
CUDA semantics
PyTorch Custom Operators Landing Page
Distributed Data Parallel
Extending PyTorch
Extending torch.func with autograd.Function
Frequently Asked Questions
FSDP Notes
Getting Started on Intel GPU
Gradcheck mechanics
HIP (ROCm) semantics
Features for large-scale deployments
LibTorch Stable ABI

MPS backend
Multiprocessing best practices
Numerical accuracy

Reproducibility
Serialization semantics
Windows FAQ

List:
torch.nn.functional
torch.Tensor
Tensor Attributes
Tensor Views

torch.autograd
torch.library
torch.accelerator

torch.cuda
torch.cuda.memory


torch.mtia
torch.mtia.memory
Meta device
torch.backends
torch.export
torch.distributed
torch.distributed.tensor
torch.distributed.algorithms.join
torch.distributed.elastic
torch.distributed.fsdp
torch.distributed.fsdp.fully_shard
torch.distributed.tensor.parallel
torch.distributed.optim
torch.distributed.pipelining
torch.distributed.checkpoint
torch.distributions
torch.compiler

torch.func
torch.futures

torch.fx.experimental

================================================================================

# PyTorch documentation (Part 4)


torch.linalg
torch.monitor
torch.signal
torch.special
torch.overrides
torch.package
torch.profiler
torch.nn.init
torch.nn.attention
torch.onnx
torch.optim
Complex Numbers
DDP Communication Hooks
Quantization
Distributed RPC Framework
torch.random
torch.masked
torch.nested
torch.Size
torch.sparse
torch.Storage
torch.testing
torch.utils
torch.utils.benchmark
torch.utils.bottleneck
torch.utils.checkpoint
torch.utils.cpp_extension
torch.utils.data
torch.utils.deterministic
torch.utils.jit
torch.utils.dlpack
torch.utils.mobile_optimizer
torch.utils.model_zoo
torch.utils.tensorboard
torch.utils.module_tracker

Named Tensors
Named Tensors operator coverage
torch.__config__
torch.__future__
torch._logging
Torch Environment Variables

List:
Automatic Mixed Precision examples
Autograd mechanics
Broadcasting semantics
CPU threading and TorchScript inference
CUDA semantics
PyTorch Custom Operators Landing Page
Distributed Data Parallel
Extending PyTorch
Extending torch.func with autograd.Function
Frequently Asked Questions
FSDP Notes
Getting Started on Intel GPU
Gradcheck mechanics
HIP (ROCm) semantics
Features for large-scale deployments
LibTorch Stable ABI

================================================================================

# PyTorch documentation (Part 5)

MPS backend
Multiprocessing best practices
Numerical accuracy

Reproducibility
Serialization semantics
Windows FAQ

================================================================================

# PyTorch documentation - Indices and tables

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Announcements - AI Infra Summit at PyTorch Conference

On October 21st, the AI Infra Summit comes to San Francisco and PyTorch Conference 2025, bringing together experts building the infrastructure behind the latest explosion in AI innovation.  This half-day summit will cover the tools and platforms that drive AI’s rapid progress, including pre and post training, fine tuning, inference…

================================================================================

# Announcements - Take Our New PyTorch Associate Training at PyTorch Conference 2025

Be one of the first to take our brand new PyTorch Associate Training Course which will be offered in-person at the PyTorch Conference on Tuesday, October 21, 2025. Whether you aim to launch your deep learning journey, strengthen your current ML/DL capabilities, or gain an industry-recognized credential, this training offers…

================================================================================

# Announcements - Startup Showcase Returns to the PyTorch Conference October 21 in San Francisco

The Startup Showcase returns to the PyTorch Conference on Tuesday, October 21, 2025, spotlighting the most promising early-stage teams building real-world AI applications. The program gives founders a high-visibility platform to connect with investors, potential customers, and engineering talent. Why attend See what’s next: Live, on-stage pitches from AI startups…

================================================================================

# Announcements - Open Source AI Week Heads to the San Francisco Bay Area in October 2025

Mark your calendars! The inaugural Open Source AI Week is coming to the San Francisco Bay Area from October 18–26, 2025. This week-long celebration is the premier destination for the global AI community to explore cutting-edge research, groundbreaking tools, and open collaboration in artificial intelligence and machine learning What is…

================================================================================

# Announcements - The AI future Takes Center Stage: PyTorch Conference Keynote Speakers Announced

Get ready, San Francisco. The keynote lineup for PyTorch Conference is officially here and it's packed with some of the sharpest minds in open source AI and machine learning. This October 22–23, join us to hear from leading researchers, developers, and engineers driving innovation across the PyTorch ecosystem. You’ll gain…

================================================================================

# Announcements - Nominations Open for the 2025 PyTorch Contributor Awards

Nominations are now open for the 2025 PyTorch Contributor Awards! These awards shine a spotlight on the incredible individuals whose work and dedication are driving innovation, collaboration, and community-building within the PyTorch ecosystem. Whether through code, documentation, mentoring, community leadership, or new ideas that push boundaries, contributors are at the…

================================================================================

# Announcements - PyTorch Conference 2025 Schedule Announcement

First Look at the Future of AI. The #PyTorchConf Schedule Is Here! The wait is over! 💥 The PyTorch Conference schedule is live! Join us October 22–23 in San Francisco for 2️⃣ days of cutting-edge sessions, hands-on technical content, and insights from the leaders shaping the future of AI.  From…

================================================================================

# Announcements - The Open Source Legacy and AI’s Licensing Challenge

Open source licensing revolutionized software development, creating a thriving ecosystem built on shared innovation and collaboration. Licenses like MIT and Apache-2.0 gave developers a standard, legally robust way to share code, reducing friction and accelerating adoption. Today, we stand at a similar inflection point with open AI models. These models,…

================================================================================

# Announcements - Featured Sessions: Exploring Innovation at PyTorch Day China 2025

Featured Sessions: Exploring Innovation at PyTorch Day China 2025 PyTorch Day China 2025, proudly hosted by the PyTorch Foundation, will take place on June 7 in Beijing, China collocated with the BAAI Conference. This will be the second event in the new PyTorch Day series, following the inaugural PyTorch Day…

================================================================================

# Announcements - PyTorch Foundation at MLSys 2025

PyTorch Foundation at MLSys 2025: Supporting the Future of Machine Learning Systems The PyTorch Foundation is proud to support MLSys 2025 as a Gold Sponsor. Held May 12–15 in Santa Clara, CA, this premier conference sits at the intersection of machine learning and systems, bringing together researchers, engineers, and practitioners…

================================================================================

# Case Studies - How OpenSynth Uses PyTorch to Accelerate Compute for Energy Modelling Applications

OpenSynth has recently leveraged PyTorch to improve the experience of its users and community. OpenSynth is an open source community hosted by LF Energy that is democratising access to synthetic energy demand data.  Access to smart meter data is essential to rapid and successful energy transitions. Researchers, modelers and policymakers…

May 1, 2025 In Geospatial AI,

================================================================================

# Case Studies - How IBM Research Uses PyTorch and TerraTorch to Make Geospatial Computer Vision Accessible for Everyone

Earth Observation-based analytics are becoming essential for understanding our planet — from monitoring deforestation to tracking urban development and analyzing the impacts of climate change. However, the coding and deep learning skills for applying AI models to satellite imagery and earth observation data has traditionally been a major barrier for…

================================================================================

# Case Studies - How Intel Uses PyTorch to Empower Generative AI through Intel Arc GPUs

Intel has long been at the forefront of technological innovation, and its recent venture into Generative AI (GenAI) solutions is no exception. With the rise of AI-powered gaming experiences, Intel sought to deliver an accessible and intuitive GenAI inferencing solution tailored for AI PCs powered by Intel’s latest GPUs. By…

================================================================================

# Case Studies - How Intel Uses PyTorch to Empower Generative AI through Intel Arc GPUs

Intel has long been at the forefront of technological innovation, and its recent venture into Generative AI (GenAI) solutions is no exception. With the rise of AI-powered gaming experiences, Intel sought to deliver an accessible and intuitive GenAI inferencing solution tailored for AI PCs powered by Intel’s latest GPUs. By…

================================================================================

# Case Studies - Using PyTorch for Monocular Depth Estimation Webinar

In this webinar, Bob Chesebrough of Intel guides you through the steps he took to create a clipped image with background clutter removed from the image. He accomplished this using monocular depth estimation with PyTorch. This could potentially be used to automate structure from motion and other image-related tasks where…

================================================================================

# Case Studies - AI Helps Duolingo Personalize Language Learning

Learning a foreign language was probably one of your goals last year. And the year before, and the year before that. Like gym memberships, our best intentions often don’t survive very long. Aside from the time required to achieve proficiency with a new language, most people struggle with traditional approaches…

Oct 11, 2023 In Technology

================================================================================

# Case Studies - ML Model Server Resource Saving – Transition From High-Cost GPUs to Intel CPUs and oneAPI powered Software with performance

Here, We will be sharing our experience in moving AI workloads from our GPU servers to our Intel CPU servers without any performance or quality degradation, and saving annual costs of approximately 340 thousand U.S. Dollar (refer to the Conclusion) in the process.

Mar 9, 2023 In Technology

================================================================================

# Case Studies - Axon offers technology boost for public safety with in-car Automated License Plate Recognition on Azure

Axon, a technology leader in public safety, developed AI technology to add cutting-edge license plate recognition capabilities to its in-car camera products, which now can identify plates for vehicles of interest and provide law enforcement with proactive notifications and alerts. Axon AI scientists and engineers chose Microsoft Azure infrastructure as…

Feb 21, 2023 In Healthcare

================================================================================

# Case Studies - HippoScreen Improves AI Performance by 2.4x with oneAPI Tools

The Taiwan-based neurotechnology startup used tools and frameworks in the Intel® oneAPI Base and AI Analytics Toolkits to the improve efficiency and build times of deep-learning models used in its Brain Waves AI system. As a result, HippoScreen is able to broaden the system’s applications to a wider range of…

================================================================================

# Case Studies - NASA and IBM to Speed AI Creation with New Foundation Models

NASA and IBM are working together to create foundation models based on NASA’s data sets — including geospatial data — with the goal of accelerating the creation of AI models.

================================================================================

# PyTorch Newsletter - Newsletter Sign Up

Subscribe to our monthly newsletter to get the latest updates, event info, and community news from the PyTorch Foundation in your inbox.

================================================================================

# PyTorch Newsletter - Join the conversation

Join the contributor’s discussion forum to learn and collaborate on the latest development across PyTorch

================================================================================

# PyTorch Foundation - Accelerating Open Source AI

Welcome to the PyTorch Foundation—a vibrant, community-driven hub for open source AI. Developers, researchers, and industry pioneers collaborate here to advance the PyTorch framework and strengthen the open source AI ecosystem.

From cutting-edge development to production-ready tools and libraries, the PyTorch Foundation thrives through transparent collaboration and collective innovation. As part of the Linux Foundation, we host global events, deliver specialized training, support research, and provide resources to accelerate your AI journey. Whether you are contributing code, sharing your expertise, or deploying real-world AI solutions, the PyTorch Foundation actively empowers you to shape the future of accessible and impactful open source AI.

================================================================================

# PyTorch Foundation - Our Guiding Principles

Our mission is to drive the adoption of AI and deep learning by supporting an open, vendor-neutral ecosystem built around PyTorch. By making state-of-the-art tools and libraries accessible to everyone, we aim to democratize innovation in AI and ML. Learn more about the mission and values that guide us in our PyTorch Foundation Principles.

================================================================================

# PyTorch Foundation - Our Governance

The PyTorch Foundation’s Governing Board oversees the Foundation’s activities according to its Guiding Principles and the PyTorch Foundation Charter.

The PyTorch Foundation Code of Conduct details our commitment to fostering an inclusive, welcoming, and safe environment for everyone involved in the PyTorch Foundation community.

The technical governance structure for the PyTorch open source project is defined by the PyTorch maintainers and is available on our PyTorch Technical Governance page.

================================================================================

# PyTorch Foundation - How to Get Involved

New to the PyTorch Foundation? Check out our guide to getting started with the PyTorch Foundation or join the PyTorch  or  community to contribute, learn, and get your questions answered.

================================================================================

# PyTorch Foundation - Get in Touch

The success of PyTorch is only possible with the contributions and support of our developer community and member companies. If you would like to learn how you can collaborate with your peers in the PyTorch Foundation, and would like to have a conversation with a PyTorch Foundation representative, please fill out this form.

for all PyTorch technical questions please go to discuss.pytorch.org

================================================================================

# Governing Board - 

Andrew is part of Arm’s Strategy & Ecosystem and Developer platforms leadership team, heading up  Arm’s Open Source Office and is responsible for Arm’s relationship and engagements with software communities. With over 25 years of deep engagement with open source communities under a variety of employers, through the years he has held various positions with a variety of projects and foundations. Currently he is the Chair of the Yocto Project and sits on the Board of the PyTorch Foundation, FreeBSD Foundation, Xen Project and on the TSC/TAC/SC of PyTorch, Zephyr, UXLFoundation. He has previously served on the Board of the Rust Foundation, OpenUK and openSUSE Project. To support his work with the various communities, Andrew is often at most of the open source events that are held globally.

================================================================================

# Governing Board - Senior Director

Ankit Patel is a senior director at NVIDIA, leading developer engagement for SDKs, APIs, and tools. He joined NVIDIA in 2011, transitioning from GPU product manager to software product management in virtualization, ray tracing, and AI. Previously, he managed video editing and live production products at Matrox Video and Blackmagic Design. Ankit holds a bachelor’s degree in computer science from Concordia University and an MBA from Cornell University.

================================================================================

# Governing Board - Senior Principal Technologist

Brian Granger is a Senior Principal Technologist at Amazon Web Services and a professor of physics and data science at Cal Poly State University in San Luis Obispo, CA. He works at the intersection of UX design and engineering on tools for scientific computing, data science, machine learning, and data visualization. Brian is a co-founder and leader of Project Jupyter, co-founder of the Altair project for statistical visualization, and creator of the PyZMQ project for ZMQ-based message passing in Python.

================================================================================

# Governing Board - Vice President of AI Engineering

Dwarak Rajagopal is a distinguished technology leader with over two decades of experience at some of the world’s most innovative companies, including Google, Meta, Uber, Apple, and AMD. As Vice President of AI Engineering at Snowflake, Dwarak oversees the AI and ML engineering organizations, helping drive innovation across Snowflake Cortex AI, Snowflake’s AI Research Team, and open source offerings. He previously served as Senior Engineering Director at Google, where he spearheaded the AI Frameworks and On-Device Machine Learning teams, driving cutting-edge advancements in AI and large language models (LLMs) to enhance efficiency across server and mobile platforms. Earlier in his career, Dwarak led Meta’s PyTorch Core Frameworks team, bridging the gap between AI research and production. His career also includes developing autonomous driving software at Uber, pioneering graphics technology at Apple, and shaping compiler innovations at AMD.

Dwarak is passionate about democratizing AI through open source initiatives and ensuring the ethical advancement of technology. His visionary leadership continues to shape the future of AI and empower global innovation.

================================================================================

# Governing Board - General Manager of Open Source Development

Fred Li, General Manager of Open Source Development at Huawei’s Computing Product Line, brings over 20 years of technology and open-source experience since 2015. Serving as the platinum director of OpenStack Foundation and representing Huawei in PyTorch Foundation and LF AI & Data, he’s a pivotal figure in open source. Fred initiated the openEuler community in 2019, nurturing multiple open-source communities with thousands of engineers and millions of users. With expertise in IT product development, project management, and open-source strategy, Fred is committed to global open-source advocacy.

================================================================================

# Governing Board - 

Joseph Spisak is the Director of Product Management for AI at Meta. A veteran of the AI space with over 10 years experience, Joe led product teams at Meta/Facebook, Google and Amazon where he focused on open source AI, open science and building developer tools such as PyTorch to help the community scale up AI in an open and collaborative way. As the leader of product for PyTorch, he and the team built an amazing platform and community that made PyTorch the leading open source AI development framework in the industry and took it to the Linux Foundation. Joe is also an angel advisor to companies like Anthropic, Lastmile.AI (former FB), EvolutionaryScale (former FB), Udacity, Lightning.AI, and others.

================================================================================

# Governing Board - VP S/W of AI Frameworks team

Kismat works as VP S/W of AI Frameworks team at Intel. The team focuses on frameworks like PyTorch, TF and other inference/serving engines. Prior to Intel Kismat worked at Nvidia for 8 focusing on AI S/W frameworks/libs like TensorRT, Deep learning compiler etc.

================================================================================

# Governing Board - CTO

Luca Antiga is the CTO at Lightning AI. He is an early contributor to PyTorch core and co-authored “Deep Learning with PyTorch” (published by Manning). He started his journey as a researcher in Bioengineering, and later co-founded Orobix, a company focused on building and deploying AI in production settings.

================================================================================

# Governing Board - Head of Open Source

Lysandre is the Head of Open Source at Hugging Face. Lysandre has been at Hugging Face since the company’s pivot to open-source, and was the first engineer to focus entirely on the open-source mission. Now leading the open-source part of the organization, Lysandre remains technically involved by being a core maintainer of the Transformers library.

================================================================================

# Governing Board - Director of Product Management

Niles Burbank is Director of Product Management at AMD, responsible for a family of data center GPU products. Based in Markham, Canada, he has been involved in the computer hardware industry since 1995 in a range of technical and management roles. Niles shares AMD’s passion for working together with the wider community to deliver the software and hardware tools that will advance AI. He holds an MASc in Electrical Engineering from the University of Toronto.

================================================================================

# Governing Board - Distinguished Engineer

Raghu Ganti, a Distinguished Engineer at IBM T. J. Watson Research Center, leads the development of tuning and training stack for LLMs on IBM’s Hybrid Cloud platform. He has extensive expertise in large scale distributed algorithms and his work on spatiotemporal data analysis has been included in 15+ IBM products.

================================================================================

# Governing Board - 

Steve Wan leads AI at Scale business development at Microsoft, aiming to empower developers to efficiently train and deploy machine learning models at unmatched scale on Azure. A pivotal aspect of this strategy involves enabling and innovating on PyTorch.

================================================================================

# Technical Advisory Council - The PyTorch Foundation’s Technical Advisory Council (TAC) provides a forum for technical communication, leadership, and collaboration for the PyTorch Foundation. The committee members are members of the PyTorch Foundation.

The TAC holds open meetings once a month that anyone in the community can attend. The committee provides thought leadership on technical topics, knowledge sharing, and a forum to discuss issues with other technical experts in the community.

================================================================================

# Technical Advisory Council - Resources

List:
View the TAC agenda
Attend a TAC meeting (Public calendar)
Access TAC presentations and meeting recordings (TAC list-serv)
Discuss technical topics
Getting started with the PyTorch Foundation
Attend DeepSpeed virtual office hours

================================================================================

# Technical Advisory Council - Senior Principal Technologist

Brian Granger is a Senior Principal Technologist at Amazon Web Services and a professor of physics and data science at Cal Poly State University in San Luis Obispo, CA. He works at the intersection of UX design and engineering on tools for scientific computing, data science, machine learning, and data visualization. Brian is a co-founder and leader of Project Jupyter, co-founder of the Altair project for statistical visualization, and creator of the PyZMQ project for ZMQ-based message passing in Python.

================================================================================

# Technical Advisory Council - Fellow

Jeff Daily is a Fellow at AMD and the chief architect of the Machine Learning Software Engineering group supporting ML frameworks such as PyTorch and onnxruntime on AMD GPUs. He enjoys delivering open source software to answer the challenges of the rapidly-changing ML landscape. For over five years, he has contributed to the PyTorch core as well as its extension libraries. His sustained contributions earned him the first ever Linux Foundation PyTorch Award “for excellence in long-term contributions across all PyTorch modalities.” Though he has not achieved the status of maintainer, Jeff is a trusted contributor. Jeff’s technical leadership is one of the reasons PyTorch runs out of the box without any code modifications on AMD GPUs. In addition to PyTorch technical contributions, Jeff is the AMD representative for the PyTorch Foundation’s Technical Advisory Committee.

================================================================================

# Technical Advisory Council - CTO

Luca Antiga is the CTO at Lightning AI. He is an early contributor to PyTorch core and co-authored “Deep Learning with PyTorch” (published by Manning). He started his journey as a researcher in Bioengineering, and later co-founded Orobix, a company focused on building and deploying AI in production settings.

================================================================================

# Technical Advisory Council - Technical Director

Milos Puzovic is Technical Director at Arm where he is working on accelerating machine learning frameworks such as PyTorch on AArch64. In the past, he worked on designing and developing infrastructure for rapid development and deployment to edge devices and cloud of novel neural models that were trained using semi-supervised approach. He also has interest in optimizing applications through hardware and software co-design by using machine learning, code generation, optimization and verification of high-level models for different types of architecture. Milos has PhD in Computer Science from University of Cambridge where his thesis was on hardware/software interface for dynamic multicore scheduling and BSc with First Class Honors in Joint Mathematics and Computer Science from Imperial College London.

================================================================================

# Technical Advisory Council - 

Piotr joined PyTorch team at NVIDIA in 2019 and currently manages the team. He drives NVIDIA’s effort in maintaining and advancing PyTorch’s CUDA backend and received the PyTorch SUPERHERO award in 2023 for his community contributions especially in the PyTorch discussion board. As a Core Maintainer, he is also focussed on PyTorch’s long-term vision and development.

================================================================================

# Technical Advisory Council - Cloud Infrastructure and Open Source Lead

Ricardo is a seasoned technology leader with over two decades of experience across the enterprise and startup landscape. He works at Snowflake as a Cloud Infrastructure and Open Source Lead, focusing on automating AI/ML infrastructure using cloud-native technologies at scale. A passionate open source advocate, Ricardo also serves as a shadow member of the CNCF Technical Oversight Committee and CNCF AI Sub-project, where he helps shape the future of AI computing infrastructure.

Throughout his career, Ricardo has held key engineering and leadership roles at major companies such as Rakuten, Cisco, and VMware, as well as at innovative startups including Truera, Branch Metrics, Coupa, HyTrust, Exablox, and SnapLogic. He’s committed to community-driven innovation and regularly contributes to industry initiatives that bridge the gap between open source communities and enterprise adoption.

================================================================================

# Technical Advisory Council - 

Soumith Chintala is a Scientist-Engineer focused on AI and Robotics, leading influential AI work such as PyTorch, DCGAN and Torch-7; work which is used by several top institutions including NASA, Meta, Google, Tesla, Microsoft, Disney, Genentech, and numerous other Fortune-500 companies and in the curriculum of top-ranked universities such as Stanford, Harvard, Oxford and MIT. He currently leads PyTorch and other AI projects at Meta, is a Visiting Professor at New York University, and maintains advisory roles at various institutions.

================================================================================

# Technical Advisory Council - 

Graduated in 1999 from the ENSAE and doctor in 2004, Xavier Dupré began his career at A2iA, a company specialized in automatic reading of cheques and handwriting recognition. After a short passage in finance, he joined Yahoo in 2008 to work on search query rewriting problems by implementing statistical algorithms designed for all languages. In 2010, Xavier joined Microsoft to participate in the local search engine. He contributed to the partnership between PagesJaunes and Bing. Xavier worked on very large scale problems for Microsoft’s search engine Bing. He is now working on Azure Machine Learning. Meanwhile, Xavier Dupré have been teaching programming since 2001 at the ENSAE. The courses have expanded in 2014 to machine learning and technologies associated to big data, including Azure through a partnership between Microsoft France and the ENSAE. More recently, Xavier has explored new ways to teach through hackathons (hackathon Microsoft- ENSAE-Croix-Rouge – November 2015 – video – article), collaborations between academic and non-profitable organization through students projects or coding snack.

================================================================================

# Technical Advisory Council - Principal Engineer

Yikun Jiang is principal software engineer from Huawei computing opensource development team, working on multi-arch, heterogeneous hardware support and improvement of projects in computing area. He has more than 10 years experience in computing area, and leads an active and creative team in R&D under the principle of “upstream first”, which aims to make diverse computing power ubiquitous.

================================================================================

# PyTorch Cloud Credit Program - We believe providing public, self-service, and automated access to cloud infrastructure is essential for every project’s incubation, growth, and success.

To support this, PyTorch has established a program that enables organizations to contribute either cloud credits or financial donations directly towards maintaining and expanding our Continuous Integration (CI) infrastructure and other foundation-hosted project infrastructure. Contributions from organizations like AWS have already provided cloud credits, demonstrating a clear commitment to the success and sustainability of the PyTorch’ Foundation’s hosted projects. Many organizations continue to sponsor PyTorch projects, recognizing that supporting foundational infrastructure contributes directly to their own business growth and success.

================================================================================

# PyTorch Cloud Credit Program - Sponsors

Organizations can get started sponsoring cloud credits today.

================================================================================

# PyTorch Cloud Credit Program - Advance the ecosystem

Join the PyTorch Credits program and benefit in the following ways:

List:
Highlight your company participation in the ecosystem
Help shape the future of cloud native
Establish your company as a thought leader in the space
Collaborate with various companies and organizations — improving open source technologies

================================================================================

# PyTorch Cloud Credit Program - Supporter

List:
Coordinated PyTorch blog
Appropriate placement on the PyTorch website

================================================================================

# PyTorch Cloud Credit Program - Advocate

Everything included in Supporter benefits, plus:

List:
Bonus online program slot (live webinar, live stream, or on-demand webinar – your choice) to highlight your participation in the program

================================================================================

# PyTorch Cloud Credit Program - Champion

Everything included in Advocate benefits, plus:

List:
Top placement on the PyTorch website with an explanation of the offering
PyTorch marketing campaign to drive awareness towards the donation
– Coordinated blog post and media outreach
– Social media shout out to thank your company for their contribution
– Mention in the PyTorch keynote at the next PyTorch Conference
PyTorch will set up resource pooling and CI capacity for self-service of your offering

================================================================================

# PyTorch Cloud Credit Program - Decide how to donate

Cloud credits can be donated via cash or credit:

List:
Credit, for products a sponsor usually sells as a service to the public
Cash, to be earmarked for paying for services when we run out of donated credits

================================================================================

# PyTorch Cloud Credit Program - Determine the technical points of contact

Projects often need help utilizing credits. We ask that you provide a technical support resource with a defined SLA.

================================================================================

# PyTorch Cloud Credit Program - Projects benefit from your donation

After getting set up with PyTorch, projects can access these credits via a curated self-service portal, managed by PyTorch.

================================================================================

# Staff - Executive Director

Matt White (he/him) is the Executive Director of the PyTorch Foundation and GM of AI at the Linux Foundation. Matt is a distinguished expert in artificial intelligence and business, renowned for successfully deploying large-scale AI platforms across the telecom, gaming, media, and entertainment industries. With over two decades of experience, Matt has consistently demonstrated his ability to stay ahead of the curve in technological innovation, spearheading advancements in AI applications in diverse domains.

================================================================================

# Staff - Director of Marketing and Communications

Jennifer Bly (she/her) is Director of Marketing and Communications at the Linux Foundation where she leads marketing and communications for open source projects like the PyTorch Foundation, High Performance Software Foundation, OpenSSF, and Ultra Ethernet Consortium. She develops marketing strategies that encompass integrated campaigns with an emphasis on public relations, communications, social media engagement, content development, brand storytelling and more. Jennifer has a knack for making complex technical issues easy to understand in ways that motivate people to pay attention and take action. Before joining the Linux Foundation she was the External Relations Manager at the American Registry for Internet Numbers (ARIN), where she led the outreach team to increase the organization’s visibility in the internet community and develop strategic relationships that build customer growth and engagement. Jennifer earned her MA in Mass Communication from the University of Florida and Accreditation in Public Relations (APR) from the Public Relations Society of America (PRSA). Within PRSA she is active on the Executive Committee of the Technology Section. She is passionate about creating a better internet and enjoys getting the word out about people doing great work in the open source community.

================================================================================

# Staff - Director of Events

Deb Giles (she/her) is the Event Director for the Linux Foundation’s Collaborative Projects, where she oversees global events, including the PyTorch Conference. With over 15 years of experience in producing conferences, corporate meetings, and trade shows, Deb has worked across a wide range of industries including tech, gaming, and construction. She holds a Bachelor’s degree in Marketing from Azusa Pacific University. Based in Nashville, TN, Deb enjoys traveling, hiking, and spending time with her dog, Shadow, in her free time.

================================================================================

# Staff - Senior Cloud Operations Engineer

Thanh Ha is a Senior Cloud Operations Engineer at The Linux Foundation, working on the PyTorch CI infrastructure. A seasoned DevOps professional with over 15 years experience, Thanh excels in Infrastructure Optimization: Enhancing the backbone of software development for scalability and performance, Automation Mastery: Streamlining CI/CD pipelines to accelerate development cycles and ensure consistency, and Quality Assurance: Innovating QA and testing processes to deliver robust, high-quality software. With a passion for merging hands-on technical expertise with industry best practices, Thanh collaborates closely with development teams, driving technological advancements and mentoring emerging talent in the DevOps field.

================================================================================

# Staff - Program Manager

Regina Nkenchor (she/her) is a Program Manager at the Linux Foundation, where she oversees high-impact projects, including PyTorch, the Civil Infrastructure Platform (CIP), and RISE. With over a decade of experience in the technology industry, Regina has successfully led digital transformation initiatives across sectors such as tax, finance, education, retail, and open source ecosystems.

Prior to joining the Linux Foundation, Regina served as a Software Engineer and as an Open Source Program Office (OSPO) Ambassador at IKEA. She holds a Master’s degree in Informatics from Halmstad University, Sweden, and a Professional Diploma in Software Engineering from NIIT in Port Harcourt, Nigeria.

She also earned a Bachelor’s degree in Public Administration and a Diploma in Law from the University of Jos, Nigeria.
Outside of work, Regina enjoys traveling and connecting with new people. Her goal is to empower developers and communities to thrive while fostering a healthier, more inclusive open source ecosystem for everyone.

================================================================================

# Staff - Communications and Marketing Associate

Bazil Sterling (he/him) is a Communications and Marketing Associate at The Linux Foundation, where he drives marketing and communications efforts for key open source initiatives, including the PyTorch Foundation, LF Decentralized Trust, and DENT. With a background in journalism, public relations, marketing, and advertising, Bazil is a storyteller who crafts compelling narratives that elevate open source communities. He is particularly passionate about supporting projects advancing sustainability and open collaboration.

================================================================================

# Staff - Senior Cloud Operations Engineer

Jordan Conway is a Senior Cloud Operations Engineer at The Linux Foundation, mastering multi cloud environments and CI/CD workflows. He’s a skilled problem-solver with a passion for automation and Open Source technology. Jordan also brings a collaborative spirit and deep technical skills to his work. With experience that includes leadership roles in the Open Source Technology and Medical Imaging Technology industries, he’s a fast learner always exploring new tech.

When he’s not “brew installing” homebrew packages you can find him homebrewing beer with his wife and exploring the wilderness with his two daughters.

================================================================================

# Staff - Sr. Program Manager

Naomi is an accomplished professional with extensive experience in project and event management. A detail-oriented, innovative professional with 10+ years of experience in leading projects to successful completion and a performance history of amplifying efficiency and productivity.

================================================================================

# Staff - PR Manager

Grace Lucier (she/her) is a PR Manager at The Linux Foundation, where she drives media relations and strategy for the PyTorch Foundation, the OpenSearch Software Foundation and the Open Software Security Foundation, amongst other programs. Her background in communications, marketing and journalism informs her PR expertise. Prior to joining the Linux Foundation, she worked at two Boston-based PR agencies. She holds a B.A. in English and Philosophy from The College of the Holy Cross.

================================================================================

# Staff - Membership Solutions

Meredith Roach (she/her) works on the Memberships team at The Linux Foundation, where she drives membership growth for the PyTorch Foundation. Prior to joining the Linux Foundation, she worked as a technical trainer and instructional designer in the software industry. She holds a B.A. in Organizational Development from Temple University, and when not at work, you can likely find her hiking and fishing in the mountains with her family.

================================================================================

# Contact Us - NEED TECHNICAL SUPPORT?

Please visit discuss.pytorch.org for technical support issues, you will receive the fastest response for your issue. Thank you.

================================================================================

# Join PyTorch Foundation - Become part of a thriving community in the booming global AI market.

According to statistics from MIT Sloan, 75% of top executives believe AI will help their organizations grow and gain a competitive edge. Since 2020, there has been a 14X increase in the number of active AI startups, and venture capitalist-funded startups have increased by 6X. The PwC Global Artificial Intelligence Study indicates that AI has the potential to contribute $15.7 trillion to the global economy by 2030, with 45% of the total economic gains coming from product enhancements that stimulate consumer demand.

By joining the PyTorch Foundation, you can help build and shape the future of end-to-end machine learning frameworks alongside your industry peers. PyTorch offers a user-friendly front-end, distributed training, and an ecosystem of tools and libraries that enable fast, flexible experimentation and efficient production.

As a member of the PyTorch Foundation, you’ll have access to resources that allow you to be stewards of stable, secure, and long-lasting codebases. You can collaborate on training, local and regional events, open-source developer tooling, academic research, and guides to help new users and contributors have a productive experience.

================================================================================

# Join PyTorch Foundation - Apply for Membership Today

If your organization is interested in becoming a Member of the PyTorch Foundation, please complete the Member Enrollment form and designate someone with signature authority for your institution for the membership approval process.

For Premier Membership you must also complete the Premier Membership Questionnaire after submitting your membership application. The Foundation will review your application after both have been submitted.

Please note, membership is not required to participate in PyTorch as an individual contributor. You can join the community at any time and collaborate with others interested in the project.

Are you a student interested in learning about PyTorch? Get Started!

================================================================================

# Join PyTorch Foundation - Insight

Gain technical traction and insight for your organization’s products by immersing your teams with other industry leaders.

================================================================================

# Join PyTorch Foundation - Influence

Influence technical priorities, approaches, and code and gain early access to technical deliverables in motion.

================================================================================

# Join PyTorch Foundation - Thought Leadership

Provide thought leadership and expand industry awareness as PyTorch amplifies member progress across the industry.

================================================================================

# Join PyTorch Foundation - Talent

Retain, attract, and increase engineering skills and employees and build your innovation partner network, supply chain, and customer pipeline.

================================================================================

# Join PyTorch Foundation - Community

Deepen your engagement and leadership in local and industry developer networks and conferences.

================================================================================

# Join PyTorch Foundation - Join the Membership that fits your goals (Part 1)

Table:
Guaranteed one (1) seat on the PTF Governing Board |  |  | 
Guaranteed one (1) voting representative on the PyTorch Technical Advisory Committee (TAC) |  |  | 
Guaranteed one (1) voting representative in any other subcommittees  of the PTF |  |  | 
Enjoy most prominent placement in displays of membership including website, landscape and marketing materials |  |  | 
Add resources to the PyTorch Infrastructure team and get the ability to execute self hosted runners for tests on your own hardware/soc/systems |  |  | 
Receive greater insight into PTF strategy and activities through engagement with the PTF leadership team |  |  | 
Opportunity to be considered for a PTF Board seat (Premier are guaranteed a seat) |  |  | 
Opportunity to be considered for a PTF TAC seat (Premier are guaranteed a seat) |  |  | 
Create an individualized amplification plan  with the PTF team |  |  | 
Participate in Marketing, Community, and Thought Leadership opportunities |  |  | 
Receive discounts on PTF event sponsorships and training courses |  |  | 
Your company logo displayed on the PTF website and in marketing materials |  |  | 
Opportunity to host PyTorch on-demand webinars and livestreams |  |  | 
Annual Fee | LF Silver Membership + $150,000 | LF Silver Membership + see sliding scale | Free: Exclusive to academic and nonprofit organizations

================================================================================

# Join PyTorch Foundation - Join the Membership that fits your goals (Part 2)

Table:
Consolidated Employees
5,000 employees +
3,000 - 4,999 employees
1,000 - 2,999 employees
500 - 999 employees
1 - 499 employees
1 Board Seat per 10 members  (max 3)

================================================================================

# Join PyTorch Foundation - Questions? Contact Us

The success of PyTorch is only possible with the contributions and support of our developer community and member companies. To learn more about how you can join your industry peers in supporting PyTorch, or for any questions you may have, please fill out this form to be contacted by a PyTorch Foundation representative.

for all PyTorch technical questions please go to discuss.pytorch.org

================================================================================

# Python API (Part 1)

Created On: Apr 16, 2025 | Last Updated On: Apr 16, 2025

List:
torch.nn.functional
torch.Tensor
Tensor Attributes
Tensor Views

torch.autograd
torch.library
torch.accelerator

torch.cuda
torch.cuda.memory


torch.mtia
torch.mtia.memory
Meta device
torch.backends
torch.export
torch.distributed
torch.distributed.tensor
torch.distributed.algorithms.join
torch.distributed.elastic
torch.distributed.fsdp
torch.distributed.fsdp.fully_shard
torch.distributed.tensor.parallel
torch.distributed.optim
torch.distributed.pipelining
torch.distributed.checkpoint
torch.distributions
torch.compiler

torch.func
torch.futures

torch.fx.experimental

================================================================================

# Python API (Part 2)


torch.linalg
torch.monitor
torch.signal
torch.special
torch.overrides
torch.package
torch.profiler
torch.nn.init
torch.nn.attention
torch.onnx
torch.optim
Complex Numbers
DDP Communication Hooks
Quantization
Distributed RPC Framework
torch.random
torch.masked
torch.nested
torch.Size
torch.sparse
torch.Storage
torch.testing
torch.utils
torch.utils.benchmark
torch.utils.bottleneck
torch.utils.checkpoint
torch.utils.cpp_extension
torch.utils.data
torch.utils.deterministic
torch.utils.jit
torch.utils.dlpack
torch.utils.mobile_optimizer
torch.utils.model_zoo
torch.utils.tensorboard
torch.utils.module_tracker

Named Tensors
Named Tensors operator coverage
torch.__config__
torch.__future__
torch._logging
Torch Environment Variables

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Developer Notes

Created On: Apr 16, 2025 | Last Updated On: Apr 16, 2025

List:
Automatic Mixed Precision examples
Autograd mechanics
Broadcasting semantics
CPU threading and TorchScript inference
CUDA semantics
PyTorch Custom Operators Landing Page
Distributed Data Parallel
Extending PyTorch
Extending torch.func with autograd.Function
Frequently Asked Questions
FSDP Notes
Getting Started on Intel GPU
Gradcheck mechanics
HIP (ROCm) semantics
Features for large-scale deployments
LibTorch Stable ABI

MPS backend
Multiprocessing best practices
Numerical accuracy

Reproducibility
Serialization semantics
Windows FAQ

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Community

Created On: Apr 16, 2025 | Last Updated On: May 12, 2025

PyTorch is more than just a deep learning framework—it’s a vibrant
ecosystem powered by a diverse global community. The PyTorch community
brings together researchers, developers, students, and industry
professionals who collaborate to advance the state of machine learning.

Check out the resources below to learn how to contribute code to the
core framework, report and fix bugs, improve documentation, and much more.

List:
The Ultimate Guide to PyTorch Contributions
PyTorch Governance | Build + CI
PyTorch Contribution Guide
PyTorch Design Philosophy
PyTorch Governance | Mechanics
PyTorch Governance | Maintainers

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Welcome to PyTorch Tutorials

What’s new in PyTorch tutorials?

List:
Integrating Custom Operators with SYCL for Intel GPU
Supporting Custom C++ Classes in torch.compile/torch.export
Accelerating torch.save and torch.load with GPUDirect Storage
Getting Started with Fully Sharded Data Parallel (FSDP2)

Integrating Custom Operators with SYCL for Intel GPU

Supporting Custom C++ Classes in torch.compile/torch.export

Accelerating torch.save and torch.load with GPUDirect Storage

Getting Started with Fully Sharded Data Parallel (FSDP2)

Learn the Basics
Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.
Get started with PyTorch

================================================================================

# Welcome to PyTorch Tutorials - Learn the Basics

Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in this quickstart guide.

PyTorch Recipes
Bite-size, ready-to-deploy PyTorch code examples.
Explore Recipes

================================================================================

# Welcome to PyTorch Tutorials - PyTorch Recipes

Bite-size, ready-to-deploy PyTorch code examples.

Learn the Basics

A step-by-step guide to building a complete ML workflow with PyTorch.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Learn the Basics

A step-by-step guide to building a complete ML workflow with PyTorch.

Introduction to PyTorch on YouTube

An introduction to building a complete ML workflow with PyTorch. Follows the PyTorch Beginner Series on YouTube.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Introduction to PyTorch on YouTube

An introduction to building a complete ML workflow with PyTorch. Follows the PyTorch Beginner Series on YouTube.

Learning PyTorch with Examples

This tutorial introduces the fundamental concepts of PyTorch through self-contained examples.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Learning PyTorch with Examples

This tutorial introduces the fundamental concepts of PyTorch through self-contained examples.

What is torch.nn really?

Use torch.nn to create and train a neural network.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - What is torch.nn really?

Use torch.nn to create and train a neural network.

Visualizing Models, Data, and Training with TensorBoard

Learn to use TensorBoard to visualize data and model training.
Interpretability,Getting-Started,TensorBoard

================================================================================

# Welcome to PyTorch Tutorials - Visualizing Models, Data, and Training with TensorBoard

Learn to use TensorBoard to visualize data and model training.

Interpretability,Getting-Started,TensorBoard

Good usage of `non_blocking` and `pin_memory()` in PyTorch

A guide on best practices to copy data from CPU to GPU.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Good usage of `non_blocking` and `pin_memory()` in PyTorch

A guide on best practices to copy data from CPU to GPU.

Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors

Learn the subtleties of requires_grad, retain_grad, leaf, and non-leaf tensors
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Understanding requires_grad, retain_grad, Leaf, and Non-leaf Tensors

Learn the subtleties of requires_grad, retain_grad, leaf, and non-leaf tensors

Visualizing Gradients in PyTorch

Visualize the gradient flow of a network.
Getting-Started

================================================================================

# Welcome to PyTorch Tutorials - Visualizing Gradients in PyTorch

Visualize the gradient flow of a network.

TorchVision Object Detection Finetuning Tutorial

Finetune a pre-trained Mask R-CNN model.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - TorchVision Object Detection Finetuning Tutorial

Finetune a pre-trained Mask R-CNN model.

Transfer Learning for Computer Vision Tutorial

Train a convolutional neural network for image classification using transfer learning.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Transfer Learning for Computer Vision Tutorial

Train a convolutional neural network for image classification using transfer learning.

Adversarial Example Generation

Train a convolutional neural network for image classification using transfer learning.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Adversarial Example Generation

Train a convolutional neural network for image classification using transfer learning.

DCGAN Tutorial

Train a generative adversarial network (GAN) to generate new celebrities.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - DCGAN Tutorial

Train a generative adversarial network (GAN) to generate new celebrities.

Spatial Transformer Networks Tutorial

Learn how to augment your network using a visual attention mechanism.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Spatial Transformer Networks Tutorial

Learn how to augment your network using a visual attention mechanism.

Inference on Whole Slide Images with TIAToolbox

Learn how to use the TIAToolbox to perform inference on whole slide images.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Inference on Whole Slide Images with TIAToolbox

Learn how to use the TIAToolbox to perform inference on whole slide images.

Semi-Supervised Learning Tutorial Based on USB

Learn how to train semi-supervised learning algorithms (on custom data) using USB and PyTorch.
Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Semi-Supervised Learning Tutorial Based on USB

Learn how to train semi-supervised learning algorithms (on custom data) using USB and PyTorch.

Audio IO

Learn to load data with torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio IO

Learn to load data with torchaudio.

Audio Resampling

Learn to resample audio waveforms using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Resampling

Learn to resample audio waveforms using torchaudio.

Audio Data Augmentation

Learn to apply data augmentations using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Data Augmentation

Learn to apply data augmentations using torchaudio.

Audio Feature Extractions

Learn to extract features using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Feature Extractions

Learn to extract features using torchaudio.

Audio Feature Augmentation

Learn to augment features using torchaudio.

================================================================================

# Welcome to PyTorch Tutorials - Audio Feature Augmentation

Learn to augment features using torchaudio.

Audio Datasets

Learn to use torchaudio datasets.

================================================================================

# Welcome to PyTorch Tutorials - Audio Datasets

Learn to use torchaudio datasets.

Automatic Speech Recognition with Wav2Vec2 in torchaudio

Learn how to use torchaudio's pretrained models for building a speech recognition application.

================================================================================

# Welcome to PyTorch Tutorials - Automatic Speech Recognition with Wav2Vec2 in torchaudio

Learn how to use torchaudio's pretrained models for building a speech recognition application.

Speech Command Classification

Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset.

================================================================================

# Welcome to PyTorch Tutorials - Speech Command Classification

Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset.

Text-to-Speech with torchaudio

Learn how to use torchaudio's pretrained models for building a text-to-speech application.

================================================================================

# Welcome to PyTorch Tutorials - Text-to-Speech with torchaudio

Learn how to use torchaudio's pretrained models for building a text-to-speech application.

Forced Alignment with Wav2Vec2 in torchaudio

Learn how to use torchaudio's Wav2Vec2 pretrained models for aligning text to speech

================================================================================

# Welcome to PyTorch Tutorials - Forced Alignment with Wav2Vec2 in torchaudio

Learn how to use torchaudio's Wav2Vec2 pretrained models for aligning text to speech

NLP from Scratch: Classifying Names with a Character-level RNN

Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials.

================================================================================

# Welcome to PyTorch Tutorials - NLP from Scratch: Classifying Names with a Character-level RNN

Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials.

NLP from Scratch: Generating Names with a Character-level RNN

After using character-level RNN to classify names, learn how to generate names from languages. Second in a series of three tutorials.

================================================================================

# Welcome to PyTorch Tutorials - NLP from Scratch: Generating Names with a Character-level RNN

After using character-level RNN to classify names, learn how to generate names from languages. Second in a series of three tutorials.

NLP from Scratch: Translation with a Sequence-to-sequence Network and Attention

This is the third and final tutorial on doing “NLP From Scratch”, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks.

================================================================================

# Welcome to PyTorch Tutorials - NLP from Scratch: Translation with a Sequence-to-sequence Network and Attention

This is the third and final tutorial on doing “NLP From Scratch”, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks.

Exporting a PyTorch model to ONNX using TorchDynamo backend and Running it using ONNX Runtime

Build a image classifier model in PyTorch and convert it to ONNX before deploying it with ONNX Runtime.
Production,ONNX,Backends

================================================================================

# Welcome to PyTorch Tutorials - Exporting a PyTorch model to ONNX using TorchDynamo backend and Running it using ONNX Runtime

Build a image classifier model in PyTorch and convert it to ONNX before deploying it with ONNX Runtime.

Production,ONNX,Backends

Extending the ONNX exporter operator support

Demonstrate end-to-end how to address unsupported operators in ONNX.
Production,ONNX,Backends

================================================================================

# Welcome to PyTorch Tutorials - Extending the ONNX exporter operator support

Demonstrate end-to-end how to address unsupported operators in ONNX.

Production,ONNX,Backends

Exporting a model with control flow to ONNX

Demonstrate how to handle control flow logic while exporting a PyTorch model to ONNX.
Production,ONNX,Backends

================================================================================

# Welcome to PyTorch Tutorials - Exporting a model with control flow to ONNX

Demonstrate how to handle control flow logic while exporting a PyTorch model to ONNX.

Production,ONNX,Backends

Reinforcement Learning (DQN)

Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym.
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Reinforcement Learning (DQN)

Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym.

Reinforcement-Learning

Reinforcement Learning (PPO) with TorchRL

Learn how to use PyTorch and TorchRL to train a Proximal Policy Optimization agent on the Inverted Pendulum task from Gym.
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Reinforcement Learning (PPO) with TorchRL

Learn how to use PyTorch and TorchRL to train a Proximal Policy Optimization agent on the Inverted Pendulum task from Gym.

Reinforcement-Learning

Train a Mario-playing RL Agent

Use PyTorch to train a Double Q-learning agent to play Mario.
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Train a Mario-playing RL Agent

Use PyTorch to train a Double Q-learning agent to play Mario.

Reinforcement-Learning

Recurrent DQN

Use TorchRL to train recurrent policies
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Recurrent DQN

Use TorchRL to train recurrent policies

Reinforcement-Learning

Code a DDPG Loss

Use TorchRL to code a DDPG Loss
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Code a DDPG Loss

Use TorchRL to code a DDPG Loss

Reinforcement-Learning

Writing your environment and transforms

Use TorchRL to code a Pendulum
Reinforcement-Learning

================================================================================

# Welcome to PyTorch Tutorials - Writing your environment and transforms

Use TorchRL to code a Pendulum

Reinforcement-Learning

Profiling PyTorch

Learn how to profile a PyTorch application

================================================================================

# Welcome to PyTorch Tutorials - Profiling PyTorch

Learn how to profile a PyTorch application

Profiling PyTorch

Introduction to Holistic Trace Analysis

_static/img/thumbnails/default.png

================================================================================

# Welcome to PyTorch Tutorials - Profiling PyTorch

Introduction to Holistic Trace Analysis

Profiling PyTorch

Trace Diff using Holistic Trace Analysis

_static/img/thumbnails/default.png

================================================================================

# Welcome to PyTorch Tutorials - Profiling PyTorch

Trace Diff using Holistic Trace Analysis

Building a Simple Performance Profiler with FX

Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics

================================================================================

# Welcome to PyTorch Tutorials - Building a Simple Performance Profiler with FX

Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics

(beta) Channels Last Memory Format in PyTorch

Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions.
Memory-Format,Best-Practice,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - (beta) Channels Last Memory Format in PyTorch

Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions.

Memory-Format,Best-Practice,Frontend-APIs

Using the PyTorch C++ Frontend

Walk through an end-to-end example of training a model with the C++ frontend by training a DCGAN – a kind of generative model – to generate images of MNIST digits.
Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Using the PyTorch C++ Frontend

Walk through an end-to-end example of training a model with the C++ frontend by training a DCGAN – a kind of generative model – to generate images of MNIST digits.

PyTorch Custom Operators Landing Page

This is the landing page for all things related to custom operators in PyTorch.
Extending-PyTorch,Frontend-APIs,C++,CUDA

================================================================================

# Welcome to PyTorch Tutorials - PyTorch Custom Operators Landing Page

This is the landing page for all things related to custom operators in PyTorch.

Extending-PyTorch,Frontend-APIs,C++,CUDA

Custom Python Operators

Create Custom Operators in Python. Useful for black-boxing a Python function for use with torch.compile.
Extending-PyTorch,Frontend-APIs,C++,CUDA

================================================================================

# Welcome to PyTorch Tutorials - Custom Python Operators

Create Custom Operators in Python. Useful for black-boxing a Python function for use with torch.compile.

Extending-PyTorch,Frontend-APIs,C++,CUDA

Compiled Autograd: Capturing a larger backward graph for ``torch.compile``

Learn how to use compiled autograd to capture a larger backward graph.
Model-Optimization,CUDA

================================================================================

# Welcome to PyTorch Tutorials - Compiled Autograd: Capturing a larger backward graph for ``torch.compile``

Learn how to use compiled autograd to capture a larger backward graph.

Model-Optimization,CUDA

Custom C++ and CUDA Operators

How to extend PyTorch with custom C++ and CUDA operators.
Extending-PyTorch,Frontend-APIs,C++,CUDA

================================================================================

# Welcome to PyTorch Tutorials - Custom C++ and CUDA Operators

How to extend PyTorch with custom C++ and CUDA operators.

Extending-PyTorch,Frontend-APIs,C++,CUDA

Autograd in C++ Frontend

The autograd package helps build flexible and dynamic neural netorks. In this tutorial, explore several examples of doing autograd in PyTorch C++ frontend
Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Autograd in C++ Frontend

The autograd package helps build flexible and dynamic neural netorks. In this tutorial, explore several examples of doing autograd in PyTorch C++ frontend

Registering a Dispatched Operator in C++

The dispatcher is an internal component of PyTorch which is responsible for figuring out what code should actually get run when you call a function like torch::add.
Extending-PyTorch,Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Registering a Dispatched Operator in C++

The dispatcher is an internal component of PyTorch which is responsible for figuring out what code should actually get run when you call a function like torch::add.

Extending-PyTorch,Frontend-APIs,C++

Extending Dispatcher For a New Backend in C++

Learn how to extend the dispatcher to add a new device living outside of the pytorch/pytorch repo and maintain it to keep in sync with native PyTorch devices.
Extending-PyTorch,Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Extending Dispatcher For a New Backend in C++

Learn how to extend the dispatcher to add a new device living outside of the pytorch/pytorch repo and maintain it to keep in sync with native PyTorch devices.

Extending-PyTorch,Frontend-APIs,C++

Facilitating New Backend Integration by PrivateUse1

Learn how to integrate a new backend living outside of the pytorch/pytorch repo and maintain it to keep in sync with the native PyTorch backend.
Extending-PyTorch,Frontend-APIs,C++

================================================================================

# Welcome to PyTorch Tutorials - Facilitating New Backend Integration by PrivateUse1

Learn how to integrate a new backend living outside of the pytorch/pytorch repo and maintain it to keep in sync with the native PyTorch backend.

Extending-PyTorch,Frontend-APIs,C++

Custom Function Tutorial: Double Backward

Learn how to write a custom autograd Function that supports double backward.
Extending-PyTorch,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Custom Function Tutorial: Double Backward

Learn how to write a custom autograd Function that supports double backward.

Extending-PyTorch,Frontend-APIs

Custom Function Tutorial: Fusing Convolution and Batch Norm

Learn how to create a custom autograd Function that fuses batch norm into a convolution to improve memory usage.
Extending-PyTorch,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Custom Function Tutorial: Fusing Convolution and Batch Norm

Learn how to create a custom autograd Function that fuses batch norm into a convolution to improve memory usage.

Extending-PyTorch,Frontend-APIs

Forward-mode Automatic Differentiation

Learn how to use forward-mode automatic differentiation.
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Forward-mode Automatic Differentiation

Learn how to use forward-mode automatic differentiation.

Jacobians, Hessians, hvp, vhp, and more

Learn how to compute advanced autodiff quantities using torch.func
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Jacobians, Hessians, hvp, vhp, and more

Learn how to compute advanced autodiff quantities using torch.func

Model Ensembling

Learn how to ensemble models using torch.vmap
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Model Ensembling

Learn how to ensemble models using torch.vmap

Per-Sample-Gradients

Learn how to compute per-sample-gradients using torch.func
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Per-Sample-Gradients

Learn how to compute per-sample-gradients using torch.func

Neural Tangent Kernels

Learn how to compute neural tangent kernels using torch.func
Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - Neural Tangent Kernels

Learn how to compute neural tangent kernels using torch.func

Performance Profiling in PyTorch

Learn how to use the PyTorch Profiler to benchmark your module's performance.
Model-Optimization,Best-Practice,Profiling

================================================================================

# Welcome to PyTorch Tutorials - Performance Profiling in PyTorch

Learn how to use the PyTorch Profiler to benchmark your module's performance.

Model-Optimization,Best-Practice,Profiling

Performance Profiling in TensorBoard

Learn how to use the TensorBoard plugin to profile and analyze your model's performance.
Model-Optimization,Best-Practice,Profiling,TensorBoard

================================================================================

# Welcome to PyTorch Tutorials - Performance Profiling in TensorBoard

Learn how to use the TensorBoard plugin to profile and analyze your model's performance.

Model-Optimization,Best-Practice,Profiling,TensorBoard

Hyperparameter Tuning Tutorial

Learn how to use Ray Tune to find the best performing set of hyperparameters for your model.
Model-Optimization,Best-Practice

================================================================================

# Welcome to PyTorch Tutorials - Hyperparameter Tuning Tutorial

Learn how to use Ray Tune to find the best performing set of hyperparameters for your model.

Model-Optimization,Best-Practice

Parametrizations Tutorial

Learn how to use torch.nn.utils.parametrize to put constraints on your parameters (e.g. make them orthogonal, symmetric positive definite, low-rank...)
Model-Optimization,Best-Practice

================================================================================

# Welcome to PyTorch Tutorials - Parametrizations Tutorial

Learn how to use torch.nn.utils.parametrize to put constraints on your parameters (e.g. make them orthogonal, symmetric positive definite, low-rank...)

Model-Optimization,Best-Practice

Pruning Tutorial

Learn how to use torch.nn.utils.prune to sparsify your neural networks, and how to extend it to implement your own custom pruning technique.
Model-Optimization,Best-Practice

================================================================================

# Welcome to PyTorch Tutorials - Pruning Tutorial

Learn how to use torch.nn.utils.prune to sparsify your neural networks, and how to extend it to implement your own custom pruning technique.

Model-Optimization,Best-Practice

How to save memory by fusing the optimizer step into the backward pass

Learn a memory-saving technique through fusing the optimizer step into the backward pass using memory snapshots.
Model-Optimization,Best-Practice,CUDA,Frontend-APIs

================================================================================

# Welcome to PyTorch Tutorials - How to save memory by fusing the optimizer step into the backward pass

Learn a memory-saving technique through fusing the optimizer step into the backward pass using memory snapshots.

Model-Optimization,Best-Practice,CUDA,Frontend-APIs

(beta) Accelerating BERT with semi-structured sparsity

Train BERT, prune it to be 2:4 sparse, and then accelerate it to achieve 2x inference speedups with semi-structured sparsity and torch.compile.
Text,Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - (beta) Accelerating BERT with semi-structured sparsity

Train BERT, prune it to be 2:4 sparse, and then accelerate it to achieve 2x inference speedups with semi-structured sparsity and torch.compile.

Text,Model-Optimization

Multi-Objective Neural Architecture Search with Ax

Learn how to use Ax to search over architectures find optimal tradeoffs between accuracy and latency.
Model-Optimization,Best-Practice,Ax,TorchX

================================================================================

# Welcome to PyTorch Tutorials - Multi-Objective Neural Architecture Search with Ax

Learn how to use Ax to search over architectures find optimal tradeoffs between accuracy and latency.

Model-Optimization,Best-Practice,Ax,TorchX

torch.compile Tutorial

Speed up your models with minimal code changes using torch.compile, the latest PyTorch compiler solution.
Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - torch.compile Tutorial

Speed up your models with minimal code changes using torch.compile, the latest PyTorch compiler solution.

Building a Convolution/Batch Norm fuser in torch.compile

Build a simple pattern matcher pass that fuses batch norm into convolution to improve performance during inference.
Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - Building a Convolution/Batch Norm fuser in torch.compile

Build a simple pattern matcher pass that fuses batch norm into convolution to improve performance during inference.

Inductor CPU Backend Debugging and Profiling

Learn the usage, debugging and performance profiling for ``torch.compile`` with Inductor CPU backend.
Model-Optimization

================================================================================

# Welcome to PyTorch Tutorials - Inductor CPU Backend Debugging and Profiling

Learn the usage, debugging and performance profiling for ``torch.compile`` with Inductor CPU backend.

(beta) Implementing High-Performance Transformers with SCALED DOT PRODUCT ATTENTION

This tutorial explores the new torch.nn.functional.scaled_dot_product_attention and how it can be used to construct Transformer components.
Model-Optimization,Attention,Transformer

================================================================================

# Welcome to PyTorch Tutorials - (beta) Implementing High-Performance Transformers with SCALED DOT PRODUCT ATTENTION

This tutorial explores the new torch.nn.functional.scaled_dot_product_attention and how it can be used to construct Transformer components.

Model-Optimization,Attention,Transformer

Knowledge Distillation in Convolutional Neural Networks

Learn how to improve the accuracy of lightweight models using more powerful models as teachers.
Model-Optimization,Image/Video

================================================================================

# Welcome to PyTorch Tutorials - Knowledge Distillation in Convolutional Neural Networks

Learn how to improve the accuracy of lightweight models using more powerful models as teachers.

Model-Optimization,Image/Video

Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()

This tutorial goes over recommended best practices for implementing Transformers with native PyTorch.
Transformer

================================================================================

# Welcome to PyTorch Tutorials - Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile()

This tutorial goes over recommended best practices for implementing Transformers with native PyTorch.

PyTorch Distributed Overview

Briefly go over all concepts and features in the distributed package. Use this document to find the distributed training technology that can best serve your application.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - PyTorch Distributed Overview

Briefly go over all concepts and features in the distributed package. Use this document to find the distributed training technology that can best serve your application.

Parallel-and-Distributed-Training

Distributed Data Parallel in PyTorch - Video Tutorials

This series of video tutorials walks you through distributed training in PyTorch via DDP.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Distributed Data Parallel in PyTorch - Video Tutorials

This series of video tutorials walks you through distributed training in PyTorch via DDP.

Parallel-and-Distributed-Training

Single-Machine Model Parallel Best Practices

Learn how to implement model parallel, a distributed training technique which splits a single model onto different GPUs, rather than replicating the entire model on each GPU
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Single-Machine Model Parallel Best Practices

Learn how to implement model parallel, a distributed training technique which splits a single model onto different GPUs, rather than replicating the entire model on each GPU

Parallel-and-Distributed-Training

Getting Started with Distributed Data Parallel

Learn the basics of when to use distributed data paralle versus data parallel and work through an example to set it up.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Getting Started with Distributed Data Parallel

Learn the basics of when to use distributed data paralle versus data parallel and work through an example to set it up.

Parallel-and-Distributed-Training

Writing Distributed Applications with PyTorch

Set up the distributed package of PyTorch, use the different communication strategies, and go over some the internals of the package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Writing Distributed Applications with PyTorch

Set up the distributed package of PyTorch, use the different communication strategies, and go over some the internals of the package.

Parallel-and-Distributed-Training

Large Scale Transformer model training with Tensor Parallel

Learn how to train large models with Tensor Parallel package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Large Scale Transformer model training with Tensor Parallel

Learn how to train large models with Tensor Parallel package.

Parallel-and-Distributed-Training

Customize Process Group Backends Using Cpp Extensions

Extend ProcessGroup with custom collective communication implementations.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Customize Process Group Backends Using Cpp Extensions

Extend ProcessGroup with custom collective communication implementations.

Parallel-and-Distributed-Training

Getting Started with Distributed RPC Framework

Learn how to build distributed training using the torch.distributed.rpc package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Getting Started with Distributed RPC Framework

Learn how to build distributed training using the torch.distributed.rpc package.

Parallel-and-Distributed-Training

Implementing a Parameter Server Using Distributed RPC Framework

Walk through a through a simple example of implementing a parameter server using PyTorch’s Distributed RPC framework.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Implementing a Parameter Server Using Distributed RPC Framework

Walk through a through a simple example of implementing a parameter server using PyTorch’s Distributed RPC framework.

Parallel-and-Distributed-Training

Introduction to Distributed Pipeline Parallelism

Demonstrate how to implement pipeline parallelism using torch.distributed.pipelining
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Introduction to Distributed Pipeline Parallelism

Demonstrate how to implement pipeline parallelism using torch.distributed.pipelining

Parallel-and-Distributed-Training

Implementing Batch RPC Processing Using Asynchronous Executions

Learn how to use rpc.functions.async_execution to implement batch RPC
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Implementing Batch RPC Processing Using Asynchronous Executions

Learn how to use rpc.functions.async_execution to implement batch RPC

Parallel-and-Distributed-Training

Combining Distributed DataParallel with Distributed RPC Framework

Walk through a through a simple example of how to combine distributed data parallelism with distributed model parallelism.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Combining Distributed DataParallel with Distributed RPC Framework

Walk through a through a simple example of how to combine distributed data parallelism with distributed model parallelism.

Parallel-and-Distributed-Training

Getting Started with Fully Sharded Data Parallel (FSDP2)

Learn how to train models with Fully Sharded Data Parallel (fully_shard) package.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Getting Started with Fully Sharded Data Parallel (FSDP2)

Learn how to train models with Fully Sharded Data Parallel (fully_shard) package.

Parallel-and-Distributed-Training

Introduction to Libuv TCPStore Backend

TCPStore now uses a new server backend for faster connection and better scalability.
Parallel-and-Distributed-Training

================================================================================

# Welcome to PyTorch Tutorials - Introduction to Libuv TCPStore Backend

TCPStore now uses a new server backend for faster connection and better scalability.

Parallel-and-Distributed-Training

Exporting to ExecuTorch Tutorial

Learn about how to use ExecuTorch, a unified ML stack for lowering PyTorch models to edge devices.

================================================================================

# Welcome to PyTorch Tutorials - Exporting to ExecuTorch Tutorial

Learn about how to use ExecuTorch, a unified ML stack for lowering PyTorch models to edge devices.

Running an ExecuTorch Model in C++ Tutorial

Learn how to load and execute an ExecuTorch model in C++

================================================================================

# Welcome to PyTorch Tutorials - Running an ExecuTorch Model in C++ Tutorial

Learn how to load and execute an ExecuTorch model in C++

Using the ExecuTorch SDK to Profile a Model

Explore how to use the ExecuTorch SDK to profile, debug, and visualize ExecuTorch models

================================================================================

# Welcome to PyTorch Tutorials - Using the ExecuTorch SDK to Profile a Model

Explore how to use the ExecuTorch SDK to profile, debug, and visualize ExecuTorch models

Building an ExecuTorch iOS Demo App

Explore how to set up the ExecuTorch iOS Demo App, which uses the MobileNet v3 model to process live camera images leveraging three different backends: XNNPACK, Core ML, and Metal Performance Shaders (MPS).

================================================================================

# Welcome to PyTorch Tutorials - Building an ExecuTorch iOS Demo App

Explore how to set up the ExecuTorch iOS Demo App, which uses the MobileNet v3 model to process live camera images leveraging three different backends: XNNPACK, Core ML, and Metal Performance Shaders (MPS).

Building an ExecuTorch Android Demo App

Learn how to set up the ExecuTorch Android Demo App for image segmentation tasks using the DeepLab v3 model and XNNPACK FP32 backend.

================================================================================

# Welcome to PyTorch Tutorials - Building an ExecuTorch Android Demo App

Learn how to set up the ExecuTorch Android Demo App for image segmentation tasks using the DeepLab v3 model and XNNPACK FP32 backend.

Lowering a Model as a Delegate

Learn to accelerate your program using ExecuTorch by applying delegates through three methods: lowering the whole module, composing it with another module, and partitioning parts of a module.

================================================================================

# Welcome to PyTorch Tutorials - Lowering a Model as a Delegate

Learn to accelerate your program using ExecuTorch by applying delegates through three methods: lowering the whole module, composing it with another module, and partitioning parts of a module.

Introduction to TorchRec

TorchRec is a PyTorch domain library built to provide common sparsity & parallelism primitives needed for large-scale recommender systems.
TorchRec,Recommender

================================================================================

# Welcome to PyTorch Tutorials - Introduction to TorchRec

TorchRec is a PyTorch domain library built to provide common sparsity & parallelism primitives needed for large-scale recommender systems.

Exploring TorchRec sharding

This tutorial covers the sharding schemes of embedding tables by using EmbeddingPlanner and DistributedModelParallel API.
TorchRec,Recommender

================================================================================

# Welcome to PyTorch Tutorials - Exploring TorchRec sharding

This tutorial covers the sharding schemes of embedding tables by using EmbeddingPlanner and DistributedModelParallel API.

================================================================================

# Welcome to PyTorch Tutorials - Additional Resources

Examples of PyTorch
A set of examples around PyTorch in Vision, Text, Reinforcement Learning that you can incorporate in your existing work.
Check Out Examples

================================================================================

# Welcome to PyTorch Tutorials - Examples of PyTorch

A set of examples around PyTorch in Vision, Text, Reinforcement Learning that you can incorporate in your existing work.

Run Tutorials on Google Colab
Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.

================================================================================

# Welcome to PyTorch Tutorials - Run Tutorials on Google Colab

Learn how to copy tutorial data into Google Drive so that you can run tutorials on Google Colab.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch

Created On: Dec 23, 2016 | Last Updated On: Mar 10, 2025

The torch package contains data structures for multi-dimensional
tensors and defines mathematical operations over these tensors.
Additionally, it provides many utilities for efficient serialization of
Tensors and arbitrary types, and other useful utilities.

It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability >= 3.0.

================================================================================

# torch - Tensors (Part 1)

Table:
 | Returns True if  is a PyTorch tensor.
is_storage | Returns True if  is a PyTorch storage object.
is_complex | Returns True if the data type of  is a complex data type i.e., one of torch.complex64, and torch.complex128.
 | Returns True if the  is a conjugated tensor, i.e. its conjugate bit is set to .
is_floating_point | Returns True if the data type of  is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.
is_nonzero | Returns True if the  is a single element tensor which is not equal to zero after type conversions.
set_default_dtype | Sets the default floating point dtype to .
get_default_dtype | Get the current default floating point torch.dtype.
set_default_device | Sets the default torch.Tensor to be allocated on .
get_default_device | Gets the default torch.Tensor to be allocated on
set_default_tensor_type | 
 | Returns the total number of elements in the  tensor.
set_printoptions | Set options for printing.
set_flush_denormal | Disables denormal floating numbers on CPU.

Returns True if  is a PyTorch tensor.

Returns True if  is a PyTorch storage object.

================================================================================

# torch - Tensors (Part 2)

Returns True if the data type of  is a complex data type i.e., one of torch.complex64, and torch.complex128.

Returns True if the  is a conjugated tensor, i.e. its conjugate bit is set to .

Returns True if the data type of  is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.

Returns True if the  is a single element tensor which is not equal to zero after type conversions.

Sets the default floating point dtype to .

Get the current default floating point torch.dtype.

Sets the default torch.Tensor to be allocated on .

Gets the default torch.Tensor to be allocated on

set_default_tensor_type

Returns the total number of elements in the  tensor.

Set options for printing.

Disables denormal floating numbers on CPU.

================================================================================

# torch - Creation Ops (Part 1)

Random sampling creation ops are listed under Random sampling and
include:
torch.rand()
torch.rand_like()
torch.randn()
torch.randn_like()
torch.randint()
torch.randint_like()
torch.randperm()
You may also use torch.empty() with the In-place random sampling
methods to create torch.Tensor s with values sampled from a broader
range of distributions.

================================================================================

# torch - Creation Ops (Part 2)

Table:
 | Constructs a tensor with no autograd history (also known as a "leaf tensor", see Autograd mechanics) by copying .
sparse_coo_tensor | Constructs a sparse tensor in COO(rdinate) format with specified values at the given .
sparse_csr_tensor | Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices.
sparse_csc_tensor | Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given ccol_indices and row_indices.
sparse_bsr_tensor | Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given crow_indices and col_indices.
sparse_bsc_tensor | Constructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given ccol_indices and row_indices.
 | Converts  to a tensor.
 | Converts  into a tensor, sharing data and preserving autograd history if possible.
as_strided | Create a view of an existing torch.Tensor  with specified ,  and storage_offset.
 | Creates a CPU tensor with a storage backed by a memory-mapped file.
from_numpy | Creates a  from a numpy.ndarray.
from_dlpack | Converts a tensor from an external library into a torch.Tensor.
frombuffer | Creates a 1-dimensional  from an object that implements the Python buffer protocol.
 | Returns a tensor filled with the scalar value , with the shape defined by the variable argument .
zeros_like | Returns a tensor filled with the scalar value , with the same size as .
 | Returns a tensor filled with the scalar value , with the shape defined by the variable argument .
 | Returns a tensor filled with the scalar value , with the same size as .
 | Returns a 1-D tensor of size \left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil with values from the interval   taken with common difference  beginning from .
 | Returns a 1-D tensor of size \left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1 with values from  to  with step .
 | Creates a one-dimensional tensor of size  whose values are evenly spaced from  to , inclusive.
 | Creates a one-dimensional tensor of size  whose values are evenly spaced from {{\text{{base}}}}^{{\text{{start}}}} to {{\text{{base}}}}^{{\text{{end}}}}, inclusive, on a logarithmic scale with base .
 | Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.
 | Returns a tensor filled with uninitialized data.
empty_like | Returns an uninitialized tensor with the same size as .
empty_strided | Creates a tensor with the specified  and  and filled with undefined data.
 | Creates a tensor of size  filled with fill_value.
 | Returns a tensor with the same size as  filled with fill_value.
quantize_per_tensor | Converts a float tensor to a quantized tensor with given scale and zero point.
quantize_per_channel | Converts a float tensor to a per-channel quantized tensor with given scales and zero points.
dequantize | Returns an fp32 Tensor by dequantizing a quantized Tensor
 | Constructs a complex tensor with its real part equal to  and its imaginary part equal to .
 | Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value  and angle .
 | Computes the Heaviside step function for each element in .

================================================================================

# torch - Creation Ops (Part 3)

Constructs a tensor with no autograd history (also known as a "leaf tensor", see Autograd mechanics) by copying .

Constructs a sparse tensor in COO(rdinate) format with specified values at the given .

Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices.

Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given ccol_indices and row_indices.

Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given crow_indices and col_indices.

Constructs a sparse tensor in BSC (Block Compressed Sparse Column)) with specified 2-dimensional blocks at the given ccol_indices and row_indices.

Converts  to a tensor.

Converts  into a tensor, sharing data and preserving autograd history if possible.

Create a view of an existing torch.Tensor  with specified ,  and storage_offset.

Creates a CPU tensor with a storage backed by a memory-mapped file.

Creates a  from a numpy.ndarray.

Converts a tensor from an external library into a torch.Tensor.

Creates a 1-dimensional  from an object that implements the Python buffer protocol.

================================================================================

# torch - Creation Ops (Part 4)

Returns a tensor filled with the scalar value , with the shape defined by the variable argument .

Returns a tensor filled with the scalar value , with the same size as .

Returns a tensor filled with the scalar value , with the shape defined by the variable argument .

Returns a tensor filled with the scalar value , with the same size as .

Returns a 1-D tensor of size \left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil with values from the interval   taken with common difference  beginning from .

Returns a 1-D tensor of size \left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1 with values from  to  with step .

Creates a one-dimensional tensor of size  whose values are evenly spaced from  to , inclusive.

Creates a one-dimensional tensor of size  whose values are evenly spaced from {{\text{{base}}}}^{{\text{{start}}}} to {{\text{{base}}}}^{{\text{{end}}}}, inclusive, on a logarithmic scale with base .

Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.

Returns a tensor filled with uninitialized data.

Returns an uninitialized tensor with the same size as .

Creates a tensor with the specified  and  and filled with undefined data.

================================================================================

# torch - Creation Ops (Part 5)

Creates a tensor of size  filled with fill_value.

Returns a tensor with the same size as  filled with fill_value.

Converts a float tensor to a quantized tensor with given scale and zero point.

Converts a float tensor to a per-channel quantized tensor with given scales and zero points.

Returns an fp32 Tensor by dequantizing a quantized Tensor

Constructs a complex tensor with its real part equal to  and its imaginary part equal to .

Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value  and angle .

Computes the Heaviside step function for each element in .

================================================================================

# torch - Indexing, Slicing, Joining, Mutating Ops (Part 1)

Table:
 | Returns a view of the tensor conjugated and with the last two dimensions transposed.
 | Returns a tensor containing the indices of all non-zero elements of .
 | Concatenates the given sequence of tensors in  in the given dimension.
 | Alias of torch.cat().
concatenate | Alias of torch.cat().
 | Returns a view of  with a flipped conjugate bit.
 | Attempts to split a tensor into the specified number of chunks.
 | Splits , a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.
column_stack | Creates a new tensor by horizontally stacking the tensors in .
 | Stack tensors in sequence depthwise (along third axis).
 | Gathers values along an axis specified by .
 | Splits , a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.
 | Stack tensors in sequence horizontally (column wise).
 | See index_add_() for function description.
index_copy | See index_add_() for function description.
index_reduce | See index_reduce_() for function description.
index_select | Returns a new tensor which indexes the  tensor along dimension  using the entries in  which is a LongTensor.
masked_select | Returns a new 1-D tensor which indexes the  tensor according to the boolean mask  which is a BoolTensor.
 | Moves the dimension(s) of  at the position(s) in  to the position(s) in destination.
 | Alias for torch.movedim().
 | Returns a new tensor that is a narrowed version of  tensor.
narrow_copy | Same as Tensor.narrow() except this returns a copy rather than shared storage.
 | Returns a view of the original tensor  with its dimensions permuted.
 | Returns a tensor with the same data and number of elements as , but with the specified shape.
 | Alias of torch.vstack().
 | Slices the  tensor along the selected dimension at the given index.
 | Out-of-place version of torch.Tensor.scatter_()
diagonal_scatter | Embeds the values of the  tensor into  along the diagonal elements of , with respect to  and .
select_scatter | Embeds the values of the  tensor into  at the given index.
slice_scatter | Embeds the values of the  tensor into  at the given dimension.
scatter_add | Out-of-place version of torch.Tensor.scatter_add_()
scatter_reduce | Out-of-place version of torch.Tensor.scatter_reduce_()
 | Splits the tensor into chunks.
 | Returns a tensor with all specified dimensions of  of size  removed.
 | Concatenates a sequence of tensors along a new dimension.
 | Alias for torch.transpose().
 | Alias for torch.transpose().
 | Expects  to be <= 2-D tensor and transposes dimensions 0 and 1.
 | Returns a new tensor with the elements of  at the given indices.
take_along_dim | Selects values from  at the 1-dimensional indices from  along the given .
tensor_split | Splits a tensor into multiple sub-tensors, all of which are views of , along dimension  according to the indices or number of sections specified by indices_or_sections.
 | Constructs a tensor by repeating the elements of .
 | Returns a tensor that is a transposed version of .
 | Removes a tensor dimension.
unravel_index | Converts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.
 | Returns a new tensor with a dimension of size one inserted at the specified position.
 | Splits , a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.
 | Stack tensors in sequence vertically (row wise).
 | Return a tensor of elements selected from either  or , depending on .

================================================================================

# torch - Indexing, Slicing, Joining, Mutating Ops (Part 2)

Returns a view of the tensor conjugated and with the last two dimensions transposed.

Returns a tensor containing the indices of all non-zero elements of .

Concatenates the given sequence of tensors in  in the given dimension.

Alias of torch.cat().

Alias of torch.cat().

Returns a view of  with a flipped conjugate bit.

Attempts to split a tensor into the specified number of chunks.

Splits , a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections.

Creates a new tensor by horizontally stacking the tensors in .

Stack tensors in sequence depthwise (along third axis).

Gathers values along an axis specified by .

Splits , a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections.

Stack tensors in sequence horizontally (column wise).

See index_add_() for function description.

See index_add_() for function description.

See index_reduce_() for function description.

Returns a new tensor which indexes the  tensor along dimension  using the entries in  which is a LongTensor.

Returns a new 1-D tensor which indexes the  tensor according to the boolean mask  which is a BoolTensor.

================================================================================

# torch - Indexing, Slicing, Joining, Mutating Ops (Part 3)

Moves the dimension(s) of  at the position(s) in  to the position(s) in destination.

Alias for torch.movedim().

Returns a new tensor that is a narrowed version of  tensor.

Same as Tensor.narrow() except this returns a copy rather than shared storage.

Returns a view of the original tensor  with its dimensions permuted.

Returns a tensor with the same data and number of elements as , but with the specified shape.

Alias of torch.vstack().

Slices the  tensor along the selected dimension at the given index.

Out-of-place version of torch.Tensor.scatter_()

Embeds the values of the  tensor into  along the diagonal elements of , with respect to  and .

Embeds the values of the  tensor into  at the given index.

Embeds the values of the  tensor into  at the given dimension.

Out-of-place version of torch.Tensor.scatter_add_()

Out-of-place version of torch.Tensor.scatter_reduce_()

Splits the tensor into chunks.

Returns a tensor with all specified dimensions of  of size  removed.

Concatenates a sequence of tensors along a new dimension.

Alias for torch.transpose().

Alias for torch.transpose().

Expects  to be <= 2-D tensor and transposes dimensions 0 and 1.

================================================================================

# torch - Indexing, Slicing, Joining, Mutating Ops (Part 4)

Returns a new tensor with the elements of  at the given indices.

Selects values from  at the 1-dimensional indices from  along the given .

Splits a tensor into multiple sub-tensors, all of which are views of , along dimension  according to the indices or number of sections specified by indices_or_sections.

Constructs a tensor by repeating the elements of .

Returns a tensor that is a transposed version of .

Removes a tensor dimension.

Converts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.

Returns a new tensor with a dimension of size one inserted at the specified position.

Splits , a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections.

Stack tensors in sequence vertically (row wise).

Return a tensor of elements selected from either  or , depending on .

================================================================================

# torch - Accelerators (Part 1)

Within the PyTorch repo, we define an “Accelerator” as a torch.device that is being used
alongside a CPU to speed up computation. These device use an asynchronous execution scheme,
using torch.Stream and torch.Event as their main way to perform synchronization.
We also assume that only one such accelerator can be available at once on a given host. This allows
us to use the current accelerator as the default device for relevant concepts such as pinned memory,
Stream device_type, FSDP, etc.

As of today, accelerator devices are (in no particular order) , ,
, , “HPU”, and PrivateUse1 (many device not in the PyTorch repo itself).

================================================================================

# torch - Accelerators (Part 2)

Many tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading
or intra-op parallelism), it is thus important to delay as much as possible any
operation that would prevent further forks. This is especially important here as most accelerator’s initialization has such effect.
In practice, you should keep in mind that checking torch.accelerator.current_accelerator()
is a compile-time check by default, it is thus always fork-safe.
On the contrary, passing the check_available=True flag to this function or calling
torch.accelerator.is_available() will usually prevent later fork.

Some backends provide an experimental opt-in option to make the runtime availability
check fork-safe. When using the CUDA device PYTORCH_NVML_BASED_CUDA_CHECK=1 can be
used for example.

Table:
 | An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.
 | Query and record Stream status to identify or control dependencies across Stream and measure timing.

An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.

================================================================================

# torch - Accelerators (Part 3)

Query and record Stream status to identify or control dependencies across Stream and measure timing.

================================================================================

# torch - Generators

Table:
 | Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.

Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.

================================================================================

# torch - Random sampling (Part 1)

Table:
 | Sets the seed for generating random numbers to a non-deterministic random number on all devices.
manual_seed | Sets the seed for generating random numbers on all devices.
initial_seed | Returns the initial seed for generating random numbers as a Python .
get_rng_state | Returns the random number generator state as a torch.ByteTensor.
set_rng_state | Sets the random number generator state.

Sets the seed for generating random numbers to a non-deterministic random number on all devices.

Sets the seed for generating random numbers on all devices.

Returns the initial seed for generating random numbers as a Python .

Returns the random number generator state as a torch.ByteTensor.

Sets the random number generator state.

default_generator    torch.Generator

================================================================================

# torch - Random sampling (Part 2)

Table:
 | Draws binary random numbers (0 or 1) from a Bernoulli distribution.
multinomial | Returns a tensor where each row contains num_samples indices sampled from the multinomial (a stricter definition would be multivariate, refer to torch.distributions.multinomial.Multinomial for more details) probability distribution located in the corresponding row of tensor .
 | Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.
 | Returns a tensor of the same size as  with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in  i.e.,
 | Returns a tensor filled with random numbers from a uniform distribution on the interval
 | Returns a tensor with the same size as  that is filled with random numbers from a uniform distribution on the interval .
 | Returns a tensor filled with random integers generated uniformly between  (inclusive) and  (exclusive).
randint_like | Returns a tensor with the same shape as Tensor  filled with random integers generated uniformly between  (inclusive) and  (exclusive).
 | Returns a tensor filled with random numbers from a normal distribution with mean  and variance  (also called the standard normal distribution).
randn_like | Returns a tensor with the same size as  that is filled with random numbers from a normal distribution with mean 0 and variance 1.
 | Returns a random permutation of integers from  to   .

================================================================================

# torch - Random sampling (Part 3)

Draws binary random numbers (0 or 1) from a Bernoulli distribution.

Returns a tensor where each row contains num_samples indices sampled from the multinomial (a stricter definition would be multivariate, refer to torch.distributions.multinomial.Multinomial for more details) probability distribution located in the corresponding row of tensor .

Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.

Returns a tensor of the same size as  with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in  i.e.,

Returns a tensor filled with random numbers from a uniform distribution on the interval

Returns a tensor with the same size as  that is filled with random numbers from a uniform distribution on the interval .

Returns a tensor filled with random integers generated uniformly between  (inclusive) and  (exclusive).

Returns a tensor with the same shape as Tensor  filled with random integers generated uniformly between  (inclusive) and  (exclusive).

================================================================================

# torch - Random sampling (Part 4)

Returns a tensor filled with random numbers from a normal distribution with mean  and variance  (also called the standard normal distribution).

Returns a tensor with the same size as  that is filled with random numbers from a normal distribution with mean 0 and variance 1.

Returns a random permutation of integers from  to   .

================================================================================

# torch - In-place random sampling (Part 1)

There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:

List:
torch.Tensor.bernoulli_() - in-place version of torch.bernoulli()
torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution
torch.Tensor.exponential_() - numbers drawn from the exponential distribution
torch.Tensor.geometric_() - elements drawn from the geometric distribution
torch.Tensor.log_normal_() - samples from the log-normal distribution
torch.Tensor.normal_() - in-place version of torch.normal()
torch.Tensor.random_() - numbers sampled from the discrete uniform distribution
torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution

torch.Tensor.bernoulli_() - in-place version of torch.bernoulli()

torch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution

torch.Tensor.exponential_() - numbers drawn from the exponential distribution

torch.Tensor.geometric_() - elements drawn from the geometric distribution

torch.Tensor.log_normal_() - samples from the log-normal distribution

torch.Tensor.normal_() - in-place version of torch.normal()

================================================================================

# torch - In-place random sampling (Part 2)

torch.Tensor.random_() - numbers sampled from the discrete uniform distribution

torch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution

================================================================================

# torch - Quasi-random sampling

Table:
quasirandom.SobolEngine | The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.

quasirandom.SobolEngine

The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.

================================================================================

# torch - Serialization

Table:
 | Saves an object to a disk file.
 | Loads an object saved with torch.save() from a file.

Saves an object to a disk file.

Loads an object saved with torch.save() from a file.

================================================================================

# torch - Parallelism

Table:
get_num_threads | Returns the number of threads used for parallelizing CPU operations
set_num_threads | Sets the number of threads used for intraop parallelism on CPU.
get_num_interop_threads | Returns the number of threads used for inter-op parallelism on CPU (e.g.
set_num_interop_threads | Sets the number of threads used for interop parallelism (e.g.

Returns the number of threads used for parallelizing CPU operations

Sets the number of threads used for intraop parallelism on CPU.

get_num_interop_threads

Returns the number of threads used for inter-op parallelism on CPU (e.g.

set_num_interop_threads

Sets the number of threads used for interop parallelism (e.g.

================================================================================

# torch - Locally disabling gradient computation (Part 1)

The context managers torch.no_grad(), torch.enable_grad(), and
torch.set_grad_enabled() are helpful for locally disabling and enabling
gradient computation. See Locally disabling gradient computation for more details on
their usage.  These context managers are thread local, so they won’t
work if you send work to another thread using the  module, etc.

Code example:
requires_grad
 
        
requires_grad


  
 set_grad_enabled
        
requires_grad


set_grad_enabled  # this can also be used as a function
    
requires_grad


set_grad_enabled
    
requires_grad

Table:
 | Context-manager that disables gradient calculation.
enable_grad | Context-manager that enables gradient calculation.
autograd.grad_mode.set_grad_enabled | Context-manager that sets gradient calculation on or off.
is_grad_enabled | Returns True if grad mode is currently enabled.
autograd.grad_mode.inference_mode | Context-manager that enables or disables inference mode.
is_inference_mode_enabled | Returns True if inference mode is currently enabled.

Context-manager that disables gradient calculation.

Context-manager that enables gradient calculation.

autograd.grad_mode.set_grad_enabled

================================================================================

# torch - Locally disabling gradient computation (Part 2)

Context-manager that sets gradient calculation on or off.

Returns True if grad mode is currently enabled.

autograd.grad_mode.inference_mode

Context-manager that enables or disables inference mode.

is_inference_mode_enabled

Returns True if inference mode is currently enabled.

================================================================================

# torch - Constants

Table:
A floating-point positive infinity. Alias for .
A floating-point “not a number” value. This value is not a legal number. Alias for .

A floating-point positive infinity. Alias for .

A floating-point “not a number” value. This value is not a legal number. Alias for .

================================================================================

# torch - Pointwise Ops (Part 1)

Table:
 | Computes the absolute value of each element in .
 | Alias for torch.abs()
 | Computes the inverse cosine of each element in .
 | Alias for torch.acos().
 | Returns a new tensor with the inverse hyperbolic cosine of the elements of .
 | Alias for torch.acosh().
 | Adds , scaled by , to .
 | Performs the element-wise division of  by , multiplies the result by the scalar  and adds it to .
 | Performs the element-wise multiplication of  by , multiplies the result by the scalar  and adds it to .
 | Computes the element-wise angle (in radians) of the given  tensor.
 | Returns a new tensor with the arcsine of the elements of .
 | Alias for torch.asin().
 | Returns a new tensor with the inverse hyperbolic sine of the elements of .
 | Alias for torch.asinh().
 | Returns a new tensor with the arctangent of the elements of .
 | Alias for torch.atan().
 | Returns a new tensor with the inverse hyperbolic tangent of the elements of .
 | Alias for torch.atanh().
 | Element-wise arctangent of \text{input}_{i} / \text{other}_{i} with consideration of the quadrant.
 | Alias for torch.atan2().
bitwise_not | Computes the bitwise NOT of the given input tensor.
bitwise_and | Computes the bitwise AND of  and .
bitwise_or | Computes the bitwise OR of  and .
bitwise_xor | Computes the bitwise XOR of  and .
bitwise_left_shift | Computes the left arithmetic shift of  by  bits.
bitwise_right_shift | Computes the right arithmetic shift of  by  bits.
 | Returns a new tensor with the ceil of the elements of , the smallest integer greater than or equal to each element.
 | Clamps all elements in  into the range  ,  .
 | Alias for torch.clamp().
conj_physical | Computes the element-wise conjugate of the given  tensor.
 | Create a new floating-point tensor with the magnitude of  and the sign of , elementwise.
 | Returns a new tensor with the cosine  of the elements of .
 | Returns a new tensor with the hyperbolic cosine  of the elements of .
 | Returns a new tensor with each of the elements of  converted from angles in degrees to radians.
 | Divides each element of the input  by the corresponding element of .
 | Alias for torch.div().
 | Alias for torch.special.digamma().
 | Alias for torch.special.erf().
 | Alias for torch.special.erfc().
 | Alias for torch.special.erfinv().
 | Returns a new tensor with the exponential of the elements of the input tensor .
 | Alias for torch.special.exp2().
 | Alias for torch.special.expm1().
fake_quantize_per_channel_affine | Returns a new tensor with the data in  fake quantized per channel using , zero_point,  and , across the channel specified by .
fake_quantize_per_tensor_affine | Returns a new tensor with the data in  fake quantized using , zero_point,  and .
 | Alias for torch.trunc()
float_power | Raises  to the power of , elementwise, in double precision.
 | Returns a new tensor with the floor of the elements of , the largest integer less than or equal to each element.
floor_divide | 
 | Applies C++'s  entrywise.
 | Computes the fractional portion of each element in .
 | Decomposes  into mantissa and exponent tensors such that \text{input} = \text{mantissa} \times 2^{\text{exponent}}.
 | Estimates the gradient of a function g : \mathbb{R}^n \rightarrow \mathbb{R} in one or more dimensions using the second-order accurate central differences method and either first or second order estimates at the boundaries.
 | Returns a new tensor containing imaginary values of the  tensor.
 | Multiplies  by 2 ** .
 | Does a linear interpolation of two tensors  (given by ) and  based on a scalar or tensor  and returns the resulting  tensor.
 | Computes the natural logarithm of the absolute value of the gamma function on .
 | Returns a new tensor with the natural logarithm of the elements of .
 | Returns a new tensor with the logarithm to the base 10 of the elements of .
 | Returns a new tensor with the natural logarithm of (1 + ).
 | Returns a new tensor with the logarithm to the base 2 of the elements of .
 | Logarithm of the sum of exponentiations of the inputs.
logaddexp2 | Logarithm of the sum of exponentiations of the inputs in base-2.
logical_and | Computes the element-wise logical AND of the given input tensors.
logical_not | Computes the element-wise logical NOT of the given input tensor.
logical_or | Computes the element-wise logical OR of the given input tensors.
logical_xor | Computes the element-wise logical XOR of the given input tensors.
 | Alias for torch.special.logit().
 | Given the legs of a right triangle, return its hypotenuse.
 | Alias for torch.special.i0().
 | Alias for torch.special.gammainc().
 | Alias for torch.special.gammaincc().
 | Multiplies  by .
 | Alias for torch.mul().
 | Alias for torch.special.multigammaln().
nan_to_num | Replaces , positive infinity, and negative infinity values in  with the values specified by , , and , respectively.
 | Returns a new tensor with the negative of the elements of .
 | Alias for torch.neg()
 | Return the next floating-point value after  towards , elementwise.
 | Alias for torch.special.polygamma().
 | Returns .
 | Takes the power of each element in  with  and returns a tensor with the result.
quantized_batch_norm | Applies batch normalization on a 4D (NCHW) quantized tensor.
quantized_max_pool1d | Applies a 1D max pooling over an input quantized tensor composed of several input planes.
quantized_max_pool2d | Applies a 2D max pooling over an input quantized tensor composed of several input planes.
 | Returns a new tensor with each of the elements of  converted from angles in radians to degrees.
 | Returns a new tensor containing real values of the  tensor.
reciprocal | Returns a new tensor with the reciprocal of the elements of
 | Computes Python's modulus operation entrywise.
 | Rounds elements of  to the nearest integer.
 | Returns a new tensor with the reciprocal of the square-root of each of the elements of .
 | Alias for torch.special.expit().
 | Returns a new tensor with the signs of the elements of .
 | This function is an extension of torch.sign() to complex tensors.
 | Tests if each element of  has its sign bit set or not.
 | Returns a new tensor with the sine of the elements of .
 | Alias for torch.special.sinc().
 | Returns a new tensor with the hyperbolic sine of the elements of .
 | Alias for torch.nn.functional.softmax().
 | Returns a new tensor with the square-root of the elements of .
 | Returns a new tensor with the square of the elements of .
 | Subtracts , scaled by , from .
 | Alias for torch.sub().
 | Returns a new tensor with the tangent of the elements of .
 | Returns a new tensor with the hyperbolic tangent of the elements of .
true_divide | Alias for torch.div() with rounding_mode=None.
 | Returns a new tensor with the truncated integer values of the elements of .
 | Alias for torch.special.xlogy().

================================================================================

# torch - Pointwise Ops (Part 2)

Computes the absolute value of each element in .

Alias for torch.abs()

Computes the inverse cosine of each element in .

Alias for torch.acos().

Returns a new tensor with the inverse hyperbolic cosine of the elements of .

Alias for torch.acosh().

Adds , scaled by , to .

Performs the element-wise division of  by , multiplies the result by the scalar  and adds it to .

Performs the element-wise multiplication of  by , multiplies the result by the scalar  and adds it to .

Computes the element-wise angle (in radians) of the given  tensor.

Returns a new tensor with the arcsine of the elements of .

Alias for torch.asin().

Returns a new tensor with the inverse hyperbolic sine of the elements of .

Alias for torch.asinh().

Returns a new tensor with the arctangent of the elements of .

Alias for torch.atan().

Returns a new tensor with the inverse hyperbolic tangent of the elements of .

Alias for torch.atanh().

Element-wise arctangent of \text{input}_{i} / \text{other}_{i} with consideration of the quadrant.

Alias for torch.atan2().

Computes the bitwise NOT of the given input tensor.

Computes the bitwise AND of  and .

Computes the bitwise OR of  and .

Computes the bitwise XOR of  and .

================================================================================

# torch - Pointwise Ops (Part 3)

Computes the left arithmetic shift of  by  bits.

Computes the right arithmetic shift of  by  bits.

Returns a new tensor with the ceil of the elements of , the smallest integer greater than or equal to each element.

Clamps all elements in  into the range  ,  .

Alias for torch.clamp().

Computes the element-wise conjugate of the given  tensor.

Create a new floating-point tensor with the magnitude of  and the sign of , elementwise.

Returns a new tensor with the cosine  of the elements of .

Returns a new tensor with the hyperbolic cosine  of the elements of .

Returns a new tensor with each of the elements of  converted from angles in degrees to radians.

Divides each element of the input  by the corresponding element of .

Alias for torch.div().

Alias for torch.special.digamma().

Alias for torch.special.erf().

Alias for torch.special.erfc().

Alias for torch.special.erfinv().

Returns a new tensor with the exponential of the elements of the input tensor .

Alias for torch.special.exp2().

Alias for torch.special.expm1().

fake_quantize_per_channel_affine

Returns a new tensor with the data in  fake quantized per channel using , zero_point,  and , across the channel specified by .

fake_quantize_per_tensor_affine

================================================================================

# torch - Pointwise Ops (Part 4)

Returns a new tensor with the data in  fake quantized using , zero_point,  and .

Alias for torch.trunc()

Raises  to the power of , elementwise, in double precision.

Returns a new tensor with the floor of the elements of , the largest integer less than or equal to each element.

Applies C++'s  entrywise.

Computes the fractional portion of each element in .

Decomposes  into mantissa and exponent tensors such that \text{input} = \text{mantissa} \times 2^{\text{exponent}}.

Estimates the gradient of a function g : \mathbb{R}^n \rightarrow \mathbb{R} in one or more dimensions using the second-order accurate central differences method and either first or second order estimates at the boundaries.

Returns a new tensor containing imaginary values of the  tensor.

Multiplies  by 2 ** .

Does a linear interpolation of two tensors  (given by ) and  based on a scalar or tensor  and returns the resulting  tensor.

Computes the natural logarithm of the absolute value of the gamma function on .

Returns a new tensor with the natural logarithm of the elements of .

Returns a new tensor with the logarithm to the base 10 of the elements of .

Returns a new tensor with the natural logarithm of (1 + ).

================================================================================

# torch - Pointwise Ops (Part 5)

Returns a new tensor with the logarithm to the base 2 of the elements of .

Logarithm of the sum of exponentiations of the inputs.

Logarithm of the sum of exponentiations of the inputs in base-2.

Computes the element-wise logical AND of the given input tensors.

Computes the element-wise logical NOT of the given input tensor.

Computes the element-wise logical OR of the given input tensors.

Computes the element-wise logical XOR of the given input tensors.

Alias for torch.special.logit().

Given the legs of a right triangle, return its hypotenuse.

Alias for torch.special.i0().

Alias for torch.special.gammainc().

Alias for torch.special.gammaincc().

Alias for torch.mul().

Alias for torch.special.multigammaln().

Replaces , positive infinity, and negative infinity values in  with the values specified by , , and , respectively.

Returns a new tensor with the negative of the elements of .

Alias for torch.neg()

Return the next floating-point value after  towards , elementwise.

Alias for torch.special.polygamma().

Takes the power of each element in  with  and returns a tensor with the result.

Applies batch normalization on a 4D (NCHW) quantized tensor.

================================================================================

# torch - Pointwise Ops (Part 6)

Applies a 1D max pooling over an input quantized tensor composed of several input planes.

Applies a 2D max pooling over an input quantized tensor composed of several input planes.

Returns a new tensor with each of the elements of  converted from angles in radians to degrees.

Returns a new tensor containing real values of the  tensor.

Returns a new tensor with the reciprocal of the elements of

Computes Python's modulus operation entrywise.

Rounds elements of  to the nearest integer.

Returns a new tensor with the reciprocal of the square-root of each of the elements of .

Alias for torch.special.expit().

Returns a new tensor with the signs of the elements of .

This function is an extension of torch.sign() to complex tensors.

Tests if each element of  has its sign bit set or not.

Returns a new tensor with the sine of the elements of .

Alias for torch.special.sinc().

Returns a new tensor with the hyperbolic sine of the elements of .

Alias for torch.nn.functional.softmax().

Returns a new tensor with the square-root of the elements of .

Returns a new tensor with the square of the elements of .

Subtracts , scaled by , from .

Alias for torch.sub().

Returns a new tensor with the tangent of the elements of .

================================================================================

# torch - Pointwise Ops (Part 7)

Returns a new tensor with the hyperbolic tangent of the elements of .

Alias for torch.div() with rounding_mode=None.

Returns a new tensor with the truncated integer values of the elements of .

Alias for torch.special.xlogy().

================================================================================

# torch - Reduction Ops (Part 1)

Table:
 | Returns the indices of the maximum value of all elements in the  tensor.
 | Returns the indices of the minimum value(s) of the flattened tensor or along a dimension
 | Returns the maximum value of each slice of the  tensor in the given dimension(s) .
 | Returns the minimum value of each slice of the  tensor in the given dimension(s) .
 | Computes the minimum and maximum values of the  tensor.
 | Tests if all elements in  evaluate to .
 | Tests if any element in  evaluates to .
 | Returns the maximum value of all elements in the  tensor.
 | Returns the minimum value of all elements in the  tensor.
 | Returns the p-norm of ( - )
 | Returns the log of summed exponentials of each row of the  tensor in the given dimension .
 | Computes the mean of all  elements along the specified dimensions.
 | Returns the median of the values in .
 | Returns the median of the values in , ignoring  values.
 | Returns a namedtuple   where  is the mode value of each row of the  tensor in the given dimension , i.e. a value which appears most often in that row, and  is the index location of each mode value found.
 | Returns the matrix norm or vector norm of a given tensor.
 | Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.
 | Returns the product of all elements in the  tensor.
 | Computes the q-th quantiles of each row of the  tensor along the dimension .
nanquantile | This is a variant of torch.quantile() that "ignores"  values, computing the quantiles  as if  values in  did not exist.
 | Calculates the standard deviation over the dimensions specified by .
 | Calculates the standard deviation and mean over the dimensions specified by .
 | Returns the sum of all elements in the  tensor.
 | Returns the unique elements of the input tensor.
unique_consecutive | Eliminates all but the first element from every consecutive group of equivalent elements.
 | Calculates the variance over the dimensions specified by .
 | Calculates the variance and mean over the dimensions specified by .
count_nonzero | Counts the number of non-zero values in the tensor  along the given .

================================================================================

# torch - Reduction Ops (Part 2)

Returns the indices of the maximum value of all elements in the  tensor.

Returns the indices of the minimum value(s) of the flattened tensor or along a dimension

Returns the maximum value of each slice of the  tensor in the given dimension(s) .

Returns the minimum value of each slice of the  tensor in the given dimension(s) .

Computes the minimum and maximum values of the  tensor.

Tests if all elements in  evaluate to .

Tests if any element in  evaluates to .

Returns the maximum value of all elements in the  tensor.

Returns the minimum value of all elements in the  tensor.

Returns the p-norm of ( - )

Returns the log of summed exponentials of each row of the  tensor in the given dimension .

Computes the mean of all  elements along the specified dimensions.

Returns the median of the values in .

Returns the median of the values in , ignoring  values.

Returns a namedtuple   where  is the mode value of each row of the  tensor in the given dimension , i.e. a value which appears most often in that row, and  is the index location of each mode value found.

Returns the matrix norm or vector norm of a given tensor.

Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.

================================================================================

# torch - Reduction Ops (Part 3)

Returns the product of all elements in the  tensor.

Computes the q-th quantiles of each row of the  tensor along the dimension .

This is a variant of torch.quantile() that "ignores"  values, computing the quantiles  as if  values in  did not exist.

Calculates the standard deviation over the dimensions specified by .

Calculates the standard deviation and mean over the dimensions specified by .

Returns the sum of all elements in the  tensor.

Returns the unique elements of the input tensor.

Eliminates all but the first element from every consecutive group of equivalent elements.

Calculates the variance over the dimensions specified by .

Calculates the variance and mean over the dimensions specified by .

Counts the number of non-zero values in the tensor  along the given .

================================================================================

# torch - Comparison Ops (Part 1)

Table:
 | This function checks if  and  satisfy the condition:
 | Returns the indices that sort a tensor along a given dimension in ascending order by value.
 | Computes element-wise equality
 | if two tensors have the same size and elements,  otherwise.
 | Computes \text{input} \geq \text{other} element-wise.
greater_equal | Alias for torch.ge().
 | Computes \text{input} > \text{other} element-wise.
 | Alias for torch.gt().
 | Returns a new tensor with boolean elements representing if each element of  is "close" to the corresponding element of .
 | Returns a new tensor with boolean elements representing if each element is  or not.
 | Tests if each element of  is in test_elements.
 | Tests if each element of  is infinite (positive or negative infinity) or not.
 | Tests if each element of  is positive infinity or not.
 | Tests if each element of  is negative infinity or not.
 | Returns a new tensor with boolean elements representing if each element of  is NaN or not.
 | Returns a new tensor with boolean elements representing if each element of  is real-valued or not.
 | Returns a namedtuple   where  is the  th smallest element of each row of the  tensor in the given dimension .
 | Computes \text{input} \leq \text{other} element-wise.
less_equal | Alias for torch.le().
 | Computes \text{input} < \text{other} element-wise.
 | Alias for torch.lt().
 | Computes the element-wise maximum of  and .
 | Computes the element-wise minimum of  and .
 | Computes the element-wise maximum of  and .
 | Computes the element-wise minimum of  and .
 | Computes \text{input} \neq \text{other} element-wise.
 | Alias for torch.ne().
 | Sorts the elements of the  tensor along a given dimension in ascending order by value.
 | Returns the  largest elements of the given  tensor along a given dimension.
 | Sorts the elements of the  tensor along its first dimension in ascending order by value.

================================================================================

# torch - Comparison Ops (Part 2)

This function checks if  and  satisfy the condition:

Returns the indices that sort a tensor along a given dimension in ascending order by value.

Computes element-wise equality

if two tensors have the same size and elements,  otherwise.

Computes \text{input} \geq \text{other} element-wise.

Alias for torch.ge().

Computes \text{input} > \text{other} element-wise.

Alias for torch.gt().

Returns a new tensor with boolean elements representing if each element of  is "close" to the corresponding element of .

Returns a new tensor with boolean elements representing if each element is  or not.

Tests if each element of  is in test_elements.

Tests if each element of  is infinite (positive or negative infinity) or not.

Tests if each element of  is positive infinity or not.

Tests if each element of  is negative infinity or not.

Returns a new tensor with boolean elements representing if each element of  is NaN or not.

Returns a new tensor with boolean elements representing if each element of  is real-valued or not.

Returns a namedtuple   where  is the  th smallest element of each row of the  tensor in the given dimension .

Computes \text{input} \leq \text{other} element-wise.

Alias for torch.le().

================================================================================

# torch - Comparison Ops (Part 3)

Computes \text{input} < \text{other} element-wise.

Alias for torch.lt().

Computes the element-wise maximum of  and .

Computes the element-wise minimum of  and .

Computes the element-wise maximum of  and .

Computes the element-wise minimum of  and .

Computes \text{input} \neq \text{other} element-wise.

Alias for torch.ne().

Sorts the elements of the  tensor along a given dimension in ascending order by value.

Returns the  largest elements of the given  tensor along a given dimension.

Sorts the elements of the  tensor along its first dimension in ascending order by value.

================================================================================

# torch - Spectral Ops

Table:
 | Short-time Fourier transform (STFT).
 | Inverse short time Fourier Transform.
bartlett_window | Bartlett window function.
blackman_window | Blackman window function.
hamming_window | Hamming window function.
hann_window | Hann window function.
kaiser_window | Computes the Kaiser window with window length window_length and shape parameter .

Short-time Fourier transform (STFT).

Inverse short time Fourier Transform.

Bartlett window function.

Blackman window function.

Hamming window function.

Hann window function.

Computes the Kaiser window with window length window_length and shape parameter .

================================================================================

# torch - Other Operations (Part 1)

Table:
atleast_1d | Returns a 1-dimensional view of each input tensor with zero dimensions.
atleast_2d | Returns a 2-dimensional view of each input tensor with zero dimensions.
atleast_3d | Returns a 3-dimensional view of each input tensor with zero dimensions.
 | Count the frequency of each value in an array of non-negative ints.
block_diag | Create a block diagonal matrix from provided tensors.
broadcast_tensors | Broadcasts the given tensors according to Broadcasting semantics.
broadcast_to | Broadcasts  to the shape .
broadcast_shapes | Similar to broadcast_tensors() but for shapes.
 | Returns the indices of the buckets to which each value in the  belongs, where the boundaries of the buckets are set by boundaries.
cartesian_prod | Do cartesian product of the given sequence of tensors.
 | Computes batched the p-norm distance between each pair of the two collections of row vectors.
 | Returns a copy of .
combinations | Compute combinations of length  of the given tensor.
 | Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the  matrix, where rows are the variables and columns are the observations.
 | Estimates the covariance matrix of the variables given by the  matrix, where rows are the variables and columns are the observations.
 | Returns the cross product of vectors in dimension  of  and .
 | Returns a namedtuple   where  is the cumulative maximum of elements of  in the dimension .
 | Returns a namedtuple   where  is the cumulative minimum of elements of  in the dimension .
 | Returns the cumulative product of elements of  in the dimension .
 | Returns the cumulative sum of elements of  in the dimension .
 | If  is a vector (1-D tensor), then returns a 2-D square tensor
diag_embed | Creates a tensor whose diagonals of certain 2D planes (specified by  and ) are filled by .
 | If  is a vector (1-D tensor), then returns a 2-D square tensor
 | Returns a partial view of  with the its diagonal elements with respect to  and  appended as a dimension at the end of the shape.
 | Computes the n-th forward difference along the given dimension.
 | Sums the product of the elements of the input  along dimensions specified using a notation based on the Einstein summation convention.
 | Flattens  by reshaping it into a one-dimensional tensor.
 | Reverse the order of an n-D tensor along given axis in dims.
 | Flip tensor in the left/right direction, returning a new tensor.
 | Flip tensor in the up/down direction, returning a new tensor.
 | Computes the Kronecker product, denoted by , of  and .
 | Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.
 | Computes the element-wise greatest common divisor (GCD) of  and .
 | Computes the histogram of a tensor.
 | Computes a histogram of the values in a tensor.
histogramdd | Computes a multi-dimensional histogram of the values in a tensor.
 | Creates grids of coordinates specified by the 1D inputs in :tensors.
 | Computes the element-wise least common multiple (LCM) of  and .
logcumsumexp | Returns the logarithm of the cumulative summation of the exponentiation of elements of  in the dimension .
 | Return a contiguous flattened tensor.
 | Returns a tensor where each sub-tensor of  along dimension  is normalized such that the -norm of the sub-tensor is lower than the value
repeat_interleave | Repeat elements of a tensor.
 | Roll the tensor  along the given dimension(s).
searchsorted | Find the indices from the  dimension of sorted_sequence such that, if the corresponding values in  were inserted before the indices, when sorted, the order of the corresponding  dimension within sorted_sequence would be preserved.
 | Returns a contraction of a and b over multiple dimensions.
 | Returns the sum of the elements of the diagonal of the input 2-D matrix.
 | Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices , the other elements of the result tensor  are set to 0.
tril_indices | Returns the indices of the lower triangular part of a -by-  matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.
 | Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices , the other elements of the result tensor  are set to 0.
triu_indices | Returns the indices of the upper triangular part of a  by  matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.
 | Expands a dimension of the input tensor over multiple dimensions.
 | Generates a Vandermonde matrix.
view_as_real | Returns a view of  as a real tensor.
view_as_complex | Returns a view of  as a complex tensor.
resolve_conj | Returns a new tensor with materialized conjugation if 's conjugate bit is set to , else returns .
resolve_neg | Returns a new tensor with materialized negation if 's negative bit is set to , else returns .

================================================================================

# torch - Other Operations (Part 2)

Returns a 1-dimensional view of each input tensor with zero dimensions.

Returns a 2-dimensional view of each input tensor with zero dimensions.

Returns a 3-dimensional view of each input tensor with zero dimensions.

Count the frequency of each value in an array of non-negative ints.

Create a block diagonal matrix from provided tensors.

Broadcasts the given tensors according to Broadcasting semantics.

Broadcasts  to the shape .

Similar to broadcast_tensors() but for shapes.

Returns the indices of the buckets to which each value in the  belongs, where the boundaries of the buckets are set by boundaries.

Do cartesian product of the given sequence of tensors.

Computes batched the p-norm distance between each pair of the two collections of row vectors.

Compute combinations of length  of the given tensor.

Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the  matrix, where rows are the variables and columns are the observations.

Estimates the covariance matrix of the variables given by the  matrix, where rows are the variables and columns are the observations.

Returns the cross product of vectors in dimension  of  and .

================================================================================

# torch - Other Operations (Part 3)

Returns a namedtuple   where  is the cumulative maximum of elements of  in the dimension .

Returns a namedtuple   where  is the cumulative minimum of elements of  in the dimension .

Returns the cumulative product of elements of  in the dimension .

Returns the cumulative sum of elements of  in the dimension .

If  is a vector (1-D tensor), then returns a 2-D square tensor

List:
If  is a vector (1-D tensor), then returns a 2-D square tensor

If  is a vector (1-D tensor), then returns a 2-D square tensor

Creates a tensor whose diagonals of certain 2D planes (specified by  and ) are filled by .

If  is a vector (1-D tensor), then returns a 2-D square tensor

List:
If  is a vector (1-D tensor), then returns a 2-D square tensor

If  is a vector (1-D tensor), then returns a 2-D square tensor

Returns a partial view of  with the its diagonal elements with respect to  and  appended as a dimension at the end of the shape.

Computes the n-th forward difference along the given dimension.

Sums the product of the elements of the input  along dimensions specified using a notation based on the Einstein summation convention.

Flattens  by reshaping it into a one-dimensional tensor.

================================================================================

# torch - Other Operations (Part 4)

Reverse the order of an n-D tensor along given axis in dims.

Flip tensor in the left/right direction, returning a new tensor.

Flip tensor in the up/down direction, returning a new tensor.

Computes the Kronecker product, denoted by , of  and .

Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.

Computes the element-wise greatest common divisor (GCD) of  and .

Computes the histogram of a tensor.

Computes a histogram of the values in a tensor.

Computes a multi-dimensional histogram of the values in a tensor.

Creates grids of coordinates specified by the 1D inputs in :tensors.

Computes the element-wise least common multiple (LCM) of  and .

Returns the logarithm of the cumulative summation of the exponentiation of elements of  in the dimension .

Return a contiguous flattened tensor.

Returns a tensor where each sub-tensor of  along dimension  is normalized such that the -norm of the sub-tensor is lower than the value

Repeat elements of a tensor.

Roll the tensor  along the given dimension(s).

================================================================================

# torch - Other Operations (Part 5)

Find the indices from the  dimension of sorted_sequence such that, if the corresponding values in  were inserted before the indices, when sorted, the order of the corresponding  dimension within sorted_sequence would be preserved.

Returns a contraction of a and b over multiple dimensions.

Returns the sum of the elements of the diagonal of the input 2-D matrix.

Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices , the other elements of the result tensor  are set to 0.

Returns the indices of the lower triangular part of a -by-  matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.

Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices , the other elements of the result tensor  are set to 0.

Returns the indices of the upper triangular part of a  by  matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.

Expands a dimension of the input tensor over multiple dimensions.

Generates a Vandermonde matrix.

Returns a view of  as a real tensor.

Returns a view of  as a complex tensor.

================================================================================

# torch - Other Operations (Part 6)

Returns a new tensor with materialized conjugation if 's conjugate bit is set to , else returns .

Returns a new tensor with materialized negation if 's negative bit is set to , else returns .

================================================================================

# torch - BLAS and LAPACK Operations (Part 1)

Table:
 | Performs a batch matrix-matrix product of matrices stored in  and , with a reduced add step (all matrix multiplications get accumulated along the first dimension).
 | Performs a matrix multiplication of the matrices  and .
 | Performs a matrix-vector product of the matrix  and the vector .
 | Performs the outer-product of vectors  and  and adds it to the matrix .
 | Performs a batch matrix-matrix product of matrices in  and .
 | Performs a batch matrix-matrix product of matrices stored in  and .
chain_matmul | Returns the matrix product of the  2-D tensors.
 | Computes the Cholesky decomposition of a symmetric positive-definite matrix  or for batches of symmetric positive-definite matrices.
cholesky_inverse | Computes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.
cholesky_solve | Computes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.
 | Computes the dot product of two 1D tensors.
 | This is a low-level function for calling LAPACK's geqrf directly.
 | Alias of torch.outer().
 | Computes the dot product for 1D tensors.
 | Alias for torch.linalg.inv()
 | Alias for torch.linalg.det()
 | Calculates log determinant of a square matrix or batches of square matrices.
 | Alias for torch.linalg.slogdet()
 | Computes the LU factorization of a matrix or batches of matrices .
 | Returns the LU solve of the linear system  using the partially pivoted LU factorization of A from lu_factor().
 | Unpacks the LU decomposition returned by lu_factor() into the  matrices.
 | Matrix product of two tensors.
matrix_power | Alias for torch.linalg.matrix_power()
matrix_exp | Alias for torch.linalg.matrix_exp().
 | Performs a matrix multiplication of the matrices  and .
 | Performs a matrix-vector product of the matrix  and the vector .
 | Alias for torch.linalg.householder_product().
 | Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.
 | Outer product of  and .
 | Alias for torch.linalg.pinv()
 | Computes the QR decomposition of a matrix or a batch of matrices , and returns a namedtuple (Q, R) of tensors such that \text{input} = Q R with  being an orthogonal matrix or batch of orthogonal matrices and  being an upper triangular matrix or batch of upper triangular matrices.
 | Computes the singular value decomposition of either a matrix or batch of matrices .
svd_lowrank | Return the singular value decomposition    of a matrix, batches of matrices, or a sparse matrix  such that A \approx U \operatorname{diag}(S) V^{\text{H}}.
pca_lowrank | Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.
 | Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.
 | Alias for torch.trapezoid().
 | Computes the trapezoidal rule along .
cumulative_trapezoid | Cumulatively computes the trapezoidal rule along .
triangular_solve | Solves a system of equations with a square upper or lower triangular invertible matrix  and multiple right-hand sides .
 | Computes the dot product of two 1D vectors along a dimension.

================================================================================

# torch - BLAS and LAPACK Operations (Part 2)

Performs a batch matrix-matrix product of matrices stored in  and , with a reduced add step (all matrix multiplications get accumulated along the first dimension).

Performs a matrix multiplication of the matrices  and .

Performs a matrix-vector product of the matrix  and the vector .

Performs the outer-product of vectors  and  and adds it to the matrix .

Performs a batch matrix-matrix product of matrices in  and .

Performs a batch matrix-matrix product of matrices stored in  and .

Returns the matrix product of the  2-D tensors.

Computes the Cholesky decomposition of a symmetric positive-definite matrix  or for batches of symmetric positive-definite matrices.

Computes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.

Computes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.

Computes the dot product of two 1D tensors.

This is a low-level function for calling LAPACK's geqrf directly.

Alias of torch.outer().

Computes the dot product for 1D tensors.

Alias for torch.linalg.inv()

Alias for torch.linalg.det()

================================================================================

# torch - BLAS and LAPACK Operations (Part 3)

Calculates log determinant of a square matrix or batches of square matrices.

Alias for torch.linalg.slogdet()

Computes the LU factorization of a matrix or batches of matrices .

Returns the LU solve of the linear system  using the partially pivoted LU factorization of A from lu_factor().

Unpacks the LU decomposition returned by lu_factor() into the  matrices.

Matrix product of two tensors.

Alias for torch.linalg.matrix_power()

Alias for torch.linalg.matrix_exp().

Performs a matrix multiplication of the matrices  and .

Performs a matrix-vector product of the matrix  and the vector .

Alias for torch.linalg.householder_product().

Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.

Outer product of  and .

Alias for torch.linalg.pinv()

Computes the QR decomposition of a matrix or a batch of matrices , and returns a namedtuple (Q, R) of tensors such that \text{input} = Q R with  being an orthogonal matrix or batch of orthogonal matrices and  being an upper triangular matrix or batch of upper triangular matrices.

Computes the singular value decomposition of either a matrix or batch of matrices .

================================================================================

# torch - BLAS and LAPACK Operations (Part 4)

Return the singular value decomposition    of a matrix, batches of matrices, or a sparse matrix  such that A \approx U \operatorname{diag}(S) V^{\text{H}}.

Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.

Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.

Alias for torch.trapezoid().

Computes the trapezoidal rule along .

Cumulatively computes the trapezoidal rule along .

Cumulatively computes the trapezoidal rule along .

Solves a system of equations with a square upper or lower triangular invertible matrix  and multiple right-hand sides .

Computes the dot product of two 1D vectors along a dimension.

================================================================================

# torch - Foreach Operations (Part 1)

This API is in beta and subject to future changes.
Forward-mode AD is not supported.

================================================================================

# torch - Foreach Operations (Part 2)

Table:
_foreach_abs | Apply torch.abs() to each Tensor of the input list.
_foreach_abs_ | Apply torch.abs() to each Tensor of the input list.
_foreach_acos | Apply torch.acos() to each Tensor of the input list.
_foreach_acos_ | Apply torch.acos() to each Tensor of the input list.
_foreach_asin | Apply torch.asin() to each Tensor of the input list.
_foreach_asin_ | Apply torch.asin() to each Tensor of the input list.
_foreach_atan | Apply torch.atan() to each Tensor of the input list.
_foreach_atan_ | Apply torch.atan() to each Tensor of the input list.
_foreach_ceil | Apply torch.ceil() to each Tensor of the input list.
_foreach_ceil_ | Apply torch.ceil() to each Tensor of the input list.
_foreach_cos | Apply torch.cos() to each Tensor of the input list.
_foreach_cos_ | Apply torch.cos() to each Tensor of the input list.
_foreach_cosh | Apply torch.cosh() to each Tensor of the input list.
_foreach_cosh_ | Apply torch.cosh() to each Tensor of the input list.
_foreach_erf | Apply torch.erf() to each Tensor of the input list.
_foreach_erf_ | Apply torch.erf() to each Tensor of the input list.
_foreach_erfc | Apply torch.erfc() to each Tensor of the input list.
_foreach_erfc_ | Apply torch.erfc() to each Tensor of the input list.
_foreach_exp | Apply torch.exp() to each Tensor of the input list.
_foreach_exp_ | Apply torch.exp() to each Tensor of the input list.
_foreach_expm1 | Apply torch.expm1() to each Tensor of the input list.
_foreach_expm1_ | Apply torch.expm1() to each Tensor of the input list.
_foreach_floor | Apply torch.floor() to each Tensor of the input list.
_foreach_floor_ | Apply torch.floor() to each Tensor of the input list.
_foreach_log | Apply torch.log() to each Tensor of the input list.
_foreach_log_ | Apply torch.log() to each Tensor of the input list.
_foreach_log10 | Apply torch.log10() to each Tensor of the input list.
_foreach_log10_ | Apply torch.log10() to each Tensor of the input list.
_foreach_log1p | Apply torch.log1p() to each Tensor of the input list.
_foreach_log1p_ | Apply torch.log1p() to each Tensor of the input list.
_foreach_log2 | Apply torch.log2() to each Tensor of the input list.
_foreach_log2_ | Apply torch.log2() to each Tensor of the input list.
_foreach_neg | Apply torch.neg() to each Tensor of the input list.
_foreach_neg_ | Apply torch.neg() to each Tensor of the input list.
_foreach_tan | Apply torch.tan() to each Tensor of the input list.
_foreach_tan_ | Apply torch.tan() to each Tensor of the input list.
_foreach_sin | Apply torch.sin() to each Tensor of the input list.
_foreach_sin_ | Apply torch.sin() to each Tensor of the input list.
_foreach_sinh | Apply torch.sinh() to each Tensor of the input list.
_foreach_sinh_ | Apply torch.sinh() to each Tensor of the input list.
_foreach_round | Apply torch.round() to each Tensor of the input list.
_foreach_round_ | Apply torch.round() to each Tensor of the input list.
_foreach_sqrt | Apply torch.sqrt() to each Tensor of the input list.
_foreach_sqrt_ | Apply torch.sqrt() to each Tensor of the input list.
_foreach_lgamma | Apply torch.lgamma() to each Tensor of the input list.
_foreach_lgamma_ | Apply torch.lgamma() to each Tensor of the input list.
_foreach_frac | Apply torch.frac() to each Tensor of the input list.
_foreach_frac_ | Apply torch.frac() to each Tensor of the input list.
_foreach_reciprocal | Apply torch.reciprocal() to each Tensor of the input list.
_foreach_reciprocal_ | Apply torch.reciprocal() to each Tensor of the input list.
_foreach_sigmoid | Apply torch.sigmoid() to each Tensor of the input list.
_foreach_sigmoid_ | Apply torch.sigmoid() to each Tensor of the input list.
_foreach_trunc | Apply torch.trunc() to each Tensor of the input list.
_foreach_trunc_ | Apply torch.trunc() to each Tensor of the input list.
_foreach_zero_ | Apply torch.zero() to each Tensor of the input list.

================================================================================

# torch - Foreach Operations (Part 3)

Apply torch.abs() to each Tensor of the input list.

Apply torch.abs() to each Tensor of the input list.

Apply torch.acos() to each Tensor of the input list.

Apply torch.acos() to each Tensor of the input list.

Apply torch.asin() to each Tensor of the input list.

Apply torch.asin() to each Tensor of the input list.

Apply torch.atan() to each Tensor of the input list.

Apply torch.atan() to each Tensor of the input list.

Apply torch.ceil() to each Tensor of the input list.

Apply torch.ceil() to each Tensor of the input list.

Apply torch.cos() to each Tensor of the input list.

Apply torch.cos() to each Tensor of the input list.

Apply torch.cosh() to each Tensor of the input list.

Apply torch.cosh() to each Tensor of the input list.

Apply torch.erf() to each Tensor of the input list.

Apply torch.erf() to each Tensor of the input list.

Apply torch.erfc() to each Tensor of the input list.

Apply torch.erfc() to each Tensor of the input list.

Apply torch.exp() to each Tensor of the input list.

Apply torch.exp() to each Tensor of the input list.

Apply torch.expm1() to each Tensor of the input list.

Apply torch.expm1() to each Tensor of the input list.

Apply torch.floor() to each Tensor of the input list.

================================================================================

# torch - Foreach Operations (Part 4)

Apply torch.floor() to each Tensor of the input list.

Apply torch.log() to each Tensor of the input list.

Apply torch.log() to each Tensor of the input list.

Apply torch.log10() to each Tensor of the input list.

Apply torch.log10() to each Tensor of the input list.

Apply torch.log1p() to each Tensor of the input list.

Apply torch.log1p() to each Tensor of the input list.

Apply torch.log2() to each Tensor of the input list.

Apply torch.log2() to each Tensor of the input list.

Apply torch.neg() to each Tensor of the input list.

Apply torch.neg() to each Tensor of the input list.

Apply torch.tan() to each Tensor of the input list.

Apply torch.tan() to each Tensor of the input list.

Apply torch.sin() to each Tensor of the input list.

Apply torch.sin() to each Tensor of the input list.

Apply torch.sinh() to each Tensor of the input list.

Apply torch.sinh() to each Tensor of the input list.

Apply torch.round() to each Tensor of the input list.

Apply torch.round() to each Tensor of the input list.

Apply torch.sqrt() to each Tensor of the input list.

Apply torch.sqrt() to each Tensor of the input list.

Apply torch.lgamma() to each Tensor of the input list.

Apply torch.lgamma() to each Tensor of the input list.

================================================================================

# torch - Foreach Operations (Part 5)

Apply torch.frac() to each Tensor of the input list.

Apply torch.frac() to each Tensor of the input list.

Apply torch.reciprocal() to each Tensor of the input list.

Apply torch.reciprocal() to each Tensor of the input list.

Apply torch.sigmoid() to each Tensor of the input list.

Apply torch.sigmoid() to each Tensor of the input list.

Apply torch.trunc() to each Tensor of the input list.

Apply torch.trunc() to each Tensor of the input list.

Apply torch.zero() to each Tensor of the input list.

================================================================================

# torch - Utilities (Part 1)

Table:
compiled_with_cxx11_abi | Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1
result_type | Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.
 | Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.
promote_types | Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either  or .
use_deterministic_algorithms | Sets whether PyTorch operations must use "deterministic" algorithms.
are_deterministic_algorithms_enabled | Returns True if the global deterministic flag is turned on.
is_deterministic_algorithms_warn_only_enabled | Returns True if the global deterministic flag is set to warn only.
set_deterministic_debug_mode | Sets the debug mode for deterministic operations.
get_deterministic_debug_mode | Returns the current value of the debug mode for deterministic operations.
set_float32_matmul_precision | Sets the internal precision of float32 matrix multiplications.
get_float32_matmul_precision | Returns the current value of float32 matrix multiplication precision.
set_warn_always | When this flag is False (default) then some PyTorch warnings may only appear once per process.
get_device_module | Returns the module associated with a given device(e.g., torch.device('cuda'), "mtia:0", "xpu", ...).
is_warn_always_enabled | Returns True if the global warn_always flag is turned on.
 | vmap is the vectorizing map; vmap(func) returns a new function that maps  over some dimension of the inputs.
 | A wrapper around Python's assert which is symbolically traceable.

================================================================================

# torch - Utilities (Part 2)

compiled_with_cxx11_abi

Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1

Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.

Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.

Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either  or .

use_deterministic_algorithms

Sets whether PyTorch operations must use "deterministic" algorithms.

are_deterministic_algorithms_enabled

Returns True if the global deterministic flag is turned on.

is_deterministic_algorithms_warn_only_enabled

Returns True if the global deterministic flag is set to warn only.

set_deterministic_debug_mode

Sets the debug mode for deterministic operations.

get_deterministic_debug_mode

Returns the current value of the debug mode for deterministic operations.

set_float32_matmul_precision

Sets the internal precision of float32 matrix multiplications.

get_float32_matmul_precision

Returns the current value of float32 matrix multiplication precision.

When this flag is False (default) then some PyTorch warnings may only appear once per process.

================================================================================

# torch - Utilities (Part 3)

Returns the module associated with a given device(e.g., torch.device('cuda'), "mtia:0", "xpu", ...).

is_warn_always_enabled

Returns True if the global warn_always flag is turned on.

vmap is the vectorizing map; vmap(func) returns a new function that maps  over some dimension of the inputs.

A wrapper around Python's assert which is symbolically traceable.

================================================================================

# torch - Symbolic Numbers (Part 1)

Like an int (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.


as_integer_ratio
Represent this int as an exact integer ratio

Return type
[‘SymInt’, ]

Like an int (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.

as_integer_ratio
Represent this int as an exact integer ratio

Return type
[‘SymInt’, ]

Represent this int as an exact integer ratio

Return type
[‘SymInt’, ]

Like a float (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.


as_integer_ratio
Represent this float as an exact integer ratio

Return type
[, ]






Returns the complex conjugate of the float.

Return type







Returns the hexadecimal representation of the float.

Return type






is_integer
Return True if the float is an integer.

================================================================================

# torch - Symbolic Numbers (Part 2)

Like a float (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.

as_integer_ratio
Represent this float as an exact integer ratio

Return type
[, ]

Represent this float as an exact integer ratio

Returns the complex conjugate of the float.

Return type

Returns the complex conjugate of the float.

Returns the hexadecimal representation of the float.

Return type

Returns the hexadecimal representation of the float.

is_integer
Return True if the float is an integer.

Return True if the float is an integer.

Like a bool (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.
Unlike regular bools, regular boolean operators will force extra guards instead
of symbolically evaluate.  Use the bitwise operators instead to handle this.

Like a bool (including magic methods), but redirects all operations on the
wrapped node. This is used in particular to symbolically record operations
in the symbolic shape workflow.

================================================================================

# torch - Symbolic Numbers (Part 3)

Unlike regular bools, regular boolean operators will force extra guards instead
of symbolically evaluate.  Use the bitwise operators instead to handle this.

Table:
 | SymInt-aware utility for float casting.
sym_fresh_size | 
 | SymInt-aware utility for int casting.
 | SymInt-aware utility for max which avoids branching on a < b.
 | SymInt-aware utility for min().
 | SymInt-aware utility for logical negation.
 | SymInt-aware utility for ternary operator (    .)
 | N-ary add which is faster to compute for long lists than iterated binary addition.

SymInt-aware utility for float casting.

SymInt-aware utility for int casting.

SymInt-aware utility for max which avoids branching on a < b.

SymInt-aware utility for min().

SymInt-aware utility for logical negation.

SymInt-aware utility for ternary operator (    .)

N-ary add which is faster to compute for long lists than iterated binary addition.

================================================================================

# torch - Export Path

This feature is a prototype and may have compatibility breaking changes in the future.

export
generated/exportdb/index

================================================================================

# torch - Control Flow

This feature is a prototype and may have compatibility breaking changes in the future.

Table:
 | Conditionally applies  or .

Conditionally applies  or .

================================================================================

# torch - Optimizations

Table:
 | Optimizes given model/function using TorchDynamo and specified backend.

Optimizes given model/function using TorchDynamo and specified backend.

torch.compile documentation

================================================================================

# torch - Operator Tags

cudagraph_unsafe
data_dependent_output
dynamic_output_shape
flexible_layout

inplace_view
maybe_aliasing_or_mutating
needs_contiguous_strides
needs_exact_strides
needs_fixed_stride_order
nondeterministic_bitwise
nondeterministic_seeded

pt2_compliant_tag

data_dependent_output

maybe_aliasing_or_mutating

needs_contiguous_strides

needs_fixed_stride_order

nondeterministic_bitwise

nondeterministic_seeded

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.nn

Created On: Dec 23, 2016 | Last Updated On: Nov 06, 2024

These are the basic building blocks for graphs:

List:
Containers
Convolution Layers
Pooling layers
Padding Layers
Non-linear Activations (weighted sum, nonlinearity)
Non-linear Activations (other)
Normalization Layers
Recurrent Layers
Transformer Layers
Linear Layers
Dropout Layers
Sparse Layers
Distance Functions
Loss Functions
Vision Layers
Shuffle Layers
DataParallel Layers (multi-GPU, distributed)

Quantized Functions
Lazy Modules Initialization

Non-linear Activations (weighted sum, nonlinearity)

Non-linear Activations (other)

DataParallel Layers (multi-GPU, distributed)

Lazy Modules Initialization

Table:
 | A kind of Tensor that should not be considered a model parameter.
 | A kind of Tensor that is to be considered a module parameter.
UninitializedParameter | A parameter that is not initialized.
UninitializedBuffer | A buffer that is not initialized.

A kind of Tensor that should not be considered a model parameter.

A kind of Tensor that is to be considered a module parameter.

UninitializedParameter

A parameter that is not initialized.

A buffer that is not initialized.

================================================================================

# torch.nn - Containers (Part 1)

Table:
 | Base class for all neural network modules.
Sequential | A sequential container.
ModuleList | Holds submodules in a list.
ModuleDict | Holds submodules in a dictionary.
ParameterList | Holds parameters in a list.
ParameterDict | Holds parameters in a dictionary.

Base class for all neural network modules.

A sequential container.

Holds submodules in a list.

Holds submodules in a dictionary.

Holds parameters in a list.

Holds parameters in a dictionary.

Global Hooks For Module

================================================================================

# torch.nn - Containers (Part 2)

Table:
register_module_forward_pre_hook | Register a forward pre-hook common to all modules.
register_module_forward_hook | Register a global forward hook for all the modules.
register_module_backward_hook | Register a backward hook common to all the modules.
register_module_full_backward_pre_hook | Register a backward pre-hook common to all the modules.
register_module_full_backward_hook | Register a backward hook common to all the modules.
register_module_buffer_registration_hook | Register a buffer registration hook common to all modules.
register_module_module_registration_hook | Register a module registration hook common to all modules.
register_module_parameter_registration_hook | Register a parameter registration hook common to all modules.

register_module_forward_pre_hook

Register a forward pre-hook common to all modules.

register_module_forward_hook

Register a global forward hook for all the modules.

register_module_backward_hook

Register a backward hook common to all the modules.

register_module_full_backward_pre_hook

Register a backward pre-hook common to all the modules.

register_module_full_backward_hook

Register a backward hook common to all the modules.

================================================================================

# torch.nn - Containers (Part 3)

register_module_buffer_registration_hook

Register a buffer registration hook common to all modules.

register_module_module_registration_hook

Register a module registration hook common to all modules.

register_module_parameter_registration_hook

Register a parameter registration hook common to all modules.

================================================================================

# torch.nn - Convolution Layers (Part 1)

Table:
Applies a 1D convolution over an input signal composed of several input planes.
Applies a 2D convolution over an input signal composed of several input planes.
Applies a 3D convolution over an input signal composed of several input planes.
nn.ConvTranspose1d | Applies a 1D transposed convolution operator over an input image composed of several input planes.
nn.ConvTranspose2d | Applies a 2D transposed convolution operator over an input image composed of several input planes.
nn.ConvTranspose3d | Applies a 3D transposed convolution operator over an input image composed of several input planes.
nn.LazyConv1d | A torch.nn.Conv1d module with lazy initialization of the in_channels argument.
nn.LazyConv2d | A torch.nn.Conv2d module with lazy initialization of the in_channels argument.
nn.LazyConv3d | A torch.nn.Conv3d module with lazy initialization of the in_channels argument.
nn.LazyConvTranspose1d | A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument.
nn.LazyConvTranspose2d | A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument.
nn.LazyConvTranspose3d | A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument.
Extracts sliding local blocks from a batched input tensor.
Combines an array of sliding local blocks into a large containing tensor.

================================================================================

# torch.nn - Convolution Layers (Part 2)

Applies a 1D convolution over an input signal composed of several input planes.

Applies a 2D convolution over an input signal composed of several input planes.

Applies a 3D convolution over an input signal composed of several input planes.

Applies a 1D transposed convolution operator over an input image composed of several input planes.

Applies a 2D transposed convolution operator over an input image composed of several input planes.

Applies a 3D transposed convolution operator over an input image composed of several input planes.

A torch.nn.Conv1d module with lazy initialization of the in_channels argument.

A torch.nn.Conv2d module with lazy initialization of the in_channels argument.

A torch.nn.Conv3d module with lazy initialization of the in_channels argument.

nn.LazyConvTranspose1d

A torch.nn.ConvTranspose1d module with lazy initialization of the in_channels argument.

nn.LazyConvTranspose2d

A torch.nn.ConvTranspose2d module with lazy initialization of the in_channels argument.

nn.LazyConvTranspose3d

A torch.nn.ConvTranspose3d module with lazy initialization of the in_channels argument.

Extracts sliding local blocks from a batched input tensor.

================================================================================

# torch.nn - Convolution Layers (Part 3)

Combines an array of sliding local blocks into a large containing tensor.

================================================================================

# torch.nn - Pooling layers (Part 1)

Table:
nn.MaxPool1d | Applies a 1D max pooling over an input signal composed of several input planes.
nn.MaxPool2d | Applies a 2D max pooling over an input signal composed of several input planes.
nn.MaxPool3d | Applies a 3D max pooling over an input signal composed of several input planes.
nn.MaxUnpool1d | Computes a partial inverse of .
nn.MaxUnpool2d | Computes a partial inverse of .
nn.MaxUnpool3d | Computes a partial inverse of .
nn.AvgPool1d | Applies a 1D average pooling over an input signal composed of several input planes.
nn.AvgPool2d | Applies a 2D average pooling over an input signal composed of several input planes.
nn.AvgPool3d | Applies a 3D average pooling over an input signal composed of several input planes.
nn.FractionalMaxPool2d | Applies a 2D fractional max pooling over an input signal composed of several input planes.
nn.FractionalMaxPool3d | Applies a 3D fractional max pooling over an input signal composed of several input planes.
nn.LPPool1d | Applies a 1D power-average pooling over an input signal composed of several input planes.
nn.LPPool2d | Applies a 2D power-average pooling over an input signal composed of several input planes.
nn.LPPool3d | Applies a 3D power-average pooling over an input signal composed of several input planes.
nn.AdaptiveMaxPool1d | Applies a 1D adaptive max pooling over an input signal composed of several input planes.
nn.AdaptiveMaxPool2d | Applies a 2D adaptive max pooling over an input signal composed of several input planes.
nn.AdaptiveMaxPool3d | Applies a 3D adaptive max pooling over an input signal composed of several input planes.
nn.AdaptiveAvgPool1d | Applies a 1D adaptive average pooling over an input signal composed of several input planes.
nn.AdaptiveAvgPool2d | Applies a 2D adaptive average pooling over an input signal composed of several input planes.
nn.AdaptiveAvgPool3d | Applies a 3D adaptive average pooling over an input signal composed of several input planes.

================================================================================

# torch.nn - Pooling layers (Part 2)

Applies a 1D max pooling over an input signal composed of several input planes.

Applies a 2D max pooling over an input signal composed of several input planes.

Applies a 3D max pooling over an input signal composed of several input planes.

Computes a partial inverse of .

Computes a partial inverse of .

Computes a partial inverse of .

Applies a 1D average pooling over an input signal composed of several input planes.

Applies a 2D average pooling over an input signal composed of several input planes.

Applies a 3D average pooling over an input signal composed of several input planes.

nn.FractionalMaxPool2d

Applies a 2D fractional max pooling over an input signal composed of several input planes.

nn.FractionalMaxPool3d

Applies a 3D fractional max pooling over an input signal composed of several input planes.

Applies a 1D power-average pooling over an input signal composed of several input planes.

Applies a 2D power-average pooling over an input signal composed of several input planes.

Applies a 3D power-average pooling over an input signal composed of several input planes.

Applies a 1D adaptive max pooling over an input signal composed of several input planes.

================================================================================

# torch.nn - Pooling layers (Part 3)

Applies a 2D adaptive max pooling over an input signal composed of several input planes.

Applies a 3D adaptive max pooling over an input signal composed of several input planes.

Applies a 1D adaptive average pooling over an input signal composed of several input planes.

Applies a 2D adaptive average pooling over an input signal composed of several input planes.

Applies a 3D adaptive average pooling over an input signal composed of several input planes.

================================================================================

# torch.nn - Padding Layers (Part 1)

Table:
nn.ReflectionPad1d | Pads the input tensor using the reflection of the input boundary.
nn.ReflectionPad2d | Pads the input tensor using the reflection of the input boundary.
nn.ReflectionPad3d | Pads the input tensor using the reflection of the input boundary.
nn.ReplicationPad1d | Pads the input tensor using replication of the input boundary.
nn.ReplicationPad2d | Pads the input tensor using replication of the input boundary.
nn.ReplicationPad3d | Pads the input tensor using replication of the input boundary.
nn.ZeroPad1d | Pads the input tensor boundaries with zero.
nn.ZeroPad2d | Pads the input tensor boundaries with zero.
nn.ZeroPad3d | Pads the input tensor boundaries with zero.
nn.ConstantPad1d | Pads the input tensor boundaries with a constant value.
nn.ConstantPad2d | Pads the input tensor boundaries with a constant value.
nn.ConstantPad3d | Pads the input tensor boundaries with a constant value.
nn.CircularPad1d | Pads the input tensor using circular padding of the input boundary.
nn.CircularPad2d | Pads the input tensor using circular padding of the input boundary.
nn.CircularPad3d | Pads the input tensor using circular padding of the input boundary.

================================================================================

# torch.nn - Padding Layers (Part 2)

Pads the input tensor using the reflection of the input boundary.

Pads the input tensor using the reflection of the input boundary.

Pads the input tensor using the reflection of the input boundary.

Pads the input tensor using replication of the input boundary.

Pads the input tensor using replication of the input boundary.

Pads the input tensor using replication of the input boundary.

Pads the input tensor boundaries with zero.

Pads the input tensor boundaries with zero.

Pads the input tensor boundaries with zero.

Pads the input tensor boundaries with a constant value.

Pads the input tensor boundaries with a constant value.

Pads the input tensor boundaries with a constant value.

Pads the input tensor using circular padding of the input boundary.

Pads the input tensor using circular padding of the input boundary.

Pads the input tensor using circular padding of the input boundary.

================================================================================

# torch.nn - Non-linear Activations (weighted sum, nonlinearity) (Part 1)

Table:
Applies the Exponential Linear Unit (ELU) function, element-wise.
nn.Hardshrink | Applies the Hard Shrinkage (Hardshrink) function element-wise.
nn.Hardsigmoid | Applies the Hardsigmoid function element-wise.
nn.Hardtanh | Applies the HardTanh function element-wise.
nn.Hardswish | Applies the Hardswish function, element-wise.
nn.LeakyReLU | Applies the LeakyReLU function element-wise.
nn.LogSigmoid | Applies the Logsigmoid function element-wise.
nn.MultiheadAttention | Allows the model to jointly attend to information from different representation subspaces.
Applies the element-wise PReLU function.
Applies the rectified linear unit function element-wise.
Applies the ReLU6 function element-wise.
Applies the randomized leaky rectified linear unit function, element-wise.
Applies the SELU function element-wise.
Applies the CELU function element-wise.
Applies the Gaussian Error Linear Units function.
nn.Sigmoid | Applies the Sigmoid function element-wise.
Applies the Sigmoid Linear Unit (SiLU) function, element-wise.
Applies the Mish function, element-wise.
nn.Softplus | Applies the Softplus function element-wise.
nn.Softshrink | Applies the soft shrinkage function element-wise.
nn.Softsign | Applies the element-wise Softsign function.
Applies the Hyperbolic Tangent (Tanh) function element-wise.
nn.Tanhshrink | Applies the element-wise Tanhshrink function.
nn.Threshold | Thresholds each element of the input Tensor.
Applies the gated linear unit function.

================================================================================

# torch.nn - Non-linear Activations (weighted sum, nonlinearity) (Part 2)

Applies the Exponential Linear Unit (ELU) function, element-wise.

Applies the Hard Shrinkage (Hardshrink) function element-wise.

Applies the Hardsigmoid function element-wise.

Applies the HardTanh function element-wise.

Applies the Hardswish function, element-wise.

Applies the LeakyReLU function element-wise.

Applies the Logsigmoid function element-wise.

nn.MultiheadAttention

Allows the model to jointly attend to information from different representation subspaces.

Applies the element-wise PReLU function.

Applies the rectified linear unit function element-wise.

Applies the ReLU6 function element-wise.

Applies the randomized leaky rectified linear unit function, element-wise.

Applies the SELU function element-wise.

Applies the CELU function element-wise.

Applies the Gaussian Error Linear Units function.

Applies the Sigmoid function element-wise.

Applies the Sigmoid Linear Unit (SiLU) function, element-wise.

Applies the Mish function, element-wise.

Applies the Softplus function element-wise.

Applies the soft shrinkage function element-wise.

Applies the element-wise Softsign function.

Applies the Hyperbolic Tangent (Tanh) function element-wise.

Applies the element-wise Tanhshrink function.

================================================================================

# torch.nn - Non-linear Activations (weighted sum, nonlinearity) (Part 3)

Thresholds each element of the input Tensor.

Applies the gated linear unit function.

================================================================================

# torch.nn - Non-linear Activations (other)

Table:
nn.Softmin | Applies the Softmin function to an n-dimensional input Tensor.
nn.Softmax | Applies the Softmax function to an n-dimensional input Tensor.
nn.Softmax2d | Applies SoftMax over features to each spatial location.
nn.LogSoftmax | Applies the \log(\text{Softmax}(x))lo function to an n-dimensional input Tensor.
nn.AdaptiveLogSoftmaxWithLoss | Efficient softmax approximation.

Applies the Softmin function to an n-dimensional input Tensor.

Applies the Softmax function to an n-dimensional input Tensor.

Applies SoftMax over features to each spatial location.

Applies the \log(\text{Softmax}(x))lo function to an n-dimensional input Tensor.

nn.AdaptiveLogSoftmaxWithLoss

Efficient softmax approximation.

================================================================================

# torch.nn - Normalization Layers (Part 1)

Table:
nn.BatchNorm1d | Applies Batch Normalization over a 2D or 3D input.
nn.BatchNorm2d | Applies Batch Normalization over a 4D input.
nn.BatchNorm3d | Applies Batch Normalization over a 5D input.
nn.LazyBatchNorm1d | A torch.nn.BatchNorm1d module with lazy initialization.
nn.LazyBatchNorm2d | A torch.nn.BatchNorm2d module with lazy initialization.
nn.LazyBatchNorm3d | A torch.nn.BatchNorm3d module with lazy initialization.
nn.GroupNorm | Applies Group Normalization over a mini-batch of inputs.
nn.SyncBatchNorm | Applies Batch Normalization over a N-Dimensional input.
nn.InstanceNorm1d | Applies Instance Normalization.
nn.InstanceNorm2d | Applies Instance Normalization.
nn.InstanceNorm3d | Applies Instance Normalization.
nn.LazyInstanceNorm1d | A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument.
nn.LazyInstanceNorm2d | A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument.
nn.LazyInstanceNorm3d | A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument.
nn.LayerNorm | Applies Layer Normalization over a mini-batch of inputs.
nn.LocalResponseNorm | Applies local response normalization over an input signal.
nn.RMSNorm | Applies Root Mean Square Layer Normalization over a mini-batch of inputs.

================================================================================

# torch.nn - Normalization Layers (Part 2)

Applies Batch Normalization over a 2D or 3D input.

Applies Batch Normalization over a 4D input.

Applies Batch Normalization over a 5D input.

A torch.nn.BatchNorm1d module with lazy initialization.

A torch.nn.BatchNorm2d module with lazy initialization.

A torch.nn.BatchNorm3d module with lazy initialization.

Applies Group Normalization over a mini-batch of inputs.

Applies Batch Normalization over a N-Dimensional input.

Applies Instance Normalization.

Applies Instance Normalization.

Applies Instance Normalization.

nn.LazyInstanceNorm1d

A torch.nn.InstanceNorm1d module with lazy initialization of the num_features argument.

nn.LazyInstanceNorm2d

A torch.nn.InstanceNorm2d module with lazy initialization of the num_features argument.

nn.LazyInstanceNorm3d

A torch.nn.InstanceNorm3d module with lazy initialization of the num_features argument.

Applies Layer Normalization over a mini-batch of inputs.

Applies local response normalization over an input signal.

Applies Root Mean Square Layer Normalization over a mini-batch of inputs.

================================================================================

# torch.nn - Recurrent Layers

Table:
nn.RNNBase | Base class for RNN modules (RNN, LSTM, GRU).
Apply a multi-layer Elman RNN with  or \text{ReLU} non-linearity to an input sequence.
Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence.
Apply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.
nn.RNNCell | An Elman RNN cell with tanh or ReLU non-linearity.
nn.LSTMCell | A long short-term memory (LSTM) cell.
nn.GRUCell | A gated recurrent unit (GRU) cell.

Base class for RNN modules (RNN, LSTM, GRU).

Apply a multi-layer Elman RNN with  or \text{ReLU} non-linearity to an input sequence.

Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence.

Apply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.

An Elman RNN cell with tanh or ReLU non-linearity.

A long short-term memory (LSTM) cell.

A gated recurrent unit (GRU) cell.

================================================================================

# torch.nn - Transformer Layers

Table:
nn.Transformer | A basic transformer layer.
nn.TransformerEncoder | TransformerEncoder is a stack of N encoder layers.
nn.TransformerDecoder | TransformerDecoder is a stack of N decoder layers.
nn.TransformerEncoderLayer | TransformerEncoderLayer is made up of self-attn and feedforward network.
nn.TransformerDecoderLayer | TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.

A basic transformer layer.

nn.TransformerEncoder

TransformerEncoder is a stack of N encoder layers.

nn.TransformerDecoder

TransformerDecoder is a stack of N decoder layers.

nn.TransformerEncoderLayer

TransformerEncoderLayer is made up of self-attn and feedforward network.

nn.TransformerDecoderLayer

TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.

================================================================================

# torch.nn - Linear Layers

Table:
nn.Identity | A placeholder identity operator that is argument-insensitive.
Applies an affine linear transformation to the incoming data: y = xA^T + b.
nn.Bilinear | Applies a bilinear transformation to the incoming data: y = x_1^T A x_2 + b.
nn.LazyLinear | A torch.nn.Linear module where in_features is inferred.

A placeholder identity operator that is argument-insensitive.

Applies an affine linear transformation to the incoming data: y = xA^T + b.

Applies a bilinear transformation to the incoming data: y = x_1^T A x_2 + b.

A torch.nn.Linear module where in_features is inferred.

================================================================================

# torch.nn - Dropout Layers

Table:
nn.Dropout | During training, randomly zeroes some of the elements of the input tensor with probability .
nn.Dropout1d | Randomly zero out entire channels.
nn.Dropout2d | Randomly zero out entire channels.
nn.Dropout3d | Randomly zero out entire channels.
nn.AlphaDropout | Applies Alpha Dropout over the input.
nn.FeatureAlphaDropout | Randomly masks out entire channels.

During training, randomly zeroes some of the elements of the input tensor with probability .

Randomly zero out entire channels.

Randomly zero out entire channels.

Randomly zero out entire channels.

Applies Alpha Dropout over the input.

nn.FeatureAlphaDropout

Randomly masks out entire channels.

================================================================================

# torch.nn - Sparse Layers

Table:
nn.Embedding | A simple lookup table that stores embeddings of a fixed dictionary and size.
nn.EmbeddingBag | Compute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.

A simple lookup table that stores embeddings of a fixed dictionary and size.

Compute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.

================================================================================

# torch.nn - Distance Functions

Table:
nn.CosineSimilarity | Returns cosine similarity between  and , computed along .
nn.PairwiseDistance | Computes the pairwise distance between input vectors, or between columns of input matrices.

Returns cosine similarity between  and , computed along .

Computes the pairwise distance between input vectors, or between columns of input matrices.

================================================================================

# torch.nn - Loss Functions (Part 1)

Table:
Creates a criterion that measures the mean absolute error (MAE) between each element in the input  and target .
nn.MSELoss | Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input  and target .
nn.CrossEntropyLoss | This criterion computes the cross entropy loss between input logits and target.
nn.CTCLoss | The Connectionist Temporal Classification loss.
nn.NLLLoss | The negative log likelihood loss.
nn.PoissonNLLLoss | Negative log likelihood loss with Poisson distribution of target.
nn.GaussianNLLLoss | Gaussian negative log likelihood loss.
nn.KLDivLoss | The Kullback-Leibler divergence loss.
nn.BCELoss | Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:
nn.BCEWithLogitsLoss | This loss combines a  layer and the  in one single class.
nn.MarginRankingLoss | Creates a criterion that measures the loss given inputs , , two 1D mini-batch or 0D , and a label 1D mini-batch or 0D   (containing 1 or -1).
nn.HingeEmbeddingLoss | Measures the loss given an input tensor  and a labels tensor  (containing 1 or -1).
nn.MultiLabelMarginLoss | Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input  (a 2D mini-batch ) and output  (which is a 2D  of target class indices).
nn.HuberLoss | Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.
nn.SmoothL1Loss | Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.
nn.SoftMarginLoss | Creates a criterion that optimizes a two-class classification logistic loss between input tensor  and target tensor  (containing 1 or -1).
nn.MultiLabelSoftMarginLoss | Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input  and target  of size .
nn.CosineEmbeddingLoss | Creates a criterion that measures the loss given input tensors ,  and a  label  with values 1 or -1.
nn.MultiMarginLoss | Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input  (a 2D mini-batch ) and output  (which is a 1D tensor of target class indices, 0 \leq y \leq \text{x.size}(1)-1):
nn.TripletMarginLoss | Creates a criterion that measures the triplet loss given an input tensors , ,  and a margin with a value greater than .
nn.TripletMarginWithDistanceLoss | Creates a criterion that measures the triplet loss given input tensors , , and  (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function ("distance function") used to compute the relationship between the anchor and positive example ("positive distance") and the anchor and negative example ("negative distance").

================================================================================

# torch.nn - Loss Functions (Part 2)

Creates a criterion that measures the mean absolute error (MAE) between each element in the input  and target .

Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input  and target .

This criterion computes the cross entropy loss between input logits and target.

The Connectionist Temporal Classification loss.

The negative log likelihood loss.

Negative log likelihood loss with Poisson distribution of target.

Gaussian negative log likelihood loss.

The Kullback-Leibler divergence loss.

Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:

This loss combines a  layer and the  in one single class.

Creates a criterion that measures the loss given inputs , , two 1D mini-batch or 0D , and a label 1D mini-batch or 0D   (containing 1 or -1).

nn.HingeEmbeddingLoss

Measures the loss given an input tensor  and a labels tensor  (containing 1 or -1).

nn.MultiLabelMarginLoss

Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input  (a 2D mini-batch ) and output  (which is a 2D  of target class indices).

================================================================================

# torch.nn - Loss Functions (Part 3)

Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.

Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.

Creates a criterion that optimizes a two-class classification logistic loss between input tensor  and target tensor  (containing 1 or -1).

nn.MultiLabelSoftMarginLoss

Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input  and target  of size .

nn.CosineEmbeddingLoss

Creates a criterion that measures the loss given input tensors ,  and a  label  with values 1 or -1.

Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input  (a 2D mini-batch ) and output  (which is a 1D tensor of target class indices, 0 \leq y \leq \text{x.size}(1)-1):

Creates a criterion that measures the triplet loss given an input tensors , ,  and a margin with a value greater than .

nn.TripletMarginWithDistanceLoss

================================================================================

# torch.nn - Loss Functions (Part 4)

Creates a criterion that measures the triplet loss given input tensors , , and  (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function ("distance function") used to compute the relationship between the anchor and positive example ("positive distance") and the anchor and negative example ("negative distance").

================================================================================

# torch.nn - Vision Layers

Table:
nn.PixelShuffle | Rearrange elements in a tensor according to an upscaling factor.
nn.PixelUnshuffle | Reverse the PixelShuffle operation.
nn.Upsample | Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.
nn.UpsamplingNearest2d | Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.
nn.UpsamplingBilinear2d | Applies a 2D bilinear upsampling to an input signal composed of several input channels.

Rearrange elements in a tensor according to an upscaling factor.

Reverse the PixelShuffle operation.

Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.

nn.UpsamplingNearest2d

Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.

nn.UpsamplingBilinear2d

Applies a 2D bilinear upsampling to an input signal composed of several input channels.

================================================================================

# torch.nn - Shuffle Layers

Table:
nn.ChannelShuffle | Divides and rearranges the channels in a tensor.

Divides and rearranges the channels in a tensor.

================================================================================

# torch.nn - DataParallel Layers (multi-GPU, distributed)

Table:
nn.DataParallel | Implements data parallelism at the module level.
nn.parallel.DistributedDataParallel | Implement distributed data parallelism based on torch.distributed at module level.

Implements data parallelism at the module level.

nn.parallel.DistributedDataParallel

Implement distributed data parallelism based on torch.distributed at module level.

================================================================================

# torch.nn -  (Part 1)

From the torch.nn.utils module:

Utility functions to clip parameter gradients.

Table:
clip_grad_norm_ | Clip the gradient norm of an iterable of parameters.
clip_grad_norm | Clip the gradient norm of an iterable of parameters.
clip_grad_value_ | Clip the gradients of an iterable of parameters at specified value.
get_total_norm | Compute the norm of an iterable of tensors.
clip_grads_with_norm_ | Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.

Clip the gradient norm of an iterable of parameters.

Clip the gradient norm of an iterable of parameters.

Clip the gradients of an iterable of parameters at specified value.

Compute the norm of an iterable of tensors.

clip_grads_with_norm_

Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.

Utility functions to flatten and unflatten Module parameters to and from a single vector.

Table:
parameters_to_vector | Flatten an iterable of parameters into a single vector.
vector_to_parameters | Copy slices of a vector into an iterable of parameters.

Flatten an iterable of parameters into a single vector.

================================================================================

# torch.nn -  (Part 2)

Copy slices of a vector into an iterable of parameters.

Utility functions to fuse Modules with BatchNorm modules.

Table:
fuse_conv_bn_eval | Fuse a convolutional module and a BatchNorm module into a single, new convolutional module.
fuse_conv_bn_weights | Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.
fuse_linear_bn_eval | Fuse a linear module and a BatchNorm module into a single, new linear module.
fuse_linear_bn_weights | Fuse linear module parameters and BatchNorm module parameters into new linear module parameters.

Fuse a convolutional module and a BatchNorm module into a single, new convolutional module.

Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.

Fuse a linear module and a BatchNorm module into a single, new linear module.

fuse_linear_bn_weights

Fuse linear module parameters and BatchNorm module parameters into new linear module parameters.

Utility functions to convert Module parameter memory formats.

================================================================================

# torch.nn -  (Part 3)

Table:
convert_conv2d_weight_memory_format | Convert memory_format of nn.Conv2d.weight to memory_format.
convert_conv3d_weight_memory_format | Convert memory_format of nn.Conv3d.weight to memory_format The conversion recursively applies to nested , including .

convert_conv2d_weight_memory_format

Convert memory_format of nn.Conv2d.weight to memory_format.

convert_conv3d_weight_memory_format

Convert memory_format of nn.Conv3d.weight to memory_format The conversion recursively applies to nested , including .

Utility functions to apply and remove weight normalization from Module parameters.

Table:
weight_norm | Apply weight normalization to a parameter in the given module.
remove_weight_norm | Remove the weight normalization reparameterization from a module.
spectral_norm | Apply spectral normalization to a parameter in the given module.
remove_spectral_norm | Remove the spectral normalization reparameterization from a module.

Apply weight normalization to a parameter in the given module.

Remove the weight normalization reparameterization from a module.

Apply spectral normalization to a parameter in the given module.

Remove the spectral normalization reparameterization from a module.

================================================================================

# torch.nn -  (Part 4)

Utility functions for initializing Module parameters.

Table:
 | Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.

Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.

Utility classes and functions for pruning Module parameters.

================================================================================

# torch.nn -  (Part 5)

Table:
prune.BasePruningMethod | Abstract base class for creation of new pruning techniques.
prune.PruningContainer | Container holding a sequence of pruning methods for iterative pruning.
prune.Identity | Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.
prune.RandomUnstructured | Prune (currently unpruned) units in a tensor at random.
prune.L1Unstructured | Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.
prune.RandomStructured | Prune entire (currently unpruned) channels in a tensor at random.
prune.LnStructured | Prune entire (currently unpruned) channels in a tensor based on their L-norm.
prune.CustomFromMask | 
prune.identity | Apply pruning reparametrization without pruning any units.
prune.random_unstructured | Prune tensor by removing random (currently unpruned) units.
prune.l1_unstructured | Prune tensor by removing units with the lowest L1-norm.
prune.random_structured | Prune tensor by removing random channels along the specified dimension.
prune.ln_structured | Prune tensor by removing channels with the lowest L-norm along the specified dimension.
prune.global_unstructured | Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.
prune.custom_from_mask | Prune tensor corresponding to parameter called  in  by applying the pre-computed mask in .
prune.remove | Remove the pruning reparameterization from a module and the pruning method from the forward hook.
prune.is_pruned | Check if a module is pruned by looking for pruning pre-hooks.

================================================================================

# torch.nn -  (Part 6)

prune.BasePruningMethod

Abstract base class for creation of new pruning techniques.

prune.PruningContainer

Container holding a sequence of pruning methods for iterative pruning.

Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.

prune.RandomUnstructured

Prune (currently unpruned) units in a tensor at random.

Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.

prune.RandomStructured

Prune entire (currently unpruned) channels in a tensor at random.

Prune entire (currently unpruned) channels in a tensor based on their L-norm.

Apply pruning reparametrization without pruning any units.

prune.random_unstructured

Prune tensor by removing random (currently unpruned) units.

prune.l1_unstructured

Prune tensor by removing units with the lowest L1-norm.

prune.random_structured

Prune tensor by removing random channels along the specified dimension.

Prune tensor by removing channels with the lowest L-norm along the specified dimension.

prune.global_unstructured

Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method.

prune.custom_from_mask

================================================================================

# torch.nn -  (Part 7)

Prune tensor corresponding to parameter called  in  by applying the pre-computed mask in .

Remove the pruning reparameterization from a module and the pruning method from the forward hook.

Check if a module is pruned by looking for pruning pre-hooks.

Parametrizations implemented using the new parametrization functionality
in torch.nn.utils.parameterize.register_parametrization().

Table:
parametrizations.orthogonal | Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices.
parametrizations.weight_norm | Apply weight normalization to a parameter in the given module.
parametrizations.spectral_norm | Apply spectral normalization to a parameter in the given module.

parametrizations.orthogonal

Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices.

parametrizations.weight_norm

Apply weight normalization to a parameter in the given module.

parametrizations.spectral_norm

Apply spectral normalization to a parameter in the given module.

================================================================================

# torch.nn -  (Part 8)

Utility functions to parametrize Tensors on existing Modules.
Note that these functions can be used to parametrize a given Parameter
or Buffer given a specific function that maps from an input space to the
parametrized space. They are not parameterizations that would transform
an object into a parameter. See the
Parametrizations tutorial
for more information on how to implement your own parametrizations.

Table:
parametrize.register_parametrization | Register a parametrization to a tensor in a module.
parametrize.remove_parametrizations | Remove the parametrizations on a tensor in a module.
parametrize.cached | Context manager that enables the caching system within parametrizations registered with register_parametrization().
parametrize.is_parametrized | Determine if a module has a parametrization.

parametrize.register_parametrization

Register a parametrization to a tensor in a module.

parametrize.remove_parametrizations

Remove the parametrizations on a tensor in a module.

Context manager that enables the caching system within parametrizations registered with register_parametrization().

parametrize.is_parametrized

Determine if a module has a parametrization.

================================================================================

# torch.nn -  (Part 9)

Table:
parametrize.ParametrizationList | A sequential container that holds and manages the original parameters or buffers of a parametrized torch.nn.Module.

parametrize.ParametrizationList

A sequential container that holds and manages the original parameters or buffers of a parametrized torch.nn.Module.

Utility functions to call a given Module in a stateless manner.

Table:
stateless.functional_call | Perform a functional call on the module by replacing the module parameters and buffers with the provided ones.

stateless.functional_call

Perform a functional call on the module by replacing the module parameters and buffers with the provided ones.

Utility functions in other modules

================================================================================

# torch.nn -  (Part 10)

Table:
nn.utils.rnn.PackedSequence | Holds the data and list of batch_sizes of a packed sequence.
nn.utils.rnn.pack_padded_sequence | Packs a Tensor containing padded sequences of variable length.
nn.utils.rnn.pad_packed_sequence | Pad a packed batch of variable length sequences.
nn.utils.rnn.pad_sequence | Pad a list of variable length Tensors with padding_value.
nn.utils.rnn.pack_sequence | Packs a list of variable length Tensors.
nn.utils.rnn.unpack_sequence | Unpack PackedSequence into a list of variable length Tensors.
nn.utils.rnn.unpad_sequence | Unpad padded Tensor into a list of variable length Tensors.

nn.utils.rnn.PackedSequence

Holds the data and list of batch_sizes of a packed sequence.

nn.utils.rnn.pack_padded_sequence

Packs a Tensor containing padded sequences of variable length.

nn.utils.rnn.pad_packed_sequence

Pad a packed batch of variable length sequences.

nn.utils.rnn.pad_sequence

Pad a list of variable length Tensors with padding_value.

nn.utils.rnn.pack_sequence

Packs a list of variable length Tensors.

nn.utils.rnn.unpack_sequence

Unpack PackedSequence into a list of variable length Tensors.

nn.utils.rnn.unpad_sequence

================================================================================

# torch.nn -  (Part 11)

Unpad padded Tensor into a list of variable length Tensors.

Table:
nn.Flatten | Flattens a contiguous range of dims into a tensor.
nn.Unflatten | Unflattens a tensor dim expanding it to a desired shape.

Flattens a contiguous range of dims into a tensor.

Unflattens a tensor dim expanding it to a desired shape.

================================================================================

# torch.nn - Quantized Functions

Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than
floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the Quantization documentation.

================================================================================

# torch.nn - Lazy Modules Initialization

Table:
nn.modules.lazy.LazyModuleMixin | A mixin for modules that lazily initialize parameters, also known as "lazy modules".

nn.modules.lazy.LazyModuleMixin

A mixin for modules that lazily initialize parameters, also known as "lazy modules".

================================================================================

# torch.nn - 

The following are aliases to their counterparts in :

Table:
nn.modules.normalization.RMSNorm | Applies Root Mean Square Layer Normalization over a mini-batch of inputs.

nn.modules.normalization.RMSNorm

Applies Root Mean Square Layer Normalization over a mini-batch of inputs.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.nn.functional

Created On: Jun 11, 2019 | Last Updated On: Mar 25, 2024

================================================================================

# torch.nn.functional - Convolution functions (Part 1)

Table:
 | Applies a 1D convolution over an input signal composed of several input planes.
 | Applies a 2D convolution over an input image composed of several input planes.
 | Applies a 3D convolution over an input image composed of several input planes.
conv_transpose1d | Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called "deconvolution".
conv_transpose2d | Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution".
conv_transpose3d | Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution"
 | Extract sliding local blocks from a batched input tensor.
 | Combine an array of sliding local blocks into a large containing tensor.

Applies a 1D convolution over an input signal composed of several input planes.

Applies a 2D convolution over an input image composed of several input planes.

Applies a 3D convolution over an input image composed of several input planes.

================================================================================

# torch.nn.functional - Convolution functions (Part 2)

Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called "deconvolution".

Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution".

Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution"

Extract sliding local blocks from a batched input tensor.

Combine an array of sliding local blocks into a large containing tensor.

================================================================================

# torch.nn.functional - Pooling functions (Part 1)

Table:
avg_pool1d | Applies a 1D average pooling over an input signal composed of several input planes.
avg_pool2d | Applies 2D average-pooling operation in kH \times kW regions by step size sH \times sW steps.
avg_pool3d | Applies 3D average-pooling operation in kT \times kH \times kW regions by step size sT \times sH \times sW steps.
max_pool1d | Applies a 1D max pooling over an input signal composed of several input planes.
max_pool2d | Applies a 2D max pooling over an input signal composed of several input planes.
max_pool3d | Applies a 3D max pooling over an input signal composed of several input planes.
max_unpool1d | Compute a partial inverse of .
max_unpool2d | Compute a partial inverse of .
max_unpool3d | Compute a partial inverse of .
 | Apply a 1D power-average pooling over an input signal composed of several input planes.
 | Apply a 2D power-average pooling over an input signal composed of several input planes.
 | Apply a 3D power-average pooling over an input signal composed of several input planes.
adaptive_max_pool1d | Applies a 1D adaptive max pooling over an input signal composed of several input planes.
adaptive_max_pool2d | Applies a 2D adaptive max pooling over an input signal composed of several input planes.
adaptive_max_pool3d | Applies a 3D adaptive max pooling over an input signal composed of several input planes.
adaptive_avg_pool1d | Applies a 1D adaptive average pooling over an input signal composed of several input planes.
adaptive_avg_pool2d | Apply a 2D adaptive average pooling over an input signal composed of several input planes.
adaptive_avg_pool3d | Apply a 3D adaptive average pooling over an input signal composed of several input planes.
fractional_max_pool2d | Applies 2D fractional max pooling over an input signal composed of several input planes.
fractional_max_pool3d | Applies 3D fractional max pooling over an input signal composed of several input planes.

================================================================================

# torch.nn.functional - Pooling functions (Part 2)

Applies a 1D average pooling over an input signal composed of several input planes.

Applies 2D average-pooling operation in kH \times kW regions by step size sH \times sW steps.

Applies 3D average-pooling operation in kT \times kH \times kW regions by step size sT \times sH \times sW steps.

Applies a 1D max pooling over an input signal composed of several input planes.

Applies a 2D max pooling over an input signal composed of several input planes.

Applies a 3D max pooling over an input signal composed of several input planes.

Compute a partial inverse of .

Compute a partial inverse of .

Compute a partial inverse of .

Apply a 1D power-average pooling over an input signal composed of several input planes.

Apply a 2D power-average pooling over an input signal composed of several input planes.

Apply a 3D power-average pooling over an input signal composed of several input planes.

Applies a 1D adaptive max pooling over an input signal composed of several input planes.

Applies a 2D adaptive max pooling over an input signal composed of several input planes.

Applies a 3D adaptive max pooling over an input signal composed of several input planes.

================================================================================

# torch.nn.functional - Pooling functions (Part 3)

Applies a 1D adaptive average pooling over an input signal composed of several input planes.

Apply a 2D adaptive average pooling over an input signal composed of several input planes.

Apply a 3D adaptive average pooling over an input signal composed of several input planes.

fractional_max_pool2d

Applies 2D fractional max pooling over an input signal composed of several input planes.

fractional_max_pool3d

Applies 3D fractional max pooling over an input signal composed of several input planes.

================================================================================

# torch.nn.functional - Attention Mechanisms

The torch.nn.attention.bias module contains attention_biases that are designed to be used with
scaled_dot_product_attention.

Table:
scaled_dot_product_attention | scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,

scaled_dot_product_attention

scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,

================================================================================

# torch.nn.functional - Non-linear activation functions (Part 1)

Table:
 | Apply a threshold to each element of the input Tensor.
threshold_ | In-place version of threshold().
 | Applies the rectified linear unit function element-wise.
 | In-place version of .
 | Applies the HardTanh function element-wise.
 | In-place version of hardtanh().
 | Apply hardswish function, element-wise.
 | Applies the element-wise function \text{ReLU6}(x) = \min(\max(0,x), 6).
 | Apply the Exponential Linear Unit (ELU) function element-wise.
 | In-place version of .
 | Applies element-wise, \text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1))), with 1.6732632423543772848170429916717\alpha=1.67326324235437728481704299167171.6732632423543772848170429916717 and 1.0507009873554804934193349852946scale=1.05070098735548049341933498529461.0507009873554804934193349852946.
 | Applies element-wise, \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1)).
leaky_relu | Applies element-wise, negative_slope\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)negative_slope
leaky_relu_ | In-place version of leaky_relu().
 | Applies element-wise the function \text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x) where weight is a learnable parameter.
 | Randomized leaky ReLU.
 | In-place version of .
 | The gated linear unit.
 | When the approximate argument is 'none', it applies element-wise the function \text{GELU}(x) = x * \Phi(x)
logsigmoid | Applies element-wise LogSigmoid\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)LogSigmoidlo
hardshrink | Applies the hard shrinkage function element-wise
tanhshrink | Applies element-wise, Tanhshrink\text{Tanhshrink}(x) = x - \text{Tanh}(x)Tanhshrink
 | Applies element-wise, the function \text{SoftSign}(x) = \frac{x}{1 + |x|}
 | Applies element-wise, the function \text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))lo.
 | Apply a softmin function.
 | Apply a softmax function.
softshrink | Applies the soft shrinkage function elementwise
gumbel_softmax | Sample from the Gumbel-Softmax distribution ( ) and optionally discretize.
log_softmax | Apply a softmax followed by a logarithm.
 | Applies element-wise, \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}
 | Applies the element-wise function \text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}
hardsigmoid | Apply the Hardsigmoid function element-wise.
 | Apply the Sigmoid Linear Unit (SiLU) function, element-wise.
 | Apply the Mish function, element-wise.
batch_norm | Apply Batch Normalization for each channel across a batch of data.
group_norm | Apply Group Normalization for last certain number of dimensions.
instance_norm | Apply Instance Normalization independently for each channel in every data sample within a batch.
layer_norm | Apply Layer Normalization for last certain number of dimensions.
local_response_norm | Apply local response normalization over an input signal.
 | Apply Root Mean Square Layer Normalization.
 | Perform  normalization of inputs over specified dimension.

================================================================================

# torch.nn.functional - Non-linear activation functions (Part 2)

Apply a threshold to each element of the input Tensor.

In-place version of threshold().

Applies the rectified linear unit function element-wise.

In-place version of .

Applies the HardTanh function element-wise.

In-place version of hardtanh().

Apply hardswish function, element-wise.

Applies the element-wise function \text{ReLU6}(x) = \min(\max(0,x), 6).

Apply the Exponential Linear Unit (ELU) function element-wise.

In-place version of .

Applies element-wise, \text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1))), with 1.6732632423543772848170429916717\alpha=1.67326324235437728481704299167171.6732632423543772848170429916717 and 1.0507009873554804934193349852946scale=1.05070098735548049341933498529461.0507009873554804934193349852946.

Applies element-wise, \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1)).

Applies element-wise, negative_slope\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)negative_slope

In-place version of leaky_relu().

Applies element-wise the function \text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x) where weight is a learnable parameter.

Randomized leaky ReLU.

In-place version of .

The gated linear unit.

================================================================================

# torch.nn.functional - Non-linear activation functions (Part 3)

When the approximate argument is 'none', it applies element-wise the function \text{GELU}(x) = x * \Phi(x)

Applies element-wise LogSigmoid\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)LogSigmoidlo

Applies the hard shrinkage function element-wise

Applies element-wise, Tanhshrink\text{Tanhshrink}(x) = x - \text{Tanh}(x)Tanhshrink

Applies element-wise, the function \text{SoftSign}(x) = \frac{x}{1 + |x|}

Applies element-wise, the function \text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))lo.

Apply a softmin function.

Apply a softmax function.

Applies the soft shrinkage function elementwise

Sample from the Gumbel-Softmax distribution ( ) and optionally discretize.

Apply a softmax followed by a logarithm.

Applies element-wise, \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}

Applies the element-wise function \text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}

Apply the Hardsigmoid function element-wise.

Apply the Sigmoid Linear Unit (SiLU) function, element-wise.

Apply the Mish function, element-wise.

Apply Batch Normalization for each channel across a batch of data.

Apply Group Normalization for last certain number of dimensions.

================================================================================

# torch.nn.functional - Non-linear activation functions (Part 4)

Apply Instance Normalization independently for each channel in every data sample within a batch.

Apply Layer Normalization for last certain number of dimensions.

Apply local response normalization over an input signal.

Apply Root Mean Square Layer Normalization.

Perform  normalization of inputs over specified dimension.

================================================================================

# torch.nn.functional - Linear functions

Table:
 | Applies a linear transformation to the incoming data: y = xA^T + b.
 | Applies a bilinear transformation to the incoming data: y = x_1^T A x_2 + b

Applies a linear transformation to the incoming data: y = xA^T + b.

Applies a bilinear transformation to the incoming data: y = x_1^T A x_2 + b

================================================================================

# torch.nn.functional - Dropout functions

Table:
 | During training, randomly zeroes some elements of the input tensor with probability .
alpha_dropout | Apply alpha dropout to the input.
feature_alpha_dropout | Randomly masks out entire channels (a channel is a feature map).
 | Randomly zero out entire channels (a channel is a 1D feature map).
 | Randomly zero out entire channels (a channel is a 2D feature map).
 | Randomly zero out entire channels (a channel is a 3D feature map).

During training, randomly zeroes some elements of the input tensor with probability .

Apply alpha dropout to the input.

feature_alpha_dropout

Randomly masks out entire channels (a channel is a feature map).

Randomly zero out entire channels (a channel is a 1D feature map).

Randomly zero out entire channels (a channel is a 2D feature map).

Randomly zero out entire channels (a channel is a 3D feature map).

================================================================================

# torch.nn.functional - Sparse functions

Table:
 | Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.
embedding_bag | Compute sums, means or maxes of  of embeddings.
 | Takes LongTensor with index values of shape  and returns a tensor of shape  num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.

Generate a simple lookup table that looks up embeddings in a fixed dictionary and size.

Compute sums, means or maxes of  of embeddings.

Takes LongTensor with index values of shape  and returns a tensor of shape  num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.

================================================================================

# torch.nn.functional - Distance functions

Table:
pairwise_distance | See torch.nn.PairwiseDistance for details
cosine_similarity | Returns cosine similarity between  and , computed along dim.
 | Computes the p-norm distance between every pair of row vectors in the input.

See torch.nn.PairwiseDistance for details

Returns cosine similarity between  and , computed along dim.

Computes the p-norm distance between every pair of row vectors in the input.

================================================================================

# torch.nn.functional - Loss functions (Part 1)

Table:
binary_cross_entropy | Compute Binary Cross Entropy between the target and input probabilities.
binary_cross_entropy_with_logits | Compute Binary Cross Entropy between target and input logits.
poisson_nll_loss | Compute the Poisson negative log likelihood loss.
cosine_embedding_loss | Compute the cosine embedding loss.
cross_entropy | Compute the cross entropy loss between input logits and target.
 | Compute the Connectionist Temporal Classification loss.
gaussian_nll_loss | Compute the Gaussian negative log likelihood loss.
hinge_embedding_loss | Compute the hinge embedding loss.
 | Compute the KL Divergence loss.
 | Compute the L1 loss, with optional weighting.
 | Compute the element-wise mean squared error, with optional weighting.
margin_ranking_loss | Compute the margin ranking loss.
multilabel_margin_loss | Compute the multilabel margin loss.
multilabel_soft_margin_loss | Compute the multilabel soft margin loss.
multi_margin_loss | Compute the multi margin loss, with optional weighting.
 | Compute the negative log likelihood loss.
huber_loss | Compute the Huber loss, with optional weighting.
smooth_l1_loss | Compute the Smooth L1 loss.
soft_margin_loss | Compute the soft margin loss.
triplet_margin_loss | Compute the triplet loss between given input tensors and a margin greater than 0.
triplet_margin_with_distance_loss | Compute the triplet margin loss for input tensors using a custom distance function.

================================================================================

# torch.nn.functional - Loss functions (Part 2)

Compute Binary Cross Entropy between the target and input probabilities.

binary_cross_entropy_with_logits

Compute Binary Cross Entropy between target and input logits.

Compute the Poisson negative log likelihood loss.

cosine_embedding_loss

Compute the cosine embedding loss.

Compute the cross entropy loss between input logits and target.

Compute the Connectionist Temporal Classification loss.

Compute the Gaussian negative log likelihood loss.

Compute the hinge embedding loss.

Compute the KL Divergence loss.

Compute the L1 loss, with optional weighting.

Compute the element-wise mean squared error, with optional weighting.

Compute the margin ranking loss.

multilabel_margin_loss

Compute the multilabel margin loss.

multilabel_soft_margin_loss

Compute the multilabel soft margin loss.

Compute the multi margin loss, with optional weighting.

Compute the negative log likelihood loss.

Compute the Huber loss, with optional weighting.

Compute the Smooth L1 loss.

Compute the soft margin loss.

Compute the triplet loss between given input tensors and a margin greater than 0.

triplet_margin_with_distance_loss

Compute the triplet margin loss for input tensors using a custom distance function.

================================================================================

# torch.nn.functional - Vision functions (Part 1)

Table:
pixel_shuffle | Rearranges elements in a tensor of shape (*, C \times r^2, H, W) to a tensor of shape (*, C, H \times r, W \times r), where r is the upscale_factor.
pixel_unshuffle | Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (*, C, H \times r, W \times r) to a tensor of shape (*, C \times r^2, H, W), where r is the downscale_factor.
 | Pads tensor.
interpolate | Down/up samples the input.
 | Upsample input.
upsample_nearest | Upsamples the input, using nearest neighbours' pixel values.
upsample_bilinear | Upsamples the input, using bilinear upsampling.
grid_sample | Compute grid sample.
affine_grid | Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices .

Rearranges elements in a tensor of shape (*, C \times r^2, H, W) to a tensor of shape (*, C, H \times r, W \times r), where r is the upscale_factor.

Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (*, C, H \times r, W \times r) to a tensor of shape (*, C \times r^2, H, W), where r is the downscale_factor.

Down/up samples the input.

Upsamples the input, using nearest neighbours' pixel values.

================================================================================

# torch.nn.functional - Vision functions (Part 2)

Upsamples the input, using bilinear upsampling.

Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices .

================================================================================

# torch.nn.functional - data_parallel

Table:
torch.nn.parallel.data_parallel | Evaluate module(input) in parallel across the GPUs given in device_ids.

torch.nn.parallel.data_parallel

Evaluate module(input) in parallel across the GPUs given in device_ids.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.Tensor

Created On: Dec 23, 2016 | Last Updated On: Jun 11, 2024

A torch.Tensor is a multi-dimensional matrix containing elements of
a single data type.

================================================================================

# torch.Tensor - Data types (Part 1)

Torch defines tensor types with the following data types:

Table:
32-bit floating point | torch.float32 or torch.float
64-bit floating point | torch.float64 or torch.double
16-bit floating point | torch.float16 or torch.half
16-bit floating point | torch.bfloat16
32-bit complex | torch.complex32 or torch.chalf
64-bit complex | torch.complex64 or torch.cfloat
128-bit complex | torch.complex128 or torch.cdouble
8-bit integer (unsigned) | torch.uint8
16-bit integer (unsigned) | torch.uint16 (limited support)
32-bit integer (unsigned) | torch.uint32 (limited support)
64-bit integer (unsigned) | torch.uint64 (limited support)
8-bit integer (signed) | torch.int8
16-bit integer (signed) | torch.int16 or torch.short
32-bit integer (signed) | torch.int32 or
64-bit integer (signed) | torch.int64 or torch.long
torch.bool
quantized 8-bit integer (unsigned) | torch.quint8
quantized 8-bit integer (signed) | torch.qint8
quantized 32-bit integer (signed) | torch.qint32
quantized 4-bit integer (unsigned) | torch.quint4x2
8-bit floating point, e4m3 | torch.float8_e4m3fn (limited support)
8-bit floating point, e5m2 | torch.float8_e5m2 (limited support)

32-bit floating point

torch.float32 or torch.float

================================================================================

# torch.Tensor - Data types (Part 2)

64-bit floating point

torch.float64 or torch.double

16-bit floating point

torch.float16 or torch.half

16-bit floating point

torch.complex32 or torch.chalf

torch.complex64 or torch.cfloat

torch.complex128 or torch.cdouble

8-bit integer (unsigned)

16-bit integer (unsigned)

torch.uint16 (limited support)

32-bit integer (unsigned)

torch.uint32 (limited support)

64-bit integer (unsigned)

torch.uint64 (limited support)

8-bit integer (signed)

16-bit integer (signed)

torch.int16 or torch.short

32-bit integer (signed)

64-bit integer (signed)

torch.int64 or torch.long

quantized 8-bit integer (unsigned)

quantized 8-bit integer (signed)

quantized 32-bit integer (signed)

quantized 4-bit integer (unsigned)

8-bit floating point, e4m3

torch.float8_e4m3fn (limited support)

8-bit floating point, e5m2

torch.float8_e5m2 (limited support)

Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10
significand bits. Useful when precision is important at the expense of range.


Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7
significand bits. Useful when range is important, since it has the same
number of exponent bits as 

================================================================================

# torch.Tensor - Data types (Part 3)


quantized 4-bit integer is stored as a 8-bit signed integer. Currently it’s only supported in EmbeddingBag operator.

(,,)
Unsigned types asides from  are currently planned to only have
limited support in eager mode (they primarily exist to assist usage with
torch.compile); if you need eager support and the extra range is not needed,
we recommend using their signed variants instead.  See
pytorch/pytorch#58734 for more details.

(,)
torch.float8_e4m3fn and torch.float8_e5m2 implement the spec for 8-bit
floating point types from https://arxiv.org/abs/2209.05433. The op support
is very limited.

Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10
significand bits. Useful when precision is important at the expense of range.

Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7
significand bits. Useful when range is important, since it has the same
number of exponent bits as

quantized 4-bit integer is stored as a 8-bit signed integer. Currently it’s only supported in EmbeddingBag operator.

================================================================================

# torch.Tensor - Data types (Part 4)

Unsigned types asides from  are currently planned to only have
limited support in eager mode (they primarily exist to assist usage with
torch.compile); if you need eager support and the extra range is not needed,
we recommend using their signed variants instead.  See
pytorch/pytorch#58734 for more details.

torch.float8_e4m3fn and torch.float8_e5m2 implement the spec for 8-bit
floating point types from https://arxiv.org/abs/2209.05433. The op support
is very limited.

For backwards compatibility, we support the following alternate class names
for these data types:

================================================================================

# torch.Tensor - Data types (Part 5)

Table:
CPU tensor | GPU tensor
32-bit floating point | torch.FloatTensor | torch.cuda.FloatTensor
64-bit floating point | torch.DoubleTensor | torch.cuda.DoubleTensor
16-bit floating point | torch.HalfTensor | torch.cuda.HalfTensor
16-bit floating point | torch.BFloat16Tensor | torch.cuda.BFloat16Tensor
8-bit integer (unsigned) | torch.ByteTensor | torch.cuda.ByteTensor
8-bit integer (signed) | torch.CharTensor | torch.cuda.CharTensor
16-bit integer (signed) | torch.ShortTensor | torch.cuda.ShortTensor
32-bit integer (signed) | torch.IntTensor | torch.cuda.IntTensor
64-bit integer (signed) | torch.LongTensor | torch.cuda.LongTensor
torch.BoolTensor | torch.cuda.BoolTensor

32-bit floating point

torch.cuda.FloatTensor

64-bit floating point

torch.cuda.DoubleTensor

16-bit floating point

torch.cuda.HalfTensor

16-bit floating point

torch.cuda.BFloat16Tensor

8-bit integer (unsigned)

torch.cuda.ByteTensor

8-bit integer (signed)

torch.cuda.CharTensor

16-bit integer (signed)

torch.cuda.ShortTensor

32-bit integer (signed)

64-bit integer (signed)

torch.cuda.LongTensor

torch.cuda.BoolTensor

================================================================================

# torch.Tensor - Data types (Part 6)

However, to construct tensors, we recommend using factory functions such as
torch.empty() with the  argument instead.  The
torch.Tensor constructor is an alias for the default tensor type
(torch.FloatTensor).

================================================================================

# torch.Tensor - Initializing and basic operations (Part 1)

A tensor can be constructed from a Python  or sequence using the
torch.tensor() constructor:

Code example:
tensor([[ 1.0000, -1.0000],
        [ 1.0000, -1.0000]])
     
tensor([[ 1,  2,  3],
        [ 4,  5,  6]])

torch.tensor() always copies . If you have a Tensor
 and just want to change its requires_grad flag, use
requires_grad_() or
 to avoid a copy.
If you have a numpy array and want to avoid a copy, use
torch.as_tensor().

A tensor of specific data type can be constructed by passing a
torch.dtype and/or a torch.device to a
constructor or tensor creation op:

Code example:
tensor([[ 0,  0,  0,  0],
        [ 0,  0,  0,  0]], dtype=torch.int32)
  
   
tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')

For more information about building Tensors, see Creation Ops

The contents of a tensor can be accessed and modified using Python’s indexing
and slicing notation:

Code example:
tensor([[ 1,  8,  3],
        [ 4,  5,  6]])

Use torch.Tensor.item() to get a Python number from a tensor containing a
single value:

Code example:
tensor([[ 1]])


  

tensor(2.5000)

================================================================================

# torch.Tensor - Initializing and basic operations (Part 2)

For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops

A tensor can be created with requires_grad=True so that
torch.autograd records operations on them for automatic differentiation.

Code example:
requires_grad
  


tensor([[ 2.0000, -2.0000],
        [ 2.0000,  2.0000]])

Each tensor has an associated torch.Storage, which holds its data.
The tensor class also provides multi-dimensional, 
view of a storage and defines numeric operations on it.

For more information on tensor views, see Tensor Views.

For more information on the torch.dtype, torch.device, and
torch.layout attributes of a torch.Tensor, see
Tensor Attributes.

Methods which mutate a tensor are marked with an underscore suffix.
For example, torch.FloatTensor.abs_() computes the absolute value
in-place and returns the modified tensor, while torch.FloatTensor.abs()
computes the result in a new tensor.

To change an existing tensor’s torch.device and/or torch.dtype, consider using
 method on the tensor.

================================================================================

# torch.Tensor - Initializing and basic operations (Part 3)

Current implementation of torch.Tensor introduces memory overhead,
thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.
If this is your case, consider using one large structure.

================================================================================

# torch.Tensor - Tensor class reference (Part 1)

There are a few main ways to create a tensor, depending on your use case.

To create a tensor with pre-existing data, use torch.tensor().
To create a tensor with specific size, use  tensor creation
ops (see Creation Ops).
To create a tensor with the same size (and similar types) as another tensor,
use torch.*_like tensor creation ops
(see Creation Ops).
To create a tensor with similar type but different size as another tensor,
use tensor.new_* creation ops.
There is a legacy constructor torch.Tensor whose use is discouraged.
Use torch.tensor() instead.

There are a few main ways to create a tensor, depending on your use case.

List:
To create a tensor with pre-existing data, use torch.tensor().
To create a tensor with specific size, use  tensor creation
ops (see Creation Ops).
To create a tensor with the same size (and similar types) as another tensor,
use torch.*_like tensor creation ops
(see Creation Ops).
To create a tensor with similar type but different size as another tensor,
use tensor.new_* creation ops.
There is a legacy constructor torch.Tensor whose use is discouraged.
Use torch.tensor() instead.

To create a tensor with pre-existing data, use torch.tensor().

================================================================================

# torch.Tensor - Tensor class reference (Part 2)

To create a tensor with specific size, use  tensor creation
ops (see Creation Ops).

To create a tensor with the same size (and similar types) as another tensor,
use torch.*_like tensor creation ops
(see Creation Ops).

To create a tensor with similar type but different size as another tensor,
use tensor.new_* creation ops.

There is a legacy constructor torch.Tensor whose use is discouraged.
Use torch.tensor() instead.

, 
This constructor is deprecated, we recommend using torch.tensor() instead.
What this constructor does depends on the type of .

If  is a Tensor, returns an alias to the original Tensor.  Unlike
torch.tensor(), this tracks autograd and will propagate gradients to
the original Tensor.   kwarg is not supported for this  type.
If  is a sequence or nested sequence, create a tensor of the default
dtype (typically torch.float32) whose data is the values in the
sequences, performing coercions if necessary.  Notably, this differs from
torch.tensor() in that this constructor will always construct a float
tensor, even if the inputs are all integers.
If  is a torch.Size, returns an empty tensor of that size.

================================================================================

# torch.Tensor - Tensor class reference (Part 3)

This constructor does not support explicitly specifying  or  of
the returned tensor.  We recommend using torch.tensor() which provides this
functionality.

data (array_like): The tensor to construct from.

Keyword args:
device (torch.device, optional): the desired device of returned tensor.Default: if None, same torch.device as this tensor.

This constructor is deprecated, we recommend using torch.tensor() instead.
What this constructor does depends on the type of .

List:
If  is a Tensor, returns an alias to the original Tensor.  Unlike
torch.tensor(), this tracks autograd and will propagate gradients to
the original Tensor.   kwarg is not supported for this  type.
If  is a sequence or nested sequence, create a tensor of the default
dtype (typically torch.float32) whose data is the values in the
sequences, performing coercions if necessary.  Notably, this differs from
torch.tensor() in that this constructor will always construct a float
tensor, even if the inputs are all integers.
If  is a torch.Size, returns an empty tensor of that size.

================================================================================

# torch.Tensor - Tensor class reference (Part 4)

If  is a Tensor, returns an alias to the original Tensor.  Unlike
torch.tensor(), this tracks autograd and will propagate gradients to
the original Tensor.   kwarg is not supported for this  type.

If  is a sequence or nested sequence, create a tensor of the default
dtype (typically torch.float32) whose data is the values in the
sequences, performing coercions if necessary.  Notably, this differs from
torch.tensor() in that this constructor will always construct a float
tensor, even if the inputs are all integers.

If  is a torch.Size, returns an empty tensor of that size.

This constructor does not support explicitly specifying  or  of
the returned tensor.  We recommend using torch.tensor() which provides this
functionality.

data (array_like): The tensor to construct from.

Keyword args:
device (torch.device, optional): the desired device of returned tensor.Default: if None, same torch.device as this tensor.

data (array_like): The tensor to construct from.

device (torch.device, optional): the desired device of returned tensor.Default: if None, same torch.device as this tensor.

Default: if None, same torch.device as this tensor.

================================================================================

# torch.Tensor - Tensor class reference (Part 5)

Returns a view of this tensor with its dimensions reversed.
If  is the number of dimensions in ,
 is equivalent to x.permute(n-1,   .


The use of Tensor.T() on tensors of dimension other than 2 to reverse their shape
is deprecated and it will throw an error in a future release. Consider 
to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse
the dimensions of a tensor.

Returns a view of this tensor with its dimensions reversed.

If  is the number of dimensions in ,
 is equivalent to x.permute(n-1,   .

The use of Tensor.T() on tensors of dimension other than 2 to reverse their shape
is deprecated and it will throw an error in a future release. Consider 
to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse
the dimensions of a tensor.

Returns a view of a matrix (2-D tensor) conjugated and transposed.
 is equivalent to x.transpose(0,  for complex matrices and
x.transpose(0,  for real matrices.


: An attribute that also works on batches of matrices.

Returns a view of a matrix (2-D tensor) conjugated and transposed.

is equivalent to x.transpose(0,  for complex matrices and
x.transpose(0,  for real matrices.

================================================================================

# torch.Tensor - Tensor class reference (Part 6)

: An attribute that also works on batches of matrices.

Returns a view of this tensor with the last two dimensions transposed.
 is equivalent to x.transpose(-2, .

Returns a view of this tensor with the last two dimensions transposed.

is equivalent to x.transpose(-2, .

Accessing this property is equivalent to calling .

Accessing this property is equivalent to calling .

================================================================================

# torch.Tensor - Tensor class reference (Part 7)

Table:
Tensor.new_tensor | Returns a new Tensor with  as the tensor data.
Tensor.new_full | Returns a Tensor of size  filled with fill_value.
Tensor.new_empty | Returns a Tensor of size  filled with uninitialized data.
Tensor.new_ones | Returns a Tensor of size  filled with .
Tensor.new_zeros | Returns a Tensor of size  filled with .
Tensor.is_cuda | Is  if the Tensor is stored on the GPU,  otherwise.
Tensor.is_quantized | Is  if the Tensor is quantized,  otherwise.
Tensor.is_meta | Is  if the Tensor is a meta tensor,  otherwise.
Tensor.device | Is the torch.device where this Tensor is.
Tensor.grad | This attribute is  by default and becomes a Tensor the first time a call to backward() computes gradients for .
Tensor.ndim | Alias for
Tensor.real | Returns a new tensor containing real values of the  tensor for a complex-valued input tensor.
Tensor.imag | Returns a new tensor containing imaginary values of the  tensor.
Tensor.nbytes | Returns the number of bytes consumed by the "view" of elements of the Tensor if the Tensor does not use sparse storage layout.
Tensor.itemsize | Alias for element_size()
Tensor.abs | See torch.abs()
Tensor.abs_ | In-place version of
Tensor.absolute | Alias for
Tensor.absolute_ | In-place version of absolute() Alias for
Tensor.acos | See torch.acos()
Tensor.acos_ | In-place version of
Tensor.arccos | See torch.arccos()
Tensor.arccos_ | In-place version of
Tensor.add | Add a scalar or tensor to  tensor.
Tensor.add_ | In-place version of
Tensor.addbmm | See torch.addbmm()
Tensor.addbmm_ | In-place version of
Tensor.addcdiv | See torch.addcdiv()
Tensor.addcdiv_ | In-place version of
Tensor.addcmul | See torch.addcmul()
Tensor.addcmul_ | In-place version of
Tensor.addmm | See torch.addmm()
Tensor.addmm_ | In-place version of
Tensor.sspaddmm | See torch.sspaddmm()
Tensor.addmv | See torch.addmv()
Tensor.addmv_ | In-place version of
Tensor.addr | See torch.addr()
Tensor.addr_ | In-place version of
Tensor.adjoint | Alias for
Tensor.allclose | See torch.allclose()
Tensor.amax | See torch.amax()
Tensor.amin | See torch.amin()
Tensor.aminmax | See torch.aminmax()
Tensor.angle | See torch.angle()
Tensor.apply_ | Applies the function  to each element in the tensor, replacing each element with the value returned by .
Tensor.argmax | See torch.argmax()
Tensor.argmin | See torch.argmin()
Tensor.argsort | See torch.argsort()
Tensor.argwhere | See torch.argwhere()
Tensor.asin | See torch.asin()
Tensor.asin_ | In-place version of
Tensor.arcsin | See torch.arcsin()
Tensor.arcsin_ | In-place version of
Tensor.as_strided | See torch.as_strided()
Tensor.atan | See torch.atan()
Tensor.atan_ | In-place version of
Tensor.arctan | See torch.arctan()
Tensor.arctan_ | In-place version of
Tensor.atan2 | See torch.atan2()
Tensor.atan2_ | In-place version of
Tensor.arctan2 | See torch.arctan2()
Tensor.arctan2_ | atan2_(other) -> Tensor
Tensor.all | See torch.all()
Tensor.any | See torch.any()
Tensor.backward | Computes the gradient of current tensor wrt graph leaves.
Tensor.baddbmm | See torch.baddbmm()
Tensor.baddbmm_ | In-place version of
Tensor.bernoulli | Returns a result tensor where each \texttt{result[i]} is independently sampled from \text{Bernoulli}(\texttt{self[i]}).
Tensor.bernoulli_ | Fills each location of  with an independent sample from \text{Bernoulli}(\texttt{p}).
Tensor.bfloat16 | self.bfloat16() is equivalent to self.to(torch.bfloat16).
Tensor.bincount | See torch.bincount()
Tensor.bitwise_not | See torch.bitwise_not()
Tensor.bitwise_not_ | In-place version of bitwise_not()
Tensor.bitwise_and | See torch.bitwise_and()
Tensor.bitwise_and_ | In-place version of bitwise_and()
Tensor.bitwise_or | See torch.bitwise_or()
Tensor.bitwise_or_ | In-place version of bitwise_or()
Tensor.bitwise_xor | See torch.bitwise_xor()
Tensor.bitwise_xor_ | In-place version of bitwise_xor()
Tensor.bitwise_left_shift | See torch.bitwise_left_shift()
Tensor.bitwise_left_shift_ | In-place version of bitwise_left_shift()
Tensor.bitwise_right_shift | See torch.bitwise_right_shift()
Tensor.bitwise_right_shift_ | In-place version of bitwise_right_shift()
Tensor.bmm | See torch.bmm()
Tensor.bool | self.bool() is equivalent to self.to(torch.bool).
Tensor.byte | self.byte() is equivalent to self.to(torch.uint8).
Tensor.broadcast_to | See torch.broadcast_to().
Tensor.cauchy_ | Fills the tensor with numbers drawn from the Cauchy distribution:
Tensor.ceil | See torch.ceil()
Tensor.ceil_ | In-place version of
Tensor.char | self.char() is equivalent to self.to(torch.int8).
Tensor.cholesky | See torch.cholesky()
Tensor.cholesky_inverse | See torch.cholesky_inverse()
Tensor.cholesky_solve | See torch.cholesky_solve()
Tensor.chunk | See torch.chunk()
Tensor.clamp | See torch.clamp()
Tensor.clamp_ | In-place version of
Tensor.clip | Alias for .
Tensor.clip_ | Alias for .
Tensor.clone | See torch.clone()
Tensor.contiguous | Returns a contiguous in memory tensor containing the same data as  tensor.
Tensor.copy_ | Copies the elements from  into  tensor and returns .
Tensor.conj | See torch.conj()
Tensor.conj_physical | See torch.conj_physical()
Tensor.conj_physical_ | In-place version of conj_physical()
Tensor.resolve_conj | See torch.resolve_conj()
Tensor.resolve_neg | See torch.resolve_neg()
Tensor.copysign | See torch.copysign()
Tensor.copysign_ | In-place version of copysign()
Tensor.cos | See torch.cos()
Tensor.cos_ | In-place version of
Tensor.cosh | See torch.cosh()
Tensor.cosh_ | In-place version of
Tensor.corrcoef | See torch.corrcoef()
Tensor.count_nonzero | See torch.count_nonzero()
Tensor.cov | See torch.cov()
Tensor.acosh | See torch.acosh()
Tensor.acosh_ | In-place version of
Tensor.arccosh | acosh() -> Tensor
Tensor.arccosh_ | acosh_() -> Tensor
Tensor.cpu | Returns a copy of this object in CPU memory.
Tensor.cross | See torch.cross()
Tensor.cuda | Returns a copy of this object in CUDA memory.
Tensor.logcumsumexp | See torch.logcumsumexp()
Tensor.cummax | See torch.cummax()
Tensor.cummin | See torch.cummin()
Tensor.cumprod | See torch.cumprod()
Tensor.cumprod_ | In-place version of
Tensor.cumsum | See torch.cumsum()
Tensor.cumsum_ | In-place version of
Tensor.chalf | self.chalf() is equivalent to self.to(torch.complex32).
Tensor.cfloat | self.cfloat() is equivalent to self.to(torch.complex64).
Tensor.cdouble | self.cdouble() is equivalent to self.to(torch.complex128).
Tensor.data_ptr | Returns the address of the first element of  tensor.
Tensor.deg2rad | See torch.deg2rad()
Tensor.dequantize | Given a quantized Tensor, dequantize it and return the dequantized float Tensor.
Tensor.det | See torch.det()
Tensor.dense_dim | Return the number of dense dimensions in a sparse tensor .
Tensor.detach | Returns a new Tensor, detached from the current graph.
Tensor.detach_ | Detaches the Tensor from the graph that created it, making it a leaf.
Tensor.diag | See torch.diag()
Tensor.diag_embed | See torch.diag_embed()
Tensor.diagflat | See torch.diagflat()
Tensor.diagonal | See torch.diagonal()
Tensor.diagonal_scatter | See torch.diagonal_scatter()
Tensor.fill_diagonal_ | Fill the main diagonal of a tensor that has at least 2-dimensions.
Tensor.fmax | See torch.fmax()
Tensor.fmin | See torch.fmin()
Tensor.diff | See torch.diff()
Tensor.digamma | See torch.digamma()
Tensor.digamma_ | In-place version of
Tensor.dim | Returns the number of dimensions of  tensor.
Tensor.dim_order | Returns the uniquely determined tuple of int describing the dim order or physical layout of .
Tensor.dist | See torch.dist()
Tensor.div | See torch.div()
Tensor.div_ | In-place version of
Tensor.divide | See torch.divide()
Tensor.divide_ | In-place version of
Tensor.dot | See torch.dot()
Tensor.double | self.double() is equivalent to self.to(torch.float64).
Tensor.dsplit | See torch.dsplit()
Tensor.element_size | Returns the size in bytes of an individual element.
See torch.eq()
Tensor.eq_ | In-place version of
Tensor.equal | See torch.equal()
Tensor.erf | See torch.erf()
Tensor.erf_ | In-place version of
Tensor.erfc | See torch.erfc()
Tensor.erfc_ | In-place version of
Tensor.erfinv | See torch.erfinv()
Tensor.erfinv_ | In-place version of
Tensor.exp | See torch.exp()
Tensor.exp_ | In-place version of
Tensor.expm1 | See torch.expm1()
Tensor.expm1_ | In-place version of
Tensor.expand | Returns a new view of the  tensor with singleton dimensions expanded to a larger size.
Tensor.expand_as | Expand this tensor to the same size as .
Tensor.exponential_ | Fills  tensor with elements drawn from the PDF (probability density function):
Tensor.fix | See torch.fix().
Tensor.fix_ | In-place version of
Tensor.fill_ | Fills  tensor with the specified value.
Tensor.flatten | See torch.flatten()
Tensor.flip | See torch.flip()
Tensor.fliplr | See torch.fliplr()
Tensor.flipud | See torch.flipud()
Tensor.float | self.float() is equivalent to self.to(torch.float32).
Tensor.float_power | See torch.float_power()
Tensor.float_power_ | In-place version of float_power()
Tensor.floor | See torch.floor()
Tensor.floor_ | In-place version of
Tensor.floor_divide | See torch.floor_divide()
Tensor.floor_divide_ | In-place version of floor_divide()
Tensor.fmod | See torch.fmod()
Tensor.fmod_ | In-place version of
Tensor.frac | See torch.frac()
Tensor.frac_ | In-place version of
Tensor.frexp | See torch.frexp()
Tensor.gather | See torch.gather()
Tensor.gcd | See torch.gcd()
Tensor.gcd_ | In-place version of
See torch.ge().
Tensor.ge_ | In-place version of .
Tensor.greater_equal | See torch.greater_equal().
Tensor.greater_equal_ | In-place version of greater_equal().
Tensor.geometric_ | Fills  tensor with elements drawn from the geometric distribution:
Tensor.geqrf | See torch.geqrf()
Tensor.ger | See torch.ger()
Tensor.get_device | For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.
See torch.gt().
Tensor.gt_ | In-place version of .
Tensor.greater | See torch.greater().
Tensor.greater_ | In-place version of .
Tensor.half | self.half() is equivalent to self.to(torch.float16).
Tensor.hardshrink | See torch.nn.functional.hardshrink()
Tensor.heaviside | See torch.heaviside()
Tensor.histc | See torch.histc()
Tensor.histogram | See torch.histogram()
Tensor.hsplit | See torch.hsplit()
Tensor.hypot | See torch.hypot()
Tensor.hypot_ | In-place version of
See torch.i0()
Tensor.i0_ | In-place version of
Tensor.igamma | See torch.igamma()
Tensor.igamma_ | In-place version of
Tensor.igammac | See torch.igammac()
Tensor.igammac_ | In-place version of
Tensor.index_add_ | Accumulate the elements of  times  into the  tensor by adding to the indices in the order given in .
Tensor.index_add | Out-of-place version of torch.Tensor.index_add_().
Tensor.index_copy_ | Copies the elements of  into the  tensor by selecting the indices in the order given in .
Tensor.index_copy | Out-of-place version of torch.Tensor.index_copy_().
Tensor.index_fill_ | Fills the elements of the  tensor with value  by selecting the indices in the order given in .
Tensor.index_fill | Out-of-place version of torch.Tensor.index_fill_().
Tensor.index_put_ | Puts values from the tensor  into the tensor  using the indices specified in  (which is a tuple of Tensors).
Tensor.index_put | Out-place version of index_put_().
Tensor.index_reduce_ | Accumulate the elements of  into the  tensor by accumulating to the indices in the order given in  using the reduction given by the  argument.
Tensor.index_reduce | 
Tensor.index_select | See torch.index_select()
Tensor.indices | Return the indices tensor of a sparse COO tensor.
Tensor.inner | See torch.inner().
Tensor.int | self.int() is equivalent to self.to(torch.int32).
Tensor.int_repr | Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.
Tensor.inverse | See torch.inverse()
Tensor.isclose | See torch.isclose()
Tensor.isfinite | See torch.isfinite()
Tensor.isinf | See torch.isinf()
Tensor.isposinf | See torch.isposinf()
Tensor.isneginf | See torch.isneginf()
Tensor.isnan | See torch.isnan()
Tensor.is_contiguous | Returns True if  tensor is contiguous in memory in the order specified by memory format.
Tensor.is_complex | Returns True if the data type of  is a complex data type.
Tensor.is_conj | Returns True if the conjugate bit of  is set to true.
Tensor.is_floating_point | Returns True if the data type of  is a floating point data type.
Tensor.is_inference | See torch.is_inference()
Tensor.is_leaf | All Tensors that have requires_grad which is  will be leaf Tensors by convention.
Tensor.is_pinned | Returns true if this tensor resides in pinned memory.
Tensor.is_set_to | Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).
Tensor.is_shared | Checks if tensor is in shared memory.
Tensor.is_signed | Returns True if the data type of  is a signed data type.
Tensor.is_sparse | Is  if the Tensor uses sparse COO storage layout,  otherwise.
Tensor.istft | See torch.istft()
Tensor.isreal | See torch.isreal()
Tensor.item | Returns the value of this tensor as a standard Python number.
Tensor.kthvalue | See torch.kthvalue()
Tensor.lcm | See torch.lcm()
Tensor.lcm_ | In-place version of
Tensor.ldexp | See torch.ldexp()
Tensor.ldexp_ | In-place version of
See torch.le().
Tensor.le_ | In-place version of .
Tensor.less_equal | See torch.less_equal().
Tensor.less_equal_ | In-place version of less_equal().
Tensor.lerp | See torch.lerp()
Tensor.lerp_ | In-place version of
Tensor.lgamma | See torch.lgamma()
Tensor.lgamma_ | In-place version of
Tensor.log | See torch.log()
Tensor.log_ | In-place version of
Tensor.logdet | See torch.logdet()
Tensor.log10 | See torch.log10()
Tensor.log10_ | In-place version of
Tensor.log1p | See torch.log1p()
Tensor.log1p_ | In-place version of
Tensor.log2 | See torch.log2()
Tensor.log2_ | In-place version of
Tensor.log_normal_ | Fills  tensor with numbers samples from the log-normal distribution parameterized by the given mean  and standard deviation .
Tensor.logaddexp | See torch.logaddexp()
Tensor.logaddexp2 | See torch.logaddexp2()
Tensor.logsumexp | See torch.logsumexp()
Tensor.logical_and | See torch.logical_and()
Tensor.logical_and_ | In-place version of logical_and()
Tensor.logical_not | See torch.logical_not()
Tensor.logical_not_ | In-place version of logical_not()
Tensor.logical_or | See torch.logical_or()
Tensor.logical_or_ | In-place version of logical_or()
Tensor.logical_xor | See torch.logical_xor()
Tensor.logical_xor_ | In-place version of logical_xor()
Tensor.logit | See torch.logit()
Tensor.logit_ | In-place version of
Tensor.long | self.long() is equivalent to self.to(torch.int64).
See torch.lt().
Tensor.lt_ | In-place version of .
Tensor.less | lt(other) -> Tensor
Tensor.less_ | In-place version of .
See torch.lu()
Tensor.lu_solve | See torch.lu_solve()
Tensor.as_subclass | Makes a  instance with the same data pointer as .
Tensor.map_ | Applies  for each element in  tensor and the given  and stores the results in  tensor.
Tensor.masked_scatter_ | Copies elements from  into  tensor at positions where the  is True.
Tensor.masked_scatter | Out-of-place version of torch.Tensor.masked_scatter_()
Tensor.masked_fill_ | Fills elements of  tensor with  where  is True.
Tensor.masked_fill | Out-of-place version of torch.Tensor.masked_fill_()
Tensor.masked_select | See torch.masked_select()
Tensor.matmul | See torch.matmul()
Tensor.matrix_power | matrix_power() is deprecated, use torch.linalg.matrix_power() instead.
Tensor.matrix_exp | See torch.matrix_exp()
Tensor.max | See torch.max()
Tensor.maximum | See torch.maximum()
Tensor.mean | See torch.mean()
Tensor.module_load | Defines how to transform  when loading it into  in load_state_dict().
Tensor.nanmean | See torch.nanmean()
Tensor.median | See torch.median()
Tensor.nanmedian | See torch.nanmedian()
Tensor.min | See torch.min()
Tensor.minimum | See torch.minimum()
See torch.mm()
Tensor.smm | See torch.smm()
Tensor.mode | See torch.mode()
Tensor.movedim | See torch.movedim()
Tensor.moveaxis | See torch.moveaxis()
Tensor.msort | See torch.msort()
Tensor.mul | See torch.mul().
Tensor.mul_ | In-place version of .
Tensor.multiply | See torch.multiply().
Tensor.multiply_ | In-place version of multiply().
Tensor.multinomial | See torch.multinomial()
See torch.mv()
Tensor.mvlgamma | See torch.mvlgamma()
Tensor.mvlgamma_ | In-place version of mvlgamma()
Tensor.nansum | See torch.nansum()
Tensor.narrow | See torch.narrow().
Tensor.narrow_copy | See torch.narrow_copy().
Tensor.ndimension | Alias for
Tensor.nan_to_num | See torch.nan_to_num().
Tensor.nan_to_num_ | In-place version of nan_to_num().
See torch.ne().
Tensor.ne_ | In-place version of .
Tensor.not_equal | See torch.not_equal().
Tensor.not_equal_ | In-place version of not_equal().
Tensor.neg | See torch.neg()
Tensor.neg_ | In-place version of
Tensor.negative | See torch.negative()
Tensor.negative_ | In-place version of negative()
Tensor.nelement | Alias for
Tensor.nextafter | See torch.nextafter()
Tensor.nextafter_ | In-place version of nextafter()
Tensor.nonzero | See torch.nonzero()
Tensor.norm | See torch.norm()
Tensor.normal_ | Fills  tensor with elements samples from the normal distribution parameterized by  and .
Tensor.numel | See torch.numel()
Tensor.numpy | Returns the tensor as a NumPy .
Tensor.orgqr | See torch.orgqr()
Tensor.ormqr | See torch.ormqr()
Tensor.outer | See torch.outer().
Tensor.permute | See torch.permute()
Tensor.pin_memory | Copies the tensor to pinned memory, if it's not already pinned.
Tensor.pinverse | See torch.pinverse()
Tensor.polygamma | See torch.polygamma()
Tensor.polygamma_ | In-place version of polygamma()
Tensor.positive | See torch.positive()
Tensor.pow | See torch.pow()
Tensor.pow_ | In-place version of
Tensor.prod | See torch.prod()
Tensor.put_ | Copies the elements from  into the positions specified by .
See torch.qr()
Tensor.qscheme | Returns the quantization scheme of a given QTensor.
Tensor.quantile | See torch.quantile()
Tensor.nanquantile | See torch.nanquantile()
Tensor.q_scale | Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().
Tensor.q_zero_point | Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().
Tensor.q_per_channel_scales | Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.
Tensor.q_per_channel_zero_points | Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.
Tensor.q_per_channel_axis | Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.
Tensor.rad2deg | See torch.rad2deg()
Tensor.random_ | Fills  tensor with numbers sampled from the discrete uniform distribution over    .
Tensor.ravel | see torch.ravel()
Tensor.reciprocal | See torch.reciprocal()
Tensor.reciprocal_ | In-place version of reciprocal()
Tensor.record_stream | Marks the tensor as having been used by this stream.
Tensor.register_hook | Registers a backward hook.
Tensor.register_post_accumulate_grad_hook | Registers a backward hook that runs after grad accumulation.
Tensor.remainder | See torch.remainder()
Tensor.remainder_ | In-place version of remainder()
Tensor.renorm | See torch.renorm()
Tensor.renorm_ | In-place version of
Tensor.repeat | Repeats this tensor along the specified dimensions.
Tensor.repeat_interleave | See torch.repeat_interleave().
Tensor.requires_grad | Is  if gradients need to be computed for this Tensor,  otherwise.
Tensor.requires_grad_ | Change if autograd should record operations on this tensor: sets this tensor's requires_grad attribute in-place.
Tensor.reshape | Returns a tensor with the same data and number of elements as  but with the specified shape.
Tensor.reshape_as | Returns this tensor as the same shape as .
Tensor.resize_ | Resizes  tensor to the specified size.
Tensor.resize_as_ | Resizes the  tensor to be the same size as the specified .
Tensor.retain_grad | Enables this Tensor to have their  populated during backward().
Tensor.retains_grad | Is  if this Tensor is non-leaf and its  is enabled to be populated during backward(),  otherwise.
Tensor.roll | See torch.roll()
Tensor.rot90 | See torch.rot90()
Tensor.round | See torch.round()
Tensor.round_ | In-place version of
Tensor.rsqrt | See torch.rsqrt()
Tensor.rsqrt_ | In-place version of
Tensor.scatter | Out-of-place version of torch.Tensor.scatter_()
Tensor.scatter_ | Writes all values from the tensor  into  at the indices specified in the  tensor.
Tensor.scatter_add_ | Adds all values from the tensor  into  at the indices specified in the  tensor in a similar fashion as scatter_().
Tensor.scatter_add | Out-of-place version of torch.Tensor.scatter_add_()
Tensor.scatter_reduce_ | Reduces all values from the  tensor to the indices specified in the  tensor in the  tensor using the applied reduction defined via the  argument (, , , , ).
Tensor.scatter_reduce | Out-of-place version of torch.Tensor.scatter_reduce_()
Tensor.select | See torch.select()
Tensor.select_scatter | See torch.select_scatter()
Tensor.set_ | Sets the underlying storage, size, and strides.
Tensor.share_memory_ | Moves the underlying storage to shared memory.
Tensor.short | self.short() is equivalent to self.to(torch.int16).
Tensor.sigmoid | See torch.sigmoid()
Tensor.sigmoid_ | In-place version of
Tensor.sign | See torch.sign()
Tensor.sign_ | In-place version of
Tensor.signbit | See torch.signbit()
Tensor.sgn | See torch.sgn()
Tensor.sgn_ | In-place version of
Tensor.sin | See torch.sin()
Tensor.sin_ | In-place version of
Tensor.sinc | See torch.sinc()
Tensor.sinc_ | In-place version of
Tensor.sinh | See torch.sinh()
Tensor.sinh_ | In-place version of
Tensor.asinh | See torch.asinh()
Tensor.asinh_ | In-place version of
Tensor.arcsinh | See torch.arcsinh()
Tensor.arcsinh_ | In-place version of
Tensor.shape | Returns the size of the  tensor.
Tensor.size | Returns the size of the  tensor.
Tensor.slogdet | See torch.slogdet()
Tensor.slice_scatter | See torch.slice_scatter()
Tensor.softmax | Alias for torch.nn.functional.softmax().
Tensor.sort | See torch.sort()
Tensor.split | See torch.split()
Tensor.sparse_mask | Returns a new sparse tensor with values from a strided tensor  filtered by the indices of the sparse tensor .
Tensor.sparse_dim | Return the number of sparse dimensions in a sparse tensor .
Tensor.sqrt | See torch.sqrt()
Tensor.sqrt_ | In-place version of
Tensor.square | See torch.square()
Tensor.square_ | In-place version of
Tensor.squeeze | See torch.squeeze()
Tensor.squeeze_ | In-place version of
Tensor.std | See torch.std()
Tensor.stft | See torch.stft()
Tensor.storage | Returns the underlying TypedStorage.
Tensor.untyped_storage | Returns the underlying UntypedStorage.
Tensor.storage_offset | Returns  tensor's offset in the underlying storage in terms of number of storage elements (not bytes).
Tensor.storage_type | Returns the type of the underlying storage.
Tensor.stride | Returns the stride of  tensor.
Tensor.sub | See torch.sub().
Tensor.sub_ | In-place version of
Tensor.subtract | See torch.subtract().
Tensor.subtract_ | In-place version of subtract().
Tensor.sum | See torch.sum()
Tensor.sum_to_size | Sum  tensor to .
Tensor.svd | See torch.svd()
Tensor.swapaxes | See torch.swapaxes()
Tensor.swapdims | See torch.swapdims()
See
In-place version of
Tensor.tensor_split | See torch.tensor_split()
Tensor.tile | See torch.tile()
Performs Tensor dtype and/or device conversion.
Tensor.to_mkldnn | Returns a copy of the tensor in torch.mkldnn layout.
Tensor.take | See torch.take()
Tensor.take_along_dim | See torch.take_along_dim()
Tensor.tan | See torch.tan()
Tensor.tan_ | In-place version of
Tensor.tanh | See torch.tanh()
Tensor.tanh_ | In-place version of
Tensor.atanh | See torch.atanh()
Tensor.atanh_ | In-place version of
Tensor.arctanh | See torch.arctanh()
Tensor.arctanh_ | In-place version of
Tensor.tolist | Returns the tensor as a (nested) list.
Tensor.topk | See torch.topk()
Tensor.to_dense | Creates a strided copy of  if  is not a strided tensor, otherwise returns .
Tensor.to_sparse | Returns a sparse copy of the tensor.
Tensor.to_sparse_csr | Convert a tensor to compressed row storage format (CSR).
Tensor.to_sparse_csc | Convert a tensor to compressed column storage (CSC) format.
Tensor.to_sparse_bsr | Convert a tensor to a block sparse row (BSR) storage format of given blocksize.
Tensor.to_sparse_bsc | Convert a tensor to a block sparse column (BSC) storage format of given blocksize.
Tensor.trace | See torch.trace()
Tensor.transpose | See torch.transpose()
Tensor.transpose_ | In-place version of transpose()
Tensor.triangular_solve | See torch.triangular_solve()
Tensor.tril | See torch.tril()
Tensor.tril_ | In-place version of
Tensor.triu | See torch.triu()
Tensor.triu_ | In-place version of
Tensor.true_divide | See torch.true_divide()
Tensor.true_divide_ | In-place version of true_divide_()
Tensor.trunc | See torch.trunc()
Tensor.trunc_ | In-place version of
Tensor.type | Returns the type if  is not provided, else casts this object to the specified type.
Tensor.type_as | Returns this tensor cast to the type of the given tensor.
Tensor.unbind | See torch.unbind()
Tensor.unflatten | See torch.unflatten().
Tensor.unfold | Returns a view of the original tensor which contains all slices of size  from  tensor in the dimension .
Tensor.uniform_ | Fills  tensor with numbers sampled from the continuous uniform distribution:
Tensor.unique | Returns the unique elements of the input tensor.
Tensor.unique_consecutive | Eliminates all but the first element from every consecutive group of equivalent elements.
Tensor.unsqueeze | See torch.unsqueeze()
Tensor.unsqueeze_ | In-place version of unsqueeze()
Tensor.values | Return the values tensor of a sparse COO tensor.
Tensor.var | See torch.var()
Tensor.vdot | See torch.vdot()
Tensor.view | Returns a new tensor with the same data as the  tensor but of a different .
Tensor.view_as | View this tensor as the same size as .
Tensor.vsplit | See torch.vsplit()
Tensor.where | self.where(condition,  is equivalent to torch.where(condition,  .
Tensor.xlogy | See torch.xlogy()
Tensor.xlogy_ | In-place version of
Tensor.xpu | Returns a copy of this object in XPU memory.
Tensor.zero_ | Fills  tensor with zeros.

================================================================================

# torch.Tensor - Tensor class reference (Part 8)

Returns a new Tensor with  as the tensor data.

Returns a Tensor of size  filled with fill_value.

Returns a Tensor of size  filled with uninitialized data.

Returns a Tensor of size  filled with .

Returns a Tensor of size  filled with .

Is  if the Tensor is stored on the GPU,  otherwise.

Is  if the Tensor is quantized,  otherwise.

Is  if the Tensor is a meta tensor,  otherwise.

Is the torch.device where this Tensor is.

This attribute is  by default and becomes a Tensor the first time a call to backward() computes gradients for .

Returns a new tensor containing real values of the  tensor for a complex-valued input tensor.

Returns a new tensor containing imaginary values of the  tensor.

Returns the number of bytes consumed by the "view" of elements of the Tensor if the Tensor does not use sparse storage layout.

Alias for element_size()

In-place version of absolute() Alias for

Add a scalar or tensor to  tensor.

Applies the function  to each element in the tensor, replacing each element with the value returned by .

See torch.as_strided()

atan2_(other) -> Tensor

Computes the gradient of current tensor wrt graph leaves.

================================================================================

# torch.Tensor - Tensor class reference (Part 9)

Returns a result tensor where each \texttt{result[i]} is independently sampled from \text{Bernoulli}(\texttt{self[i]}).

Fills each location of  with an independent sample from \text{Bernoulli}(\texttt{p}).

self.bfloat16() is equivalent to self.to(torch.bfloat16).

See torch.bitwise_not()

In-place version of bitwise_not()

See torch.bitwise_and()

In-place version of bitwise_and()

See torch.bitwise_or()

In-place version of bitwise_or()

See torch.bitwise_xor()

In-place version of bitwise_xor()

Tensor.bitwise_left_shift

See torch.bitwise_left_shift()

Tensor.bitwise_left_shift_

In-place version of bitwise_left_shift()

Tensor.bitwise_right_shift

See torch.bitwise_right_shift()

Tensor.bitwise_right_shift_

In-place version of bitwise_right_shift()

self.bool() is equivalent to self.to(torch.bool).

self.byte() is equivalent to self.to(torch.uint8).

See torch.broadcast_to().

Fills the tensor with numbers drawn from the Cauchy distribution:

self.char() is equivalent to self.to(torch.int8).

Tensor.cholesky_inverse

See torch.cholesky_inverse()

Tensor.cholesky_solve

See torch.cholesky_solve()

Returns a contiguous in memory tensor containing the same data as  tensor.

Copies the elements from  into  tensor and returns .

================================================================================

# torch.Tensor - Tensor class reference (Part 10)

See torch.conj_physical()

Tensor.conj_physical_

In-place version of conj_physical()

See torch.resolve_conj()

See torch.resolve_neg()

In-place version of copysign()

See torch.count_nonzero()

Returns a copy of this object in CPU memory.

Returns a copy of this object in CUDA memory.

See torch.logcumsumexp()

self.chalf() is equivalent to self.to(torch.complex32).

self.cfloat() is equivalent to self.to(torch.complex64).

self.cdouble() is equivalent to self.to(torch.complex128).

Returns the address of the first element of  tensor.

Given a quantized Tensor, dequantize it and return the dequantized float Tensor.

Return the number of dense dimensions in a sparse tensor .

Returns a new Tensor, detached from the current graph.

Detaches the Tensor from the graph that created it, making it a leaf.

See torch.diag_embed()

Tensor.diagonal_scatter

See torch.diagonal_scatter()

Tensor.fill_diagonal_

Fill the main diagonal of a tensor that has at least 2-dimensions.

Returns the number of dimensions of  tensor.

Returns the uniquely determined tuple of int describing the dim order or physical layout of .

self.double() is equivalent to self.to(torch.float64).

Returns the size in bytes of an individual element.

================================================================================

# torch.Tensor - Tensor class reference (Part 11)

Returns a new view of the  tensor with singleton dimensions expanded to a larger size.

Expand this tensor to the same size as .

Fills  tensor with elements drawn from the PDF (probability density function):

Fills  tensor with the specified value.

self.float() is equivalent to self.to(torch.float32).

See torch.float_power()

In-place version of float_power()

See torch.floor_divide()

In-place version of floor_divide()

In-place version of .

See torch.greater_equal().

Tensor.greater_equal_

In-place version of greater_equal().

Fills  tensor with elements drawn from the geometric distribution:

For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.

In-place version of .

In-place version of .

self.half() is equivalent to self.to(torch.float16).

See torch.nn.functional.hardshrink()

See torch.heaviside()

See torch.histogram()

Accumulate the elements of  times  into the  tensor by adding to the indices in the order given in .

Out-of-place version of torch.Tensor.index_add_().

Copies the elements of  into the  tensor by selecting the indices in the order given in .

Out-of-place version of torch.Tensor.index_copy_().

================================================================================

# torch.Tensor - Tensor class reference (Part 12)

Fills the elements of the  tensor with value  by selecting the indices in the order given in .

Out-of-place version of torch.Tensor.index_fill_().

Puts values from the tensor  into the tensor  using the indices specified in  (which is a tuple of Tensors).

Out-place version of index_put_().

Accumulate the elements of  into the  tensor by accumulating to the indices in the order given in  using the reduction given by the  argument.

See torch.index_select()

Return the indices tensor of a sparse COO tensor.

self.int() is equivalent to self.to(torch.int32).

Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.

Returns True if  tensor is contiguous in memory in the order specified by memory format.

Returns True if the data type of  is a complex data type.

Returns True if the conjugate bit of  is set to true.

Tensor.is_floating_point

Returns True if the data type of  is a floating point data type.

See torch.is_inference()

All Tensors that have requires_grad which is  will be leaf Tensors by convention.

Returns true if this tensor resides in pinned memory.

================================================================================

# torch.Tensor - Tensor class reference (Part 13)

Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).

Checks if tensor is in shared memory.

Returns True if the data type of  is a signed data type.

Is  if the Tensor uses sparse COO storage layout,  otherwise.

Returns the value of this tensor as a standard Python number.

In-place version of .

See torch.less_equal().

In-place version of less_equal().

Fills  tensor with numbers samples from the log-normal distribution parameterized by the given mean  and standard deviation .

See torch.logaddexp()

See torch.logaddexp2()

See torch.logsumexp()

See torch.logical_and()

In-place version of logical_and()

See torch.logical_not()

In-place version of logical_not()

See torch.logical_or()

In-place version of logical_or()

See torch.logical_xor()

In-place version of logical_xor()

self.long() is equivalent to self.to(torch.int64).

In-place version of .

In-place version of .

Makes a  instance with the same data pointer as .

Applies  for each element in  tensor and the given  and stores the results in  tensor.

Tensor.masked_scatter_

Copies elements from  into  tensor at positions where the  is True.

Tensor.masked_scatter

================================================================================

# torch.Tensor - Tensor class reference (Part 14)

Out-of-place version of torch.Tensor.masked_scatter_()

Fills elements of  tensor with  where  is True.

Out-of-place version of torch.Tensor.masked_fill_()

See torch.masked_select()

matrix_power() is deprecated, use torch.linalg.matrix_power() instead.

matrix_power() is deprecated, use torch.linalg.matrix_power() instead.

See torch.matrix_exp()

Defines how to transform  when loading it into  in load_state_dict().

See torch.nanmedian()

In-place version of .

See torch.multiply().

In-place version of multiply().

See torch.multinomial()

In-place version of mvlgamma()

See torch.narrow_copy().

See torch.nan_to_num().

In-place version of nan_to_num().

In-place version of .

See torch.not_equal().

In-place version of not_equal().

In-place version of negative()

See torch.nextafter()

In-place version of nextafter()

Fills  tensor with elements samples from the normal distribution parameterized by  and .

Returns the tensor as a NumPy .

Copies the tensor to pinned memory, if it's not already pinned.

See torch.polygamma()

In-place version of polygamma()

Copies the elements from  into the positions specified by .

Returns the quantization scheme of a given QTensor.

See torch.nanquantile()

================================================================================

# torch.Tensor - Tensor class reference (Part 15)

Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().

Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().

Tensor.q_per_channel_scales

Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.

Tensor.q_per_channel_zero_points

Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.

Tensor.q_per_channel_axis

Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.

Fills  tensor with numbers sampled from the discrete uniform distribution over    .

See torch.reciprocal()

In-place version of reciprocal()

Marks the tensor as having been used by this stream.

Registers a backward hook.

Tensor.register_post_accumulate_grad_hook

Registers a backward hook that runs after grad accumulation.

See torch.remainder()

In-place version of remainder()

Repeats this tensor along the specified dimensions.

Tensor.repeat_interleave

See torch.repeat_interleave().

================================================================================

# torch.Tensor - Tensor class reference (Part 16)

Is  if gradients need to be computed for this Tensor,  otherwise.

Tensor.requires_grad_

Change if autograd should record operations on this tensor: sets this tensor's requires_grad attribute in-place.

Returns a tensor with the same data and number of elements as  but with the specified shape.

Returns this tensor as the same shape as .

Resizes  tensor to the specified size.

Resizes the  tensor to be the same size as the specified .

Enables this Tensor to have their  populated during backward().

Is  if this Tensor is non-leaf and its  is enabled to be populated during backward(),  otherwise.

Out-of-place version of torch.Tensor.scatter_()

Writes all values from the tensor  into  at the indices specified in the  tensor.

Adds all values from the tensor  into  at the indices specified in the  tensor in a similar fashion as scatter_().

Out-of-place version of torch.Tensor.scatter_add_()

Tensor.scatter_reduce_

Reduces all values from the  tensor to the indices specified in the  tensor in the  tensor using the applied reduction defined via the  argument (, , , , ).

Tensor.scatter_reduce

Out-of-place version of torch.Tensor.scatter_reduce_()

Tensor.select_scatter

See torch.select_scatter()

================================================================================

# torch.Tensor - Tensor class reference (Part 17)

Sets the underlying storage, size, and strides.

Moves the underlying storage to shared memory.

self.short() is equivalent to self.to(torch.int16).

Returns the size of the  tensor.

Returns the size of the  tensor.

See torch.slice_scatter()

Alias for torch.nn.functional.softmax().

Returns a new sparse tensor with values from a strided tensor  filtered by the indices of the sparse tensor .

Return the number of sparse dimensions in a sparse tensor .

Returns the underlying TypedStorage.

Tensor.untyped_storage

Returns the underlying UntypedStorage.

Tensor.storage_offset

Returns  tensor's offset in the underlying storage in terms of number of storage elements (not bytes).

Returns the type of the underlying storage.

Returns the stride of  tensor.

See torch.subtract().

In-place version of subtract().

See torch.tensor_split()

Performs Tensor dtype and/or device conversion.

Returns a copy of the tensor in torch.mkldnn layout.

Tensor.take_along_dim

See torch.take_along_dim()

Returns the tensor as a (nested) list.

Creates a strided copy of  if  is not a strided tensor, otherwise returns .

Returns a sparse copy of the tensor.

Convert a tensor to compressed row storage format (CSR).

================================================================================

# torch.Tensor - Tensor class reference (Part 18)

Convert a tensor to compressed column storage (CSC) format.

Convert a tensor to a block sparse row (BSR) storage format of given blocksize.

Convert a tensor to a block sparse column (BSC) storage format of given blocksize.

See torch.transpose()

In-place version of transpose()

Tensor.triangular_solve

See torch.triangular_solve()

See torch.true_divide()

In-place version of true_divide_()

Returns the type if  is not provided, else casts this object to the specified type.

Returns this tensor cast to the type of the given tensor.

See torch.unflatten().

Returns a view of the original tensor which contains all slices of size  from  tensor in the dimension .

Fills  tensor with numbers sampled from the continuous uniform distribution:

Returns the unique elements of the input tensor.

Tensor.unique_consecutive

Eliminates all but the first element from every consecutive group of equivalent elements.

See torch.unsqueeze()

In-place version of unsqueeze()

Return the values tensor of a sparse COO tensor.

Returns a new tensor with the same data as the  tensor but of a different .

View this tensor as the same size as .

self.where(condition,  is equivalent to torch.where(condition,  .

================================================================================

# torch.Tensor - Tensor class reference (Part 19)

Returns a copy of this object in XPU memory.

Fills  tensor with zeros.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Tensor Attributes

Created On: Apr 21, 2018 | Last Updated On: Apr 09, 2025

Each torch.Tensor has a torch.dtype, torch.device, and torch.layout.

================================================================================

# Tensor Attributes - torch.dtype (Part 1)

A torch.dtype is an object that represents the data type of a
torch.Tensor. PyTorch has several different data types:

Table:
Legacy Constructors
32-bit floating point | torch.float32 or torch.float | torch.*.FloatTensor
64-bit floating point | torch.float64 or torch.double | torch.*.DoubleTensor
32-bit complex | torch.complex32 or torch.chalf | 
64-bit complex | torch.complex64 or torch.cfloat | 
128-bit complex | torch.complex128 or torch.cdouble | 
16-bit floating point | torch.float16 or torch.half | torch.*.HalfTensor
16-bit floating point | torch.bfloat16 | torch.*.BFloat16Tensor
8-bit integer (unsigned) | torch.uint8 | torch.*.ByteTensor
8-bit integer (signed) | torch.int8 | torch.*.CharTensor
16-bit integer (signed) | torch.int16 or torch.short | torch.*.ShortTensor
32-bit integer (signed) | torch.int32 or | torch.*.IntTensor
64-bit integer (signed) | torch.int64 or torch.long | torch.*.LongTensor
torch.bool | torch.*.BoolTensor

32-bit floating point

torch.float32 or torch.float

64-bit floating point

torch.float64 or torch.double

torch.complex32 or torch.chalf

torch.complex64 or torch.cfloat

torch.complex128 or torch.cdouble

16-bit floating point

torch.float16 or torch.half

================================================================================

# Tensor Attributes - torch.dtype (Part 2)

16-bit floating point

torch.*.BFloat16Tensor

8-bit integer (unsigned)

8-bit integer (signed)

16-bit integer (signed)

torch.int16 or torch.short

32-bit integer (signed)

64-bit integer (signed)

torch.int64 or torch.long

Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10
significand bits. Useful when precision is important.


Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7
significand bits. Useful when range is important, since it has the same
number of exponent bits as

Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10
significand bits. Useful when precision is important.

Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7
significand bits. Useful when range is important, since it has the same
number of exponent bits as

To find out if a torch.dtype is a floating point data type, the property is_floating_point
can be used, which returns  if the data type is a floating point data type.

To find out if a torch.dtype is a complex data type, the property is_complex
can be used, which returns  if the data type is a complex data type.

================================================================================

# Tensor Attributes - torch.dtype (Part 3)

When the dtypes of inputs to an arithmetic operation (, , , ) differ, we promote
by finding the minimum dtype that satisfies the following rules:

List:
If the type of a scalar operand is of a higher category than tensor operands
(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold
all scalar operands of that category.
If a zero-dimension tensor operand has a higher category than dimensioned operands,
we promote to a type with sufficient size and category to hold all zero-dim tensor operands of
that category.
If there are no higher-category zero-dim operands, we promote to a type with sufficient size
and category to hold all dimensioned operands.

If the type of a scalar operand is of a higher category than tensor operands
(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold
all scalar operands of that category.

If a zero-dimension tensor operand has a higher category than dimensioned operands,
we promote to a type with sufficient size and category to hold all zero-dim tensor operands of
that category.

================================================================================

# Tensor Attributes - torch.dtype (Part 4)

If there are no higher-category zero-dim operands, we promote to a type with sufficient size
and category to hold all dimensioned operands.

A floating point scalar operand has dtype torch.get_default_dtype() and an integral
non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect
values when determining the minimum  of an operand.  Quantized and complex types
are not yet supported.

Code example:
float_tensor   
double_tensor   
complex_float_tensor   
complex_double_tensor   complex128
int_tensor   
long_tensor   
uint_tensor   
bool_tensor   
# zero-dim tensors
long_zerodim   
int_zerodim   

 
torch.int64
# 5 is an int64, but does not have higher category than int_tensor so is not considered.
int_tensor  
torch.int32
int_tensor  long_zerodim
torch.int32
long_tensor  int_tensor
torch.int64
bool_tensor  long_tensor
torch.int64
bool_tensor  uint_tensor
torch.uint8
float_tensor  double_tensor
torch.float64
complex_float_tensor  complex_double_tensor
torch.complex128
bool_tensor  int_tensor
torch.int32
# Since long is a different kind than float, result dtype only needs to be large enough
# to hold the float.
long_tensor float_tensor
torch.float32

================================================================================

# Tensor Attributes - torch.dtype (Part 5)

When the output tensor of an arithmetic operation is specified, we allow casting to its  except that:
An integral output tensor cannot accept a floating point tensor.
A boolean output tensor cannot accept a non-boolean tensor.
A non-complex output tensor cannot accept a complex tensor

List:
An integral output tensor cannot accept a floating point tensor.
A boolean output tensor cannot accept a non-boolean tensor.
A non-complex output tensor cannot accept a complex tensor

An integral output tensor cannot accept a floating point tensor.

A boolean output tensor cannot accept a non-boolean tensor.

A non-complex output tensor cannot accept a complex tensor

Code example:
# allowed:
 float_tensor  float_tensor
 float_tensor  int_tensor
 float_tensor  uint_tensor
 float_tensor  bool_tensor
 float_tensor  double_tensor
 int_tensor  long_tensor
 int_tensor  uint_tensor
 uint_tensor  int_tensor

# disallowed (RuntimeError: result type can't be cast to the desired output type):
 int_tensor  float_tensor
 bool_tensor  int_tensor
 bool_tensor  uint_tensor
 float_tensor  complex_float_tensor

================================================================================

# Tensor Attributes - torch.device (Part 1)

A torch.device is an object representing the device on which a torch.Tensor is
or will be allocated.

The torch.device contains a device type (most commonly “cpu” or
“cuda”, but also potentially , ,
 or ) and optional
device ordinal for the device type. If the device ordinal is not present, this object will always represent
the current device for the device type, even after torch.cuda.set_device() is called; e.g.,
a torch.Tensor constructed with device  is equivalent to  where X is
the result of torch.cuda.current_device().

A torch.Tensor’s device can be accessed via the Tensor.device property.

A torch.device can be constructed via a string or via a string and device ordinal

Code example:
device(type='cuda', index=0)


device(type='cpu')


device(type='mps')

  # current cuda device
device(type='cuda')

Via a string and device ordinal:

Code example:
device(type='cuda', index=0)

 
device(type='mps', index=0)

 
device(type='cpu', index=0)

The device object can also be used as a context manager to change the default
device tensors are allocated on:

Code example:
device(type='cuda', index=1)

================================================================================

# Tensor Attributes - torch.device (Part 2)

This context manager has no effect if a factory function is passed an explicit,
non-None device argument.  To globally change the default device, see also
torch.set_default_device().

This function imposes a slight performance cost on every Python
call to the torch API (not just factory functions).  If this
is causing problems for you, please comment on
pytorch/pytorch#92701

The torch.device argument in functions can generally be substituted with a string.
This allows for fast prototyping of code.

Code example:
# Example of a function that takes in a torch.device

Code example:
# You can substitute the torch.device with a string

For legacy reasons, a device can be constructed via a single device ordinal, which is treated
as the current accelerator type.
This matches Tensor.get_device(), which returns an ordinal for device
tensors and is not supported for cpu tensors.

Code example:
device(type='cuda', index=1)

Methods which take a device will generally accept a (properly formatted) string
or (legacy) integer device ordinal, i.e. the following are all equivalent:

================================================================================

# Tensor Attributes - torch.device (Part 3)

Tensors are never moved automatically between devices and require an explicit call from the user. Scalar Tensors (with tensor.dim()==0) are the only exception to this rule and they are automatically transferred from CPU to GPU when needed as this operation can be done “for free”.
Example:

Code example:
# two scalars
    # OK, scalar auto-transferred from CPU to GPU
    # OK, scalar auto-transferred from CPU to GPU

Code example:
# one scalar (CPU), one vector (GPU)
    # OK, scalar auto-transferred from CPU to GPU
    # OK, scalar auto-transferred from CPU to GPU

Code example:
# one scalar (GPU), one vector (CPU)
    # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU
    # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU

================================================================================

# Tensor Attributes - torch.layout

The torch.layout class is in beta and subject to change.

A torch.layout is an object that represents the memory layout of a
torch.Tensor. Currently, we support torch.strided (dense Tensors)
and have beta support for torch.sparse_coo (sparse COO Tensors).

torch.strided represents dense Tensors and is the memory layout that
is most commonly used. Each strided tensor has an associated
torch.Storage, which holds its data. These tensors provide
multi-dimensional, 
view of a storage. Strides are a list of integers: the k-th stride
represents the jump in the memory necessary to go from one element to the
next one in the k-th dimension of the Tensor. This concept makes it possible
to perform many tensor operations efficiently.

For more information on torch.sparse_coo tensors, see torch.sparse.

================================================================================

# Tensor Attributes - torch.memory_format (Part 1)

A torch.memory_format is an object representing the memory format on which a torch.Tensor is
or will be allocated.

List:
torch.contiguous_format:
Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.
torch.channels_last:
Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in
strides[0]  strides[2]  strides[3]  strides[1]   aka NHWC order.
torch.channels_last_3d:
Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in
strides[0]  strides[2]  strides[3]  strides[4]  strides[1]   aka NDHWC order.
torch.preserve_format:
Used in functions like  to preserve the memory format of the input tensor. If input tensor is
allocated in dense non-overlapping memory, the output tensor strides will be copied from the input.
Otherwise output strides will follow torch.contiguous_format

torch.contiguous_format:
Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.

================================================================================

# Tensor Attributes - torch.memory_format (Part 2)

torch.channels_last:
Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in
strides[0]  strides[2]  strides[3]  strides[1]   aka NHWC order.

torch.channels_last_3d:
Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in
strides[0]  strides[2]  strides[3]  strides[4]  strides[1]   aka NDHWC order.

torch.preserve_format:
Used in functions like  to preserve the memory format of the input tensor. If input tensor is
allocated in dense non-overlapping memory, the output tensor strides will be copied from the input.
Otherwise output strides will follow torch.contiguous_format

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Tensor Views (Part 1)

Created On: Feb 28, 2020 | Last Updated On: Feb 26, 2025

PyTorch allows a tensor to be a  of an existing tensor. View tensor shares the same underlying data
with its base tensor. Supporting  avoids explicit data copy, thus allows us to do fast and memory efficient
reshaping, slicing and element-wise operations.

For example, to get a view of an existing tensor , you can call t.view(...).

Code example:
# `t` and `b` share the same underlying data.

# Modifying view tensor changes base tensor as well.
  

tensor(3.14)

Since views share underlying data with its base tensor, if you edit the data
in the view, it will be reflected in the base tensor as well.

Typically a PyTorch op returns a new tensor as output, e.g. .
But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy.
No data movement occurs when creating a view, view tensor just changes the way
it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.
Users should pay additional attention as contiguity might have implicit performance impact.
transpose() is a common example.

Code example:
is_contiguous

================================================================================

# Tensor Views (Part 2)

     # `t` is a view of `base`. No data movement happened here.
# View tensors might be non-contiguous.
is_contiguous

# To get a contiguous tensor, call `.contiguous()` to enforce
# copying data when `t` is not contiguous.
  contiguous

For reference, here’s a full list of view ops in PyTorch:

List:
Basic slicing and indexing op, e.g.    returns a view of base , see note below.

as_strided()

diagonal()

expand_as()





transpose()







view_as_real()
unflatten()

unsqueeze()






tensor_split()
split_with_sizes()
swapaxes()
swapdims()

 (sparse tensor only)
  (sparse tensor only)

Basic slicing and indexing op, e.g.    returns a view of base , see note below.

When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors
that basic indexing returns views, while advanced indexing returns a copy.
Assignment via either basic or advanced indexing is in-place. See more examples in
Numpy indexing documentation.

It’s also worth mentioning a few ops with special behaviors:

================================================================================

# Tensor Views (Part 3)

List:
, reshape_as() and  can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not.
contiguous() returns  if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.

, reshape_as() and  can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not.

contiguous() returns  if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.

For a more detailed walk-through of PyTorch internal implementation,
please refer to ezyang’s blogpost about PyTorch Internals.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Automatic Mixed Precision package - torch.amp (Part 1)

Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025

provides convenience methods for mixed precision,
where some operations use the torch.float32 () datatype and other operations
use lower precision floating point datatype (lower_precision_fp): torch.float16 () or torch.bfloat16. Some ops, like linear layers and convolutions,
are much faster in lower_precision_fp. Other ops, like reductions, often require the dynamic
range of . Mixed precision tries to match each op to its appropriate datatype.

Ordinarily, “automatic mixed precision training” with datatype of torch.float16 uses torch.autocast and
torch.amp.GradScaler together, as shown in the Automatic Mixed Precision examples
and Automatic Mixed Precision recipe.
However, torch.autocast and torch.GradScaler are modular, and may be used separately if desired.
As shown in the CPU example section of torch.autocast, “automatic mixed precision training/inference” on CPU with
datatype of torch.bfloat16 only uses torch.autocast.

================================================================================

# Automatic Mixed Precision package - torch.amp (Part 2)

torch.cuda.amp.autocast(args...) and torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast("cuda",  or torch.amp.autocast("cpu",  instead.
torch.cuda.amp.GradScaler(args...) and torch.cpu.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler("cuda",  or torch.amp.GradScaler("cpu",  instead.

torch.autocast and torch.cpu.amp.autocast are new in version .

List:
Autocasting
Gradient Scaling
Autocast Op Reference

Op Eligibility
CUDA Op-Specific Behavior

CUDA Ops that can autocast to 
CUDA Ops that can autocast to 
CUDA Ops that promote to the widest input type
Prefer binary_cross_entropy_with_logits over binary_cross_entropy


XPU Op-Specific Behavior (Experimental)

XPU Ops that can autocast to 
XPU Ops that can autocast to 
XPU Ops that promote to the widest input type


CPU Op-Specific Behavior

CPU Ops that can autocast to 
CPU Ops that can autocast to 
CPU Ops that promote to the widest input type

Autocast Op Reference

List:
Op Eligibility
CUDA Op-Specific Behavior

CUDA Ops that can autocast to 
CUDA Ops that can autocast to 
CUDA Ops that promote to the widest input type
Prefer binary_cross_entropy_with_logits over binary_cross_entropy

================================================================================

# Automatic Mixed Precision package - torch.amp (Part 3)


XPU Op-Specific Behavior (Experimental)

XPU Ops that can autocast to 
XPU Ops that can autocast to 
XPU Ops that promote to the widest input type


CPU Op-Specific Behavior

CPU Ops that can autocast to 
CPU Ops that can autocast to 
CPU Ops that promote to the widest input type

CUDA Op-Specific Behavior

List:
CUDA Ops that can autocast to 
CUDA Ops that can autocast to 
CUDA Ops that promote to the widest input type
Prefer binary_cross_entropy_with_logits over binary_cross_entropy

CUDA Ops that can autocast to

CUDA Ops that can autocast to

CUDA Ops that promote to the widest input type

Prefer binary_cross_entropy_with_logits over binary_cross_entropy

XPU Op-Specific Behavior (Experimental)

List:
XPU Ops that can autocast to 
XPU Ops that can autocast to 
XPU Ops that promote to the widest input type

XPU Ops that can autocast to

XPU Ops that can autocast to

XPU Ops that promote to the widest input type

CPU Op-Specific Behavior

List:
CPU Ops that can autocast to 
CPU Ops that can autocast to 
CPU Ops that promote to the widest input type

CPU Ops that can autocast to

CPU Ops that can autocast to

CPU Ops that promote to the widest input type

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 1)

torch.amp.autocast_mode.is_autocast_availabledevice_type
Return a bool indicating if autocast is available on device_type.

Parameters
device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

Return type

Return a bool indicating if autocast is available on device_type.

Parameters
device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

Return type

device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 2)

device_type, , , cache_enabled
Instances of  serve as context managers or decorators that
allow regions of your script to run in mixed precision.
In these regions, ops run in an op-specific dtype chosen by autocast
to improve performance while maintaining accuracy.
See the Autocast Op Reference for details.
When entering an autocast-enabled region, Tensors may be any type.
You should not call  or bfloat16() on your model(s) or inputs when using autocasting.
 should wrap only the forward pass(es) of your network, including the loss
computation(s).  Backward passes under autocast are not recommended.
Backward ops run in the same type that autocast used for corresponding forward ops.
Example for CUDA Devices:
# Creates model and optimizer in default precision
  
  parameters 

    
    

    # Enables autocasting for the forward pass (model + loss)
     device_type
          
           

    # Exits the context manager before backward()
    
    

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 3)


See the Automatic Mixed Precision examples for usage (along with gradient scaling)
in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).
 can also be used as a decorator, e.g., on the  method of your model:
 AutocastModel
    
    device_type
      
        


Floating-point Tensors produced in an autocast-enabled region may be .
After returning to an autocast-disabled region, using them with floating-point
Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)
produced in the autocast region back to  (or other dtype if desired).
If a Tensor from the autocast region is already , the cast is a no-op,
and incurs no additional overhead.
CUDA Example:
# Creates some tensors in default dtype (here assumed to be float32)
    
    
    
    

 device_type
    # torch.mm is on autocast's list of ops that should run in float16.
    # Inputs are float32, but the op runs in float16 and produces float16 output.
    # No manual casts are required.
       
    # Also handles mixed input types
       

# After exiting autocast, calls f_float16.float() to use with d_float32
   

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 4)


CPU Training Example:
# Creates model and optimizer in default precision
  
  parameters 

   
        
        

        # Runs the forward pass with autocasting.
         device_type 
              
               

        
        


CPU Inference Example:
# Creates model in default precision
  

 device_type 
       
        # Runs the forward pass with autocasting.
          


CPU Inference Example with Jit Trace:
 
      input_size num_classes
        
          input_size num_classes
      
         

input_size  
num_classes  
  input_size num_classes

# For now, we suggest to disable the Jit Autocast Pass,
# As the issue: https://github.com/pytorch/pytorch/issues/75956
_jit_set_autocast_mode

 cache_enabled
        input_size
  
# Models Run
   
     input_size

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 5)


Type mismatch errors  an autocast-enabled region are a bug; if this is what you observe,
please file an issue.
autocast(enabled=False) subregions can be nested in autocast-enabled regions.
Locally disabling autocast can be useful, for example, if you want to force a subregion
to run in a particular .  Disabling autocast gives you explicit control over
the execution type.  In the subregion, inputs from the surrounding region
should be cast to  before use:
# Creates some tensors in default dtype (here assumed to be float32)
    
    
    
    

 device_type
       
     device_type 
        # Calls e_float16.float() to ensure float32 execution
        # (necessary because e_float16 was created in an autocasted region)
           

    # No manual casts are required when re-entering the autocast-enabled region.
    # torch.mm again runs in float16 and produces float16 output, regardless of input types.
       

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 6)


The autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator
must be invoked in that thread.  This affects torch.nn.DataParallel and
torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process
(see Working with Multiple GPUs).

Parameters

device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and ‘hpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
 () – Whether autocasting should be enabled in the region.
Default: 
 (torch_dtype) – Data type for ops run in autocast. It uses the default value
(torch.float16 for CUDA and torch.bfloat16 for CPU), given by
get_autocast_dtype(), if  is .
Default: 
cache_enabled () – Whether the weight cache inside autocast should be enabled.
Default:

Instances of  serve as context managers or decorators that
allow regions of your script to run in mixed precision.

In these regions, ops run in an op-specific dtype chosen by autocast
to improve performance while maintaining accuracy.
See the Autocast Op Reference for details.

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 7)

When entering an autocast-enabled region, Tensors may be any type.
You should not call  or bfloat16() on your model(s) or inputs when using autocasting.

should wrap only the forward pass(es) of your network, including the loss
computation(s).  Backward passes under autocast are not recommended.
Backward ops run in the same type that autocast used for corresponding forward ops.

Example for CUDA Devices:

Code example:
# Creates model and optimizer in default precision
  
  parameters 

    
    

    # Enables autocasting for the forward pass (model + loss)
     device_type
          
           

    # Exits the context manager before backward()

See the Automatic Mixed Precision examples for usage (along with gradient scaling)
in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).

can also be used as a decorator, e.g., on the  method of your model:

Code example:
AutocastModel
    
    device_type

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 8)

Floating-point Tensors produced in an autocast-enabled region may be .
After returning to an autocast-disabled region, using them with floating-point
Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)
produced in the autocast region back to  (or other dtype if desired).
If a Tensor from the autocast region is already , the cast is a no-op,
and incurs no additional overhead.
CUDA Example:

Code example:
# Creates some tensors in default dtype (here assumed to be float32)
    
    
    
    

 device_type
    # torch.mm is on autocast's list of ops that should run in float16.
    # Inputs are float32, but the op runs in float16 and produces float16 output.
    # No manual casts are required.
       
    # Also handles mixed input types
       

# After exiting autocast, calls f_float16.float() to use with d_float32

CPU Training Example:

Code example:
# Creates model and optimizer in default precision
  
  parameters 

   
        
        

        # Runs the forward pass with autocasting.
         device_type

CPU Inference Example:

Code example:
# Creates model in default precision
  

 device_type 
       
        # Runs the forward pass with autocasting.

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 9)

CPU Inference Example with Jit Trace:

Code example:
input_size num_classes
        
          input_size num_classes
      
         

input_size  
num_classes  
  input_size num_classes

# For now, we suggest to disable the Jit Autocast Pass,
# As the issue: https://github.com/pytorch/pytorch/issues/75956
_jit_set_autocast_mode

 cache_enabled
        input_size
  
# Models Run
   
     input_size

Type mismatch errors  an autocast-enabled region are a bug; if this is what you observe,
please file an issue.

autocast(enabled=False) subregions can be nested in autocast-enabled regions.
Locally disabling autocast can be useful, for example, if you want to force a subregion
to run in a particular .  Disabling autocast gives you explicit control over
the execution type.  In the subregion, inputs from the surrounding region
should be cast to  before use:

Code example:
# Creates some tensors in default dtype (here assumed to be float32)
    
    
    
    

 device_type
       
     device_type 
        # Calls e_float16.float() to ensure float32 execution
        # (necessary because e_float16 was created in an autocasted region)
           

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 10)

    # No manual casts are required when re-entering the autocast-enabled region.
    # torch.mm again runs in float16 and produces float16 output, regardless of input types.

The autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator
must be invoked in that thread.  This affects torch.nn.DataParallel and
torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process
(see Working with Multiple GPUs).

Parameters

device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and ‘hpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
 () – Whether autocasting should be enabled in the region.
Default: 
 (torch_dtype) – Data type for ops run in autocast. It uses the default value
(torch.float16 for CUDA and torch.bfloat16 for CPU), given by
get_autocast_dtype(), if  is .
Default: 
cache_enabled () – Whether the weight cache inside autocast should be enabled.
Default:

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 11)

List:
device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and ‘hpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
 () – Whether autocasting should be enabled in the region.
Default: 
 (torch_dtype) – Data type for ops run in autocast. It uses the default value
(torch.float16 for CUDA and torch.bfloat16 for CPU), given by
get_autocast_dtype(), if  is .
Default: 
cache_enabled () – Whether the weight cache inside autocast should be enabled.
Default:

device_type () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’, and ‘hpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

() – Whether autocasting should be enabled in the region.
Default:

(torch_dtype) – Data type for ops run in autocast. It uses the default value
(torch.float16 for CUDA and torch.bfloat16 for CPU), given by
get_autocast_dtype(), if  is .
Default:

cache_enabled () – Whether the weight cache inside autocast should be enabled.
Default:

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 12)

torch.amp.custom_fwd, , device_type, cast_inputs
Create a helper decorator for  methods of custom autograd functions.
Autograd functions are subclasses of torch.autograd.Function.
See the example page for more detail.

Parameters

device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
cast_inputs (torch.dtype or None, optional, default=None) – If not ,
when  runs in an autocast-enabled region, casts incoming
floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),
then executes  with autocast disabled.
If , ’s internal ops execute with the current autocast state.





If the decorated  is called outside an autocast-enabled region,
custom_fwd is a no-op and cast_inputs has no effect.

Create a helper decorator for  methods of custom autograd functions.

Autograd functions are subclasses of torch.autograd.Function.
See the example page for more detail.

Parameters

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 13)

device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
cast_inputs (torch.dtype or None, optional, default=None) – If not ,
when  runs in an autocast-enabled region, casts incoming
floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),
then executes  with autocast disabled.
If , ’s internal ops execute with the current autocast state.

List:
device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
cast_inputs (torch.dtype or None, optional, default=None) – If not ,
when  runs in an autocast-enabled region, casts incoming
floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),
then executes  with autocast disabled.
If , ’s internal ops execute with the current autocast state.

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 14)

device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

cast_inputs (torch.dtype or None, optional, default=None) – If not ,
when  runs in an autocast-enabled region, casts incoming
floating-point Tensors to the target dtype (non-floating-point Tensors are not affected),
then executes  with autocast disabled.
If , ’s internal ops execute with the current autocast state.

If the decorated  is called outside an autocast-enabled region,
custom_fwd is a no-op and cast_inputs has no effect.

torch.amp.custom_bwd, , device_type
Create a helper decorator for backward methods of custom autograd functions.
Autograd functions are subclasses of torch.autograd.Function.
Ensures that  executes with the same autocast state as .
See the example page for more detail.

Parameters
device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 15)

Create a helper decorator for backward methods of custom autograd functions.

Autograd functions are subclasses of torch.autograd.Function.
Ensures that  executes with the same autocast state as .
See the example page for more detail.

Parameters
device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

device_type () – Device type to use. ‘cuda’, ‘cpu’, ‘mtia’, ‘maia’, ‘xpu’ and so on.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

torch.cuda.amp., torch.float16, cache_enabled
See torch.autocast.
torch.cuda.amp.autocast(args...) is deprecated. Please use torch.amp.autocast("cuda",  instead.

torch.cuda.amp.autocast(args...) is deprecated. Please use torch.amp.autocast("cuda",  instead.

torch.cuda.amp.custom_fwd, , cast_inputs
torch.cuda.amp.custom_fwd(args...) is deprecated. Please use
torch.amp.custom_fwd(args..., device_type='cuda') instead.

================================================================================

# Automatic Mixed Precision package - torch.amp - Autocasting (Part 16)

torch.cuda.amp.custom_fwd(args...) is deprecated. Please use
torch.amp.custom_fwd(args..., device_type='cuda') instead.

torch.cuda.amp.custom_bwd
torch.cuda.amp.custom_bwd(args...) is deprecated. Please use
torch.amp.custom_bwd(args..., device_type='cuda') instead.

torch.cuda.amp.custom_bwd(args...) is deprecated. Please use
torch.amp.custom_bwd(args..., device_type='cuda') instead.

torch.cpu.amp., torch.bfloat16, cache_enabled
See torch.autocast.
torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast("cpu",  instead.

See torch.autocast.
torch.cpu.amp.autocast(args...) is deprecated. Please use torch.amp.autocast("cpu",  instead.

================================================================================

# Automatic Mixed Precision package - torch.amp - Gradient Scaling (Part 1)

If the forward pass for a particular op has  inputs, the backward pass for
that op will produce  gradients.
Gradient values with small magnitudes may not be representable in .
These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.

To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and
invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are
then scaled by the same factor. In other words, gradient values have a larger magnitude,
so they don’t flush to zero.

Each parameter’s gradient ( attribute) should be unscaled before the optimizer
updates the parameters, so the scale factor does not interfere with the learning rate.

================================================================================

# Automatic Mixed Precision package - torch.amp - Gradient Scaling (Part 2)

AMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in
the fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In
this case, the scale factor may decrease under 1 as an attempt to bring gradients to a number
representable in the fp16 dynamic range. While one may expect the scale to always be above 1, our
GradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss
or gradients when running with AMP/fp16, verify your model is compatible.

torch.cuda.amp.GradScalerinit_scale, growth_factor, backoff_factor, growth_interval, 
See torch.amp.GradScaler.
torch.cuda.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler("cuda",  instead.

See torch.amp.GradScaler.
torch.cuda.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler("cuda",  instead.

torch.cpu.amp.GradScalerinit_scale, growth_factor, backoff_factor, growth_interval, 
See torch.amp.GradScaler.
torch.cpu.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler("cpu",  instead.

================================================================================

# Automatic Mixed Precision package - torch.amp - Gradient Scaling (Part 3)

See torch.amp.GradScaler.
torch.cpu.amp.GradScaler(args...) is deprecated. Please use torch.amp.GradScaler("cpu",  instead.

================================================================================

# Automatic Mixed Precision package - torch.amp - Op Eligibility

Ops that run in  or non-floating-point dtypes are not eligible, and will
run in these types whether or not autocast is enabled.

Only out-of-place ops and Tensor methods are eligible.
In-place variants and calls that explicitly supply an  Tensor
are allowed in autocast-enabled regions, but won’t go through autocasting.
For example, in an autocast-enabled region a.addmm(b,  can autocast,
but a.addmm_(b,  and a.addmm(b,   cannot.
For best performance and stability, prefer out-of-place ops in autocast-enabled
regions.

Ops called with an explicit  argument are not eligible,
and will produce output that respects the  argument.

================================================================================

# Automatic Mixed Precision package - torch.amp - CUDA Op-Specific Behavior

The following lists describe the behavior of eligible ops in autocast-enabled regions.
These ops always go through autocasting whether they are invoked as part of a torch.nn.Module,
as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces,
they go through autocasting regardless of the namespace.

Ops not listed below do not go through autocasting. They run in the type
defined by their inputs. However, autocasting may still change the type
in which unlisted ops run if they’re downstream from autocasted ops.

If an op is unlisted, we assume it’s numerically stable in .
If you believe an unlisted op is numerically unstable in ,
please file an issue.

================================================================================

# Automatic Mixed Precision package - torch.amp - CUDA Ops that can autocast to

__matmul__,
,
,
,
,
,
,
chain_matmul,
,
,
,
,
conv_transpose1d,
conv_transpose2d,
conv_transpose3d,
,
,
,
,
,
,
,

================================================================================

# Automatic Mixed Precision package - torch.amp - CUDA Ops that can autocast to

,
,
,
__rtruediv__,
,
,
binary_cross_entropy_with_logits,
,
cosine_embedding_loss,
,
cosine_similarity,
cross_entropy,
,
,
,
,
,
,
group_norm,
hinge_embedding_loss,
,
,
layer_norm,
,
log_softmax,
,
,
,
margin_ranking_loss,
,
multilabel_margin_loss,
multi_margin_loss,
,
,
,
,
poisson_nll_loss,
,
,
reciprocal,
,
,
smooth_l1_loss,
soft_margin_loss,
,
,
,
,
,
,
triplet_margin_loss

================================================================================

# Automatic Mixed Precision package - torch.amp - CUDA Ops that promote to the widest input type

These ops don’t require a particular dtype for stability, but take multiple inputs
and require that the inputs’ dtypes match. If all of the inputs are
, the op runs in . If any of the inputs is ,
autocast casts all inputs to  and runs the op in .

,
,
,
,
,
,
grid_sample,
,
scatter_add,

Some ops not listed here (e.g., binary ops like ) natively promote
inputs without autocasting’s intervention. If inputs are a mixture of 
and , these ops run in  and produce  output,
regardless of whether autocast is enabled.

================================================================================

# Automatic Mixed Precision package - torch.amp - Prefer binary_cross_entropy_with_logits over binary_cross_entropy

The backward passes of torch.nn.functional.binary_cross_entropy() (and torch.nn.BCELoss, which wraps it)
can produce gradients that aren’t representable in . In autocast-enabled regions, the forward input
may be , which means the backward gradient must be representable in  (autocasting 
forward inputs to  doesn’t help, because that cast must be reversed in backward).
Therefore, binary_cross_entropy and  raise an error in autocast-enabled regions.

Many models use a sigmoid layer right before the binary cross entropy layer.
In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits()
or torch.nn.BCEWithLogitsLoss. binary_cross_entropy_with_logits and BCEWithLogits
are safe to autocast.

================================================================================

# Automatic Mixed Precision package - torch.amp - XPU Op-Specific Behavior (Experimental)

The following lists describe the behavior of eligible ops in autocast-enabled regions.
These ops always go through autocasting whether they are invoked as part of a torch.nn.Module,
as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces,
they go through autocasting regardless of the namespace.

Ops not listed below do not go through autocasting. They run in the type
defined by their inputs. However, autocasting may still change the type
in which unlisted ops run if they’re downstream from autocasted ops.

If an op is unlisted, we assume it’s numerically stable in .
If you believe an unlisted op is numerically unstable in ,
please file an issue.

================================================================================

# Automatic Mixed Precision package - torch.amp - XPU Ops that can autocast to

,
,
,
,
,
,
chain_matmul,
,
,
,
,
conv_transpose1d,
conv_transpose2d,
conv_transpose3d,
,
,
,
,
,
,

================================================================================

# Automatic Mixed Precision package - torch.amp - XPU Ops that can autocast to

,
,
,
__rtruediv__,
binary_cross_entropy_with_logits,
cosine_embedding_loss,
cosine_similarity,
,
,
,
group_norm,
hinge_embedding_loss,
,
,
layer_norm,
,
log_softmax,
margin_ranking_loss,
,
,
poisson_nll_loss,
,
reciprocal,
,
soft_margin_loss,
,
,
,
triplet_margin_loss

================================================================================

# Automatic Mixed Precision package - torch.amp - XPU Ops that promote to the widest input type

These ops don’t require a particular dtype for stability, but take multiple inputs
and require that the inputs’ dtypes match. If all of the inputs are
, the op runs in . If any of the inputs is ,
autocast casts all inputs to  and runs the op in .

,
,
grid_sample,
,
scatter_add,

Some ops not listed here (e.g., binary ops like ) natively promote
inputs without autocasting’s intervention. If inputs are a mixture of 
and , these ops run in  and produce  output,
regardless of whether autocast is enabled.

================================================================================

# Automatic Mixed Precision package - torch.amp - CPU Op-Specific Behavior

The following lists describe the behavior of eligible ops in autocast-enabled regions.
These ops always go through autocasting whether they are invoked as part of a torch.nn.Module,
as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces,
they go through autocasting regardless of the namespace.

Ops not listed below do not go through autocasting. They run in the type
defined by their inputs. However, autocasting may still change the type
in which unlisted ops run if they’re downstream from autocasted ops.

If an op is unlisted, we assume it’s numerically stable in .
If you believe an unlisted op is numerically unstable in ,
please file an issue.  shares the lists of .

================================================================================

# Automatic Mixed Precision package - torch.amp - CPU Ops that can autocast to

,
,
,
,
,
linalg_vecdot,
,
,
,
,
,
_convolution,
,
mkldnn_rnn_layer,
conv_transpose1d,
conv_transpose2d,
conv_transpose3d,
,
scaled_dot_product_attention,
_native_multi_head_attention

================================================================================

# Automatic Mixed Precision package - torch.amp - CPU Ops that can autocast to

avg_pool3d,
binary_cross_entropy,
grid_sampler,
grid_sampler_2d,
_grid_sampler_2d_cpu_fallback,
grid_sampler_3d,
,
,
,
nanquantile,
,
,
,
view_as_complex,
,
cholesky_inverse,
cholesky_solve,
,
,
,
,
,
,
max_pool3d,
max_unpool2d,
max_unpool3d,
adaptive_avg_pool3d,
reflection_pad1d,
reflection_pad2d,
replication_pad1d,
replication_pad2d,
replication_pad3d,
,
cosine_embedding_loss,
,
nll_loss2d,
hinge_embedding_loss,
poisson_nll_loss,
cross_entropy_loss,
,
huber_loss,
margin_ranking_loss,
soft_margin_loss,
triplet_margin_loss,
multi_margin_loss,
,
,
multilabel_margin_loss,
binary_cross_entropy_with_logits,
,
,
,
,
,
,
,
,
,
fft_irfft2,
,
fft_irfftn,
,
,
linalg_cond,
linalg_matrix_rank,
linalg_solve,
linalg_cholesky,
linalg_svdvals,
linalg_eigvals,
linalg_eigvalsh,
linalg_inv,
linalg_householder_product,
linalg_tensorinv,
linalg_tensorsolve,
fake_quantize_per_tensor_affine,
,
_lu_with_info,
,
,
triangular_solve,
fractional_max_pool2d,
fractional_max_pool3d,
adaptive_max_pool3d,
multilabel_margin_loss_forward,
,
linalg_cholesky_ex,
linalg_svd,
linalg_eig,
linalg_eigh,
linalg_lstsq,
linalg_inv_ex

================================================================================

# Automatic Mixed Precision package - torch.amp - CPU Ops that promote to the widest input type

These ops don’t require a particular dtype for stability, but take multiple inputs
and require that the inputs’ dtypes match. If all of the inputs are
, the op runs in . If any of the inputs is ,
autocast casts all inputs to  and runs the op in .

Some ops not listed here (e.g., binary ops like ) natively promote
inputs without autocasting’s intervention. If inputs are a mixture of 
and , these ops run in  and produce  output,
regardless of whether autocast is enabled.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Automatic differentiation package - torch.autograd

Created On: Dec 23, 2016 | Last Updated On: Jun 12, 2025

torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.

It requires minimal changes to the existing code - you only need to declare  s
for which gradients should be computed with the requires_grad=True keyword.
As of now, we only support autograd for floating point  types (
half, float, double and bfloat16) and complex  types (cfloat, cdouble).

Table:
 | Compute the sum of gradients of given tensors with respect to graph leaves.
 | Compute and return the sum of gradients of outputs with respect to the inputs.

Compute the sum of gradients of given tensors with respect to graph leaves.

Compute and return the sum of gradients of outputs with respect to the inputs.

================================================================================

# Automatic differentiation package - torch.autograd - Forward-mode Automatic Differentiation (Part 1)

This API is in beta. Even though the function signatures are very unlikely to change, improved
operator coverage is planned before we consider this stable.

Please see the forward-mode AD tutorial
for detailed steps on how to use this API.

Table:
forward_ad.dual_level | Context-manager for forward AD, where all forward AD computation must occur within the dual_level context.
forward_ad.make_dual | Associate a tensor value with its tangent to create a "dual tensor" for forward AD gradient computation.
forward_ad.unpack_dual | Unpack a "dual tensor" to get both its Tensor value and its forward AD gradient.
forward_ad.enter_dual_level | Enter a new forward grad level.
forward_ad.exit_dual_level | Exit a forward grad level.
forward_ad.UnpackedDualTensor | Namedtuple returned by unpack_dual() containing the primal and tangent components of the dual tensor.

forward_ad.dual_level

Context-manager for forward AD, where all forward AD computation must occur within the dual_level context.

Associate a tensor value with its tangent to create a "dual tensor" for forward AD gradient computation.

forward_ad.unpack_dual

Unpack a "dual tensor" to get both its Tensor value and its forward AD gradient.

================================================================================

# Automatic differentiation package - torch.autograd - Forward-mode Automatic Differentiation (Part 2)

forward_ad.enter_dual_level

Enter a new forward grad level.

forward_ad.exit_dual_level

Exit a forward grad level.

forward_ad.UnpackedDualTensor

Namedtuple returned by unpack_dual() containing the primal and tangent components of the dual tensor.

================================================================================

# Automatic differentiation package - torch.autograd - Functional higher level API (Part 1)

This API is in beta. Even though the function signatures are very unlikely to change, major
improvements to performances are planned before we consider this stable.

This section contains the higher level API for the autograd that builds on the basic API above
and allows you to compute jacobians, hessians, etc.

This API works with user-provided functions that take only Tensors as input and return
only Tensors.
If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set,
you can use a lambda to capture them.
For example, for a function  that takes three inputs, a Tensor for which we want the jacobian, another
tensor that should be considered constant and a boolean flag as   flag=flag)
you can use it as functional.jacobian(lambda    flag=flag), .

================================================================================

# Automatic differentiation package - torch.autograd - Functional higher level API (Part 2)

Table:
functional.jacobian | Compute the Jacobian of a given function.
functional.hessian | Compute the Hessian of a given scalar function.
functional.vjp | Compute the dot product between a vector  and the Jacobian of the given function at the point given by the inputs.
functional.jvp | Compute the dot product between the Jacobian of the given function at the point given by the inputs and a vector .
functional.vhp | Compute the dot product between vector  and Hessian of a  given scalar function at a specified point.
functional.hvp | Compute the dot product between the scalar function's Hessian and a vector  at a specified point.

Compute the Jacobian of a given function.

Compute the Hessian of a given scalar function.

Compute the dot product between a vector  and the Jacobian of the given function at the point given by the inputs.

Compute the dot product between the Jacobian of the given function at the point given by the inputs and a vector .

Compute the dot product between vector  and Hessian of a  given scalar function at a specified point.

Compute the dot product between the scalar function's Hessian and a vector  at a specified point.

================================================================================

# Automatic differentiation package - torch.autograd - Locally disabling gradient computation

See Locally disabling gradient computation for more information on the differences
between no-grad and inference mode as well as other related mechanisms that
may be confused with the two. Also see Locally disabling gradient computation
for a list of functions that can be used to locally disable gradients.

================================================================================

# Automatic differentiation package - torch.autograd - Default gradient layouts (Part 1)

When a non-sparse  receives a non-sparse gradient during
torch.autograd.backward() or torch.Tensor.backward()
param.grad is accumulated as follows.

If param.grad is initially :

List:
If ’s memory is non-overlapping and dense,  is
created with strides matching  (thus matching ’s
layout).
Otherwise,  is created with rowmajor-contiguous strides.

If ’s memory is non-overlapping and dense,  is
created with strides matching  (thus matching ’s
layout).

Otherwise,  is created with rowmajor-contiguous strides.

If  already has a non-sparse  attribute:

List:
If create_graph=False, backward() accumulates into 
in-place, which preserves its strides.
If create_graph=True, backward() replaces  with a
new tensor    , which attempts (but does not guarantee)
matching the preexisting ’s strides.

If create_graph=False, backward() accumulates into 
in-place, which preserves its strides.

If create_graph=True, backward() replaces  with a
new tensor    , which attempts (but does not guarantee)
matching the preexisting ’s strides.

================================================================================

# Automatic differentiation package - torch.autograd - Default gradient layouts (Part 2)

The default behavior (letting s be  before the first
backward(), such that their layout is created according to 1 or 2,
and retained over time according to 3 or 4) is recommended for best performance.
Calls to model.zero_grad() or optimizer.zero_grad() will not affect 
layouts.

In fact, resetting all s to  before each
accumulation phase, e.g.:

Code example:
iterations
    
       parameters

such that they’re recreated according to 1 or 2 every time,
is a valid alternative to model.zero_grad() or optimizer.zero_grad()
that may improve performance for some networks.

================================================================================

# Automatic differentiation package - torch.autograd - Manual gradient layouts

If you need manual control over ’s strides,
assign param.grad  a zeroed tensor with desired strides
before the first backward(), and never reset it to .
3 guarantees your layout is preserved as long as create_graph=False.
4 indicates your layout is  preserved even if create_graph=True.

================================================================================

# Automatic differentiation package - torch.autograd - In-place operations on Tensors

Supporting in-place operations in autograd is a hard matter, and we discourage
their use in most cases. Autograd’s aggressive buffer freeing and reuse makes
it very efficient and there are very few occasions when in-place operations
actually lower memory usage by any significant amount. Unless you’re operating
under heavy memory pressure, you might never need to use them.

================================================================================

# Automatic differentiation package - torch.autograd - In-place correctness checks

All  s keep track of in-place operations applied to them, and
if the implementation detects that a tensor was saved for backward in one of
the functions, but it was modified in-place afterwards, an error will be raised
once backward pass is started. This ensures that if you’re using in-place
functions and not seeing any errors, you can be sure that the computed
gradients are correct.

================================================================================

# Automatic differentiation package - torch.autograd - Variable (deprecated)

The Variable API has been deprecated: Variables are no longer necessary to
use autograd with tensors. Autograd automatically supports Tensors with
requires_grad set to . Below please find a quick guide on what
has changed:

List:
Variable(tensor) and Variable(tensor, requires_grad) still work as expected,
but they return Tensors instead of Variables.
 is the same thing as tensor.data.
Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors
with the same method names.

Variable(tensor) and Variable(tensor, requires_grad) still work as expected,
but they return Tensors instead of Variables.

is the same thing as tensor.data.

Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors
with the same method names.

In addition, one can now create tensors with requires_grad=True using factory
methods such as torch.randn(), torch.zeros(), torch.ones(), and others
like the following:

autograd_tensor  torch.randn((2,   requires_grad=True)

================================================================================

# Automatic differentiation package - torch.autograd - Tensor autograd functions (Part 1)

Table:
torch.Tensor.grad | This attribute is  by default and becomes a Tensor the first time a call to backward() computes gradients for .
torch.Tensor.requires_grad | Is  if gradients need to be computed for this Tensor,  otherwise.
torch.Tensor.is_leaf | All Tensors that have requires_grad which is  will be leaf Tensors by convention.
torch.Tensor.backward([gradient, ...]) | Computes the gradient of current tensor wrt graph leaves.
torch.Tensor.detach | Returns a new Tensor, detached from the current graph.
torch.Tensor.detach_ | Detaches the Tensor from the graph that created it, making it a leaf.
torch.Tensor.register_hook(hook) | Registers a backward hook.
torch.Tensor.register_post_accumulate_grad_hook(hook) | Registers a backward hook that runs after grad accumulation.
torch.Tensor.retain_grad() | Enables this Tensor to have their  populated during backward().

This attribute is  by default and becomes a Tensor the first time a call to backward() computes gradients for .

torch.Tensor.requires_grad

Is  if gradients need to be computed for this Tensor,  otherwise.

All Tensors that have requires_grad which is  will be leaf Tensors by convention.

torch.Tensor.backward([gradient, ...])

================================================================================

# Automatic differentiation package - torch.autograd - Tensor autograd functions (Part 2)

Computes the gradient of current tensor wrt graph leaves.

Returns a new Tensor, detached from the current graph.

Detaches the Tensor from the graph that created it, making it a leaf.

torch.Tensor.register_hook(hook)

Registers a backward hook.

torch.Tensor.register_post_accumulate_grad_hook(hook)

Registers a backward hook that runs after grad accumulation.

torch.Tensor.retain_grad()

Enables this Tensor to have their  populated during backward().

================================================================================

# Automatic differentiation package - torch.autograd -  (Part 1)

torch.autograd., 
Base class to create custom autograd.Function.
To create a custom autograd.Function, subclass this class and implement
the  and backward() static methods. Then, to use your custom
op in the forward pass, call the class method . Do not call
 directly.
To ensure correctness and best performance, make sure you are calling the
correct methods on  and validating your backward function using
torch.autograd.gradcheck().
See Extending torch.autograd for more details on how to use this class.

 
    @staticmethod
      
          
        save_for_backward
         

    @staticmethod
      grad_output
          saved_tensors
         grad_output  

# Use it by calling the apply method:

Base class to create custom autograd.Function.

To create a custom autograd.Function, subclass this class and implement
the  and backward() static methods. Then, to use your custom
op in the forward pass, call the class method . Do not call
 directly.

To ensure correctness and best performance, make sure you are calling the
correct methods on  and validating your backward function using
torch.autograd.gradcheck().

See Extending torch.autograd for more details on how to use this class.

================================================================================

# Automatic differentiation package - torch.autograd -  (Part 2)

Code example:
@staticmethod
      
          
        save_for_backward
         

    @staticmethod
      grad_output
          saved_tensors
         grad_output  

# Use it by calling the apply method:

Table:
Function.forward | Define the forward of the custom autograd Function.
Function.backward | Define a formula for differentiating the operation with backward mode automatic differentiation.
Function.jvp | Define a formula for differentiating the operation with forward mode automatic differentiation.
Function.vmap | Define the behavior for this autograd.Function underneath torch.vmap().

Define the forward of the custom autograd Function.

Define a formula for differentiating the operation with backward mode automatic differentiation.

Define a formula for differentiating the operation with forward mode automatic differentiation.

Define the behavior for this autograd.Function underneath torch.vmap().

================================================================================

# Automatic differentiation package - torch.autograd - Context method mixins

When creating a new , the following methods are available to .

Table:
function.FunctionCtx.mark_dirty | Mark given tensors as modified in an in-place operation.
function.FunctionCtx.mark_non_differentiable | Mark outputs as non-differentiable.
function.FunctionCtx.save_for_backward | Save given tensors for a future call to backward().
function.FunctionCtx.set_materialize_grads | Set whether to materialize grad tensors.

function.FunctionCtx.mark_dirty

Mark given tensors as modified in an in-place operation.

function.FunctionCtx.mark_non_differentiable

Mark outputs as non-differentiable.

function.FunctionCtx.save_for_backward

Save given tensors for a future call to backward().

function.FunctionCtx.set_materialize_grads

Set whether to materialize grad tensors.

================================================================================

# Automatic differentiation package - torch.autograd - Custom Function utilities

Decorator for backward method.

Table:
function.once_differentiable | 

function.once_differentiable

Base custom  used to build PyTorch utilities

Table:
function.BackwardCFunction | This class is used for internal autograd work.
function.InplaceFunction | This class is here only for backward compatibility reasons.
function.NestedIOFunction | This class is here only for backward compatibility reasons.

function.BackwardCFunction

This class is used for internal autograd work.

function.InplaceFunction

This class is here only for backward compatibility reasons.

function.NestedIOFunction

This class is here only for backward compatibility reasons.

================================================================================

# Automatic differentiation package - torch.autograd - Numerical gradient checking

Table:
 | Check gradients computed via small finite differences against analytical gradients wrt tensors in  that are of floating point or complex type and with requires_grad=True.
gradgradcheck | Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in  and grad_outputs that are of floating point or complex type and with requires_grad=True.
GradcheckError | Error raised by gradcheck() and gradgradcheck().

Check gradients computed via small finite differences against analytical gradients wrt tensors in  that are of floating point or complex type and with requires_grad=True.

Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in  and grad_outputs that are of floating point or complex type and with requires_grad=True.

Error raised by gradcheck() and gradgradcheck().

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 1)

Autograd includes a profiler that lets you inspect the cost of different
operators inside your model - both on the CPU and GPU. There are three modes
implemented at the moment - CPU-only using .
nvprof based (registers both CPU and GPU activity) using
.
and vtune profiler based using
.

torch.autograd.profiler., , , use_device, record_shapes, with_flops, profile_memory, with_stack, with_modules, use_kineto, , experimental_config, acc_events, custom_trace_id_callback
Context manager that manages autograd profiler state and holds a summary of results.
Under the hood it just records events of functions being executed in C++ and
exposes those events to Python. You can wrap any code into it and it will
only report runtime of PyTorch functions.
Note: profiler is thread local and is automatically propagated into the async tasks

Parameters

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 2)

 () – Setting this to False makes this context manager a no-op.
 () – Enables timing of CUDA events as well
using the cudaEvent API. (will be deprecated)
use_device () – Enables timing of device events.
Adds approximately 4us of overhead to each tensor operation when use cuda.
The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’.
record_shapes () – If shapes recording is set, information
about input dimensions will be collected. This allows one to see which
dimensions have been used under the hood and further group by them
using prof.key_averages(group_by_input_shape=True). Please note that
shape recording might skew your profiling data. It is recommended to
use separate runs with and without shape recording to validate the timing.
Most likely the skew will be negligible for bottom most events (in a case
of nested function calls). But for higher level functions the total
self cpu time might be artificially increased because of the shape
collection.
with_flops () – If with_flops is set, the profiler will estimate
the FLOPs (floating point operations) value using the operator’s input shape.
This allows one to estimate the hardware performance. Currently,
this option only works for the matrix multiplication and 2D convolution operators.
profile_memory () – track tensor memory allocation/deallocation.
with_stack () – record source information (file and line number) for the ops.
with_modules () – record module hierarchy (including function names)
corresponding to the callstack of the op. e.g. If module A’s forward call’s
module B’s forward which contains an aten::add op,
then aten::add’s module hierarchy is A.B
Note that this support exist, at the moment, only for TorchScript models
and not eager mode models.
use_kineto () – experimental, enable profiling with Kineto profiler.
 () – profile CPU events; setting to  requires
use_kineto=True and can be used to lower the overhead for GPU-only profiling.
experimental_config (_ExperimentalConfig) – A set of experimental options
used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.
acc_events () – Enable the accumulation of FunctionEvents across multiple profiling cycles

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 3)





Enabling memory profiling or source attribution incurs additional profiler
overhead



This context managers should not be called recursively, i.e. no nested
instances are allowed



Due to some CUDA multiprocessing limitations (see CUDA in multiprocessing),
one cannot use the profiler with use_device   to benchmark
DataLoaders with num_workers  . If you wish to benchmark data loading,
please use use_device   or num_workers  .

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 4)


    requires_grad
   
         # any normal python code, really!
            
        
# NOTE: some columns were removed for brevity
key_averages"self_cpu_time_total"
-----------------------------------  ---------------  ---------------  ---------------
Name                                 Self CPU total   CPU time avg     Number of Calls
-----------------------------------  ---------------  ---------------  ---------------
mul                                  32.048ms         32.048ms         200
pow                                  27.041ms         27.041ms         200
PowBackward0                         9.727ms          55.483ms         100
torch::autograd::AccumulateGrad      9.148ms          9.148ms          100
torch::autograd::GraphRoot           691.816us        691.816us        100
-----------------------------------  ---------------  ---------------  ---------------

Context manager that manages autograd profiler state and holds a summary of results.

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 5)

Under the hood it just records events of functions being executed in C++ and
exposes those events to Python. You can wrap any code into it and it will
only report runtime of PyTorch functions.
Note: profiler is thread local and is automatically propagated into the async tasks

Parameters

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 6)

 () – Setting this to False makes this context manager a no-op.
 () – Enables timing of CUDA events as well
using the cudaEvent API. (will be deprecated)
use_device () – Enables timing of device events.
Adds approximately 4us of overhead to each tensor operation when use cuda.
The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’.
record_shapes () – If shapes recording is set, information
about input dimensions will be collected. This allows one to see which
dimensions have been used under the hood and further group by them
using prof.key_averages(group_by_input_shape=True). Please note that
shape recording might skew your profiling data. It is recommended to
use separate runs with and without shape recording to validate the timing.
Most likely the skew will be negligible for bottom most events (in a case
of nested function calls). But for higher level functions the total
self cpu time might be artificially increased because of the shape
collection.
with_flops () – If with_flops is set, the profiler will estimate
the FLOPs (floating point operations) value using the operator’s input shape.
This allows one to estimate the hardware performance. Currently,
this option only works for the matrix multiplication and 2D convolution operators.
profile_memory () – track tensor memory allocation/deallocation.
with_stack () – record source information (file and line number) for the ops.
with_modules () – record module hierarchy (including function names)
corresponding to the callstack of the op. e.g. If module A’s forward call’s
module B’s forward which contains an aten::add op,
then aten::add’s module hierarchy is A.B
Note that this support exist, at the moment, only for TorchScript models
and not eager mode models.
use_kineto () – experimental, enable profiling with Kineto profiler.
 () – profile CPU events; setting to  requires
use_kineto=True and can be used to lower the overhead for GPU-only profiling.
experimental_config (_ExperimentalConfig) – A set of experimental options
used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.
acc_events () – Enable the accumulation of FunctionEvents across multiple profiling cycles

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 7)

List:
() – Setting this to False makes this context manager a no-op.
 () – Enables timing of CUDA events as well
using the cudaEvent API. (will be deprecated)
use_device () – Enables timing of device events.
Adds approximately 4us of overhead to each tensor operation when use cuda.
The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’.
record_shapes () – If shapes recording is set, information
about input dimensions will be collected. This allows one to see which
dimensions have been used under the hood and further group by them
using prof.key_averages(group_by_input_shape=True). Please note that
shape recording might skew your profiling data. It is recommended to
use separate runs with and without shape recording to validate the timing.
Most likely the skew will be negligible for bottom most events (in a case
of nested function calls). But for higher level functions the total
self cpu time might be artificially increased because of the shape
collection.
with_flops () – If with_flops is set, the profiler will estimate
the FLOPs (floating point operations) value using the operator’s input shape.
This allows one to estimate the hardware performance. Currently,
this option only works for the matrix multiplication and 2D convolution operators.
profile_memory () – track tensor memory allocation/deallocation.
with_stack () – record source information (file and line number) for the ops.
with_modules () – record module hierarchy (including function names)
corresponding to the callstack of the op. e.g. If module A’s forward call’s
module B’s forward which contains an aten::add op,
then aten::add’s module hierarchy is A.B
Note that this support exist, at the moment, only for TorchScript models
and not eager mode models.
use_kineto () – experimental, enable profiling with Kineto profiler.
 () – profile CPU events; setting to  requires
use_kineto=True and can be used to lower the overhead for GPU-only profiling.
experimental_config (_ExperimentalConfig) – A set of experimental options
used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.
acc_events () – Enable the accumulation of FunctionEvents across multiple profiling cycles

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 8)

() – Setting this to False makes this context manager a no-op.

() – Enables timing of CUDA events as well
using the cudaEvent API. (will be deprecated)

use_device () – Enables timing of device events.
Adds approximately 4us of overhead to each tensor operation when use cuda.
The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’.

record_shapes () – If shapes recording is set, information
about input dimensions will be collected. This allows one to see which
dimensions have been used under the hood and further group by them
using prof.key_averages(group_by_input_shape=True). Please note that
shape recording might skew your profiling data. It is recommended to
use separate runs with and without shape recording to validate the timing.
Most likely the skew will be negligible for bottom most events (in a case
of nested function calls). But for higher level functions the total
self cpu time might be artificially increased because of the shape
collection.

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 9)

with_flops () – If with_flops is set, the profiler will estimate
the FLOPs (floating point operations) value using the operator’s input shape.
This allows one to estimate the hardware performance. Currently,
this option only works for the matrix multiplication and 2D convolution operators.

profile_memory () – track tensor memory allocation/deallocation.

with_stack () – record source information (file and line number) for the ops.

with_modules () – record module hierarchy (including function names)
corresponding to the callstack of the op. e.g. If module A’s forward call’s
module B’s forward which contains an aten::add op,
then aten::add’s module hierarchy is A.B
Note that this support exist, at the moment, only for TorchScript models
and not eager mode models.

use_kineto () – experimental, enable profiling with Kineto profiler.

() – profile CPU events; setting to  requires
use_kineto=True and can be used to lower the overhead for GPU-only profiling.

experimental_config (_ExperimentalConfig) – A set of experimental options
used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 10)

acc_events () – Enable the accumulation of FunctionEvents across multiple profiling cycles

Enabling memory profiling or source attribution incurs additional profiler
overhead

This context managers should not be called recursively, i.e. no nested
instances are allowed

Due to some CUDA multiprocessing limitations (see CUDA in multiprocessing),
one cannot use the profiler with use_device   to benchmark
DataLoaders with num_workers  . If you wish to benchmark data loading,
please use use_device   or num_workers  .

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 11)

Code example:
requires_grad
   
         # any normal python code, really!
            
        
# NOTE: some columns were removed for brevity
key_averages"self_cpu_time_total"
-----------------------------------  ---------------  ---------------  ---------------
Name                                 Self CPU total   CPU time avg     Number of Calls
-----------------------------------  ---------------  ---------------  ---------------
mul                                  32.048ms         32.048ms         200
pow                                  27.041ms         27.041ms         200
PowBackward0                         9.727ms          55.483ms         100
torch::autograd::AccumulateGrad      9.148ms          9.148ms          100
torch::autograd::GraphRoot           691.816us        691.816us        100
-----------------------------------  ---------------  ---------------  ---------------

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 12)

Table:
profiler.profile.export_chrome_trace | Export an EventList as a Chrome tracing tools file.
profiler.profile.key_averages | Averages all function events over their keys.
profiler.profile.self_cpu_time_total | Returns total time spent on CPU.
profiler.profile.total_average | Averages all events.
profiler.parse_nvprof_trace | 
profiler.EnforceUnique | Raises an error if a key is seen more than once.
profiler.KinetoStepTracker | Provides an abstraction for incrementing the step count globally.
profiler.record_function | Context manager/function decorator that adds a label to a code block/function when running autograd profiler.
profiler_util.Interval | 
profiler_util.Kernel | 
profiler_util.MemRecordsAcc | Acceleration structure for accessing mem_records in interval.
profiler_util.StringTable | 

profiler.profile.export_chrome_trace

Export an EventList as a Chrome tracing tools file.

profiler.profile.key_averages

Averages all function events over their keys.

profiler.profile.self_cpu_time_total

Returns total time spent on CPU.

profiler.profile.total_average

profiler.parse_nvprof_trace

profiler.EnforceUnique

Raises an error if a key is seen more than once.

profiler.KinetoStepTracker

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 13)

Provides an abstraction for incrementing the step count globally.

profiler.record_function

Context manager/function decorator that adds a label to a code block/function when running autograd profiler.

profiler_util.Interval

profiler_util.MemRecordsAcc

Acceleration structure for accessing mem_records in interval.

profiler_util.StringTable

torch.autograd.profiler., record_shapes
Context manager that makes every autograd operation emit an NVTX range.
It is useful when running the program under nvprof:
    trace_name    


Unfortunately, there’s no way to force nvprof to flush the data it collected
to disk, so for CUDA profiling one has to use this context manager to annotate
nvprof traces and wait for the process to exit before inspecting them.
Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or
torch.autograd.profiler.load_nvprof() can load the results for inspection
e.g. in Python REPL.

Parameters

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 14)

 () – Setting enabled=False makes this context manager a no-op.
Default: .
record_shapes () – If record_shapes=True, the nvtx range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
Default: 




 
      # Warmup CUDA memory allocator and profiler
     
        

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 15)


Forward-backward correlation
When viewing a profile created using  in the Nvidia Visual Profiler,
correlating each backward-pass op with the corresponding forward-pass op can be difficult.
To ease this task,  appends sequence number information to the ranges it
generates.
During the forward pass, each function range is decorated with .   is a running
counter, incremented each time a new backward Function object is created and stashed for backward.
Thus, the  annotation associated with each forward function range tells you that
if a backward Function object is created by this forward function,
the backward object will receive sequence number N.
During the backward pass, the top-level range wrapping each C++ backward Function’s
 call is decorated with  .   is the sequence number that
the backward object was created with.  By comparing   numbers in backward with 
numbers in forward, you can track down which forward op created each backward Function.
Any functions executed during the backward pass are also decorated with .  During
default backward (with create_graph=False) this information is irrelevant, and in fact,
 may simply be 0 for all such functions.  Only the top-level ranges associated with
backward Function objects’  methods are useful, as a way to correlate these Function
objects with the earlier forward pass.
Double-backward
If, on the other hand, a backward pass with create_graph=True is underway (in other words,
if you are setting up for a double-backward), each function’s execution during backward
is given a nonzero, useful .  Those functions may themselves create Function objects
to be executed later during double-backward, just as the original functions in the forward pass did.
The relationship between backward and double-backward is conceptually the same as the relationship
between forward and backward: The functions still emit current-sequence-number-tagged ranges,
the Function objects they create still stash those sequence numbers, and during the eventual
double-backward, the Function objects’  ranges are still tagged with  
numbers, which can be compared to  numbers from the backward pass.

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 16)

Context manager that makes every autograd operation emit an NVTX range.

It is useful when running the program under nvprof:

Unfortunately, there’s no way to force nvprof to flush the data it collected
to disk, so for CUDA profiling one has to use this context manager to annotate
nvprof traces and wait for the process to exit before inspecting them.
Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or
torch.autograd.profiler.load_nvprof() can load the results for inspection
e.g. in Python REPL.

Parameters

 () – Setting enabled=False makes this context manager a no-op.
Default: .
record_shapes () – If record_shapes=True, the nvtx range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
Default:

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 17)

List:
() – Setting enabled=False makes this context manager a no-op.
Default: .
record_shapes () – If record_shapes=True, the nvtx range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
Default:

() – Setting enabled=False makes this context manager a no-op.
Default: .

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 18)

record_shapes () – If record_shapes=True, the nvtx range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
Default:

Code example:
# Warmup CUDA memory allocator and profiler

Forward-backward correlation

When viewing a profile created using  in the Nvidia Visual Profiler,
correlating each backward-pass op with the corresponding forward-pass op can be difficult.
To ease this task,  appends sequence number information to the ranges it
generates.

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 19)

During the forward pass, each function range is decorated with .   is a running
counter, incremented each time a new backward Function object is created and stashed for backward.
Thus, the  annotation associated with each forward function range tells you that
if a backward Function object is created by this forward function,
the backward object will receive sequence number N.
During the backward pass, the top-level range wrapping each C++ backward Function’s
 call is decorated with  .   is the sequence number that
the backward object was created with.  By comparing   numbers in backward with 
numbers in forward, you can track down which forward op created each backward Function.

Any functions executed during the backward pass are also decorated with .  During
default backward (with create_graph=False) this information is irrelevant, and in fact,
 may simply be 0 for all such functions.  Only the top-level ranges associated with
backward Function objects’  methods are useful, as a way to correlate these Function
objects with the earlier forward pass.

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 20)

If, on the other hand, a backward pass with create_graph=True is underway (in other words,
if you are setting up for a double-backward), each function’s execution during backward
is given a nonzero, useful .  Those functions may themselves create Function objects
to be executed later during double-backward, just as the original functions in the forward pass did.
The relationship between backward and double-backward is conceptually the same as the relationship
between forward and backward: The functions still emit current-sequence-number-tagged ranges,
the Function objects they create still stash those sequence numbers, and during the eventual
double-backward, the Function objects’  ranges are still tagged with  
numbers, which can be compared to  numbers from the backward pass.

torch.autograd.profiler., record_shapes
Context manager that makes every autograd operation emit an ITT range.
It is useful when running the program under Intel(R) VTune Profiler:
    

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 21)


The Instrumentation and Tracing Technology (ITT) API enables your application to generate and
control the collection of trace data during its execution across different Intel tools.
This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,
you will be able to see labled ranges in Intel(R) VTune Profiler GUI.

Parameters

 () – Setting enabled=False makes this context manager a no-op.
Default: .
record_shapes () – If record_shapes=True, the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of itt range creation.
Default:

Context manager that makes every autograd operation emit an ITT range.

It is useful when running the program under Intel(R) VTune Profiler:

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 22)

The Instrumentation and Tracing Technology (ITT) API enables your application to generate and
control the collection of trace data during its execution across different Intel tools.
This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,
you will be able to see labled ranges in Intel(R) VTune Profiler GUI.

Parameters

 () – Setting enabled=False makes this context manager a no-op.
Default: .
record_shapes () – If record_shapes=True, the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of itt range creation.
Default:

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 23)

List:
() – Setting enabled=False makes this context manager a no-op.
Default: .
record_shapes () – If record_shapes=True, the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of itt range creation.
Default:

() – Setting enabled=False makes this context manager a no-op.
Default: .

================================================================================

# Automatic differentiation package - torch.autograd - Profiler (Part 24)

record_shapes () – If record_shapes=True, the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format:
[[arg0.size(0), arg0.size(1),  [arg1.size(0), arg1.size(1),  
Non-tensor arguments will be represented by .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of itt range creation.
Default:

Table:
profiler.load_nvprof | Open an nvprof trace file and parses autograd annotations.

Open an nvprof trace file and parses autograd annotations.

================================================================================

# Automatic differentiation package - torch.autograd - Debugging and anomaly detection (Part 1)

torch.autograd.detect_anomaly
Context-manager that enable anomaly detection for the autograd engine.
This does two things:

Running the forward pass with detection enabled will allow the backward
pass to print the traceback of the forward operation that created the failing
backward function.
If  is , any backward computation that generate “nan”
value will raise an error. Default .



This mode should be enabled only for debugging as the different tests
will slow down your program execution.


 
   
 
    @staticmethod
      
         
    @staticmethod
      
        # Error during the backward pass
         RuntimeError"Some error in backward"
         
 
      
     
    requires_grad
  

================================================================================

# Automatic differentiation package - torch.autograd - Debugging and anomaly detection (Part 2)

    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "/your/pytorch/install/torch/_tensor.py", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File "<stdin>", line 8, in backward
    RuntimeError: Some error in backward
 detect_anomaly
        requires_grad
      
    
    Traceback of forward call that caused the error:
      File "tmp.py", line 53, in <module>
        out = run_fn(inp)
      File "tmp.py", line 44, in run_fn
        out = MyFunc.apply(a)
    Traceback (most recent call last):
      File "<stdin>", line 4, in <module>
      File "/your/pytorch/install/torch/_tensor.py", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File "<stdin>", line 8, in backward
    RuntimeError: Some error in backward

================================================================================

# Automatic differentiation package - torch.autograd - Debugging and anomaly detection (Part 3)

Context-manager that enable anomaly detection for the autograd engine.

This does two things:

List:
Running the forward pass with detection enabled will allow the backward
pass to print the traceback of the forward operation that created the failing
backward function.
If  is , any backward computation that generate “nan”
value will raise an error. Default .

Running the forward pass with detection enabled will allow the backward
pass to print the traceback of the forward operation that created the failing
backward function.

If  is , any backward computation that generate “nan”
value will raise an error. Default .

This mode should be enabled only for debugging as the different tests
will slow down your program execution.

Code example:
@staticmethod
      
         
    @staticmethod
      
        # Error during the backward pass
         RuntimeError"Some error in backward"
         
 
      
     
    requires_grad
  

================================================================================

# Automatic differentiation package - torch.autograd - Debugging and anomaly detection (Part 4)

    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
      File "/your/pytorch/install/torch/_tensor.py", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File "<stdin>", line 8, in backward
    RuntimeError: Some error in backward
 detect_anomaly
        requires_grad
      
    
    Traceback of forward call that caused the error:
      File "tmp.py", line 53, in <module>
        out = run_fn(inp)
      File "tmp.py", line 44, in run_fn
        out = MyFunc.apply(a)
    Traceback (most recent call last):
      File "<stdin>", line 4, in <module>
      File "/your/pytorch/install/torch/_tensor.py", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File "<stdin>", line 8, in backward
    RuntimeError: Some error in backward

================================================================================

# Automatic differentiation package - torch.autograd - Debugging and anomaly detection (Part 5)

torch.autograd.set_detect_anomaly, 
Context-manager that sets the anomaly detection for the autograd engine on or off.
set_detect_anomaly will enable or disable the autograd anomaly detection
based on its argument .
It can be used as a context-manager or as a function.
See detect_anomaly above for details of the anomaly detection behaviour.

Parameters

 () – Flag whether to enable anomaly detection (),
or disable ().
 () – Flag whether to raise an error when the backward
generate “nan”

Context-manager that sets the anomaly detection for the autograd engine on or off.

set_detect_anomaly will enable or disable the autograd anomaly detection
based on its argument .
It can be used as a context-manager or as a function.

See detect_anomaly above for details of the anomaly detection behaviour.

Parameters

 () – Flag whether to enable anomaly detection (),
or disable ().
 () – Flag whether to raise an error when the backward
generate “nan”

List:
() – Flag whether to enable anomaly detection (),
or disable ().
 () – Flag whether to raise an error when the backward
generate “nan”

() – Flag whether to enable anomaly detection (),
or disable ().

================================================================================

# Automatic differentiation package - torch.autograd - Debugging and anomaly detection (Part 6)

() – Flag whether to raise an error when the backward
generate “nan”

Table:
grad_mode.set_multithreading_enabled | Context-manager that sets multithreaded backwards on or off.

grad_mode.set_multithreading_enabled

Context-manager that sets multithreaded backwards on or off.

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 1)

Autograd exposes methods that allow one to inspect the graph and interpose behavior during
the backward pass.

The  attribute of a torch.Tensor holds a torch.autograd.graph.Node
if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is
enabled and at least one of the inputs required gradients), or  otherwise.

Table:
graph.Node.name | Return the name.
graph.Node.metadata | Return the metadata.
graph.Node.next_functions | 
graph.Node.register_hook | Register a backward hook.
graph.Node.register_prehook | Register a backward pre-hook.
graph.increment_version | Update autograd metadata tracking whether the given Tensor was modified in place.

graph.Node.next_functions

graph.Node.register_hook

Register a backward hook.

graph.Node.register_prehook

Register a backward pre-hook.

graph.increment_version

Update autograd metadata tracking whether the given Tensor was modified in place.

Some operations need intermediary results to be saved during the forward pass
in order to execute the backward pass.
These intermediary results are saved as attributes on the  and can be accessed.
For example:

Code example:
requires_grad
  
isinstance 

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 2)


['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']
_saved_result

You can also define how these saved tensors should be packed / unpacked using hooks.
A common application is to trade compute for memory by saving those intermediary results
to disk or to CPU instead of leaving them on the GPU. This is especially useful if you
notice your model fits on GPU during evaluation, but not training.
Also see Hooks for saved tensors.

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 3)

torch.autograd.graph.saved_tensors_hooks, unpack_hook
Context-manager that sets a pair of pack / unpack hooks for saved tensors.
Use this context-manager to define how intermediary results of an operation
should be packed before saving, and unpacked on retrieval.
In that context, the  function will be called everytime an
operation saves a tensor for backward (this includes intermediary results
saved using
save_for_backward() but
also those recorded by a PyTorch-defined operation). The output of
 is then stored in the computation graph instead of the
original tensor.
The unpack_hook is called when the saved tensor needs to be accessed,
namely when executing torch.Tensor.backward() or
torch.autograd.grad(). It takes as argument the  object
returned by  and should return a tensor which has the same
content as the original tensor (passed as input to the corresponding
).
The hooks should have the following signatures:

pack_hook(tensor: Tensor) -> Any
unpack_hook(Any) -> Tensor

where the return value of  is a valid input to unpack_hook.
In general, you want unpack_hook(pack_hook(t)) to be equal to  in terms
of value, size, dtype and device.

 
     
     

 unpack_hook
    "Unpacking" 
     

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 4)

   requires_grad
   requires_grad  
 saved_tensors_hooks unpack_hook
        
Packing tensor([1., 1., 1., 1., 1.], requires_grad=True)
Packing tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)

Unpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)
Unpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)




Performing an inplace operation on the input to either hooks may lead
to undefined behavior.



Only one pair of hooks is allowed at a time. When recursively nesting this
context-manager, only the inner-most pair of hooks will be applied.



To avoid reference cycle, the return value of  cannot hold a
reference to the input tensor. For example, use lambda x: x.detach()
instead of lambda x: x as the pack hook.

Context-manager that sets a pair of pack / unpack hooks for saved tensors.

Use this context-manager to define how intermediary results of an operation
should be packed before saving, and unpacked on retrieval.

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 5)

In that context, the  function will be called everytime an
operation saves a tensor for backward (this includes intermediary results
saved using
save_for_backward() but
also those recorded by a PyTorch-defined operation). The output of
 is then stored in the computation graph instead of the
original tensor.

The unpack_hook is called when the saved tensor needs to be accessed,
namely when executing torch.Tensor.backward() or
torch.autograd.grad(). It takes as argument the  object
returned by  and should return a tensor which has the same
content as the original tensor (passed as input to the corresponding
).

The hooks should have the following signatures:

Quote:
pack_hook(tensor: Tensor) -> Any
unpack_hook(Any) -> Tensor

pack_hook(tensor: Tensor) -> Any

unpack_hook(Any) -> Tensor

where the return value of  is a valid input to unpack_hook.

In general, you want unpack_hook(pack_hook(t)) to be equal to  in terms
of value, size, dtype and device.

Code example:
unpack_hook
    "Unpacking" 
     

   requires_grad
   requires_grad  
 saved_tensors_hooks unpack_hook
        
Packing tensor([1., 1., 1., 1., 1.], requires_grad=True)
Packing tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 6)

Unpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)
Unpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)

Performing an inplace operation on the input to either hooks may lead
to undefined behavior.

Only one pair of hooks is allowed at a time. When recursively nesting this
context-manager, only the inner-most pair of hooks will be applied.

To avoid reference cycle, the return value of  cannot hold a
reference to the input tensor. For example, use lambda x: x.detach()
instead of lambda x: x as the pack hook.

torch.autograd.graph.save_on_cpupin_memory, device_type
Context manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.
When performing operations within this context manager, intermediary
results saved in the graph during the forward pass will be moved to CPU,
then copied back to the original device when needed for the backward pass.
If the graph was already on CPU, no tensor copy is performed.
Use this context-manager to trade compute for GPU memory usage (e.g.
when your model doesn’t fit in GPU memory during training).

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 7)

Parameters
pin_memory () – If  tensors will be saved to CPU pinned memory
during packing and copied to GPU asynchronously during unpacking.
Defaults to .
Also see Use pinned memory buffers.



   requires_grad 
   requires_grad 
   requires_grad 

   
                   # a and b are saved on GPU
     save_on_cpu
              # prod_1 and c are saved on CPU
                   # prod_2 and a are saved on GPU
     

    
     # for illustration only
# the content of a, b, and prod_2 are still alive on GPU
# the content of prod_1 and c only live on CPU
  # all CPU tensors are moved back to GPU, for backward
# all intermediary tensors are released (deleted) after the call to backward

Context manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.

When performing operations within this context manager, intermediary
results saved in the graph during the forward pass will be moved to CPU,
then copied back to the original device when needed for the backward pass.
If the graph was already on CPU, no tensor copy is performed.

Use this context-manager to trade compute for GPU memory usage (e.g.
when your model doesn’t fit in GPU memory during training).

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 8)

Parameters
pin_memory () – If  tensors will be saved to CPU pinned memory
during packing and copied to GPU asynchronously during unpacking.
Defaults to .
Also see Use pinned memory buffers.

pin_memory () – If  tensors will be saved to CPU pinned memory
during packing and copied to GPU asynchronously during unpacking.
Defaults to .
Also see Use pinned memory buffers.

Code example:
requires_grad 
   requires_grad 
   requires_grad 

   
                   # a and b are saved on GPU
     save_on_cpu
              # prod_1 and c are saved on CPU
                   # prod_2 and a are saved on GPU
     

    
     # for illustration only
# the content of a, b, and prod_2 are still alive on GPU
# the content of prod_1 and c only live on CPU
  # all CPU tensors are moved back to GPU, for backward
# all intermediary tensors are released (deleted) after the call to backward

torch.autograd.graph.disable_saved_tensors_hookserror_message
Context-manager that disables the saved tensors default hooks feature.
Useful for if you are creating a feature that does not work with saved
tensors default hooks.

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 9)

Parameters
error_message () – When saved tensors default hooks are used when they
have been are disabled, a RuntimeError with this
error message gets raised.

Return type
[None, None, None]



  "saved tensors default hooks are disabled"
 disable_saved_tensors_hooks
    # Raises RuntimeError: saved tensors default hooks are disabled
     save_on_cpu

Context-manager that disables the saved tensors default hooks feature.

Useful for if you are creating a feature that does not work with saved
tensors default hooks.

Parameters
error_message () – When saved tensors default hooks are used when they
have been are disabled, a RuntimeError with this
error message gets raised.

Return type
[None, None, None]

error_message () – When saved tensors default hooks are used when they
have been are disabled, a RuntimeError with this
error message gets raised.

Code example:
"saved tensors default hooks are disabled"
 disable_saved_tensors_hooks
    # Raises RuntimeError: saved tensors default hooks are disabled
     save_on_cpu

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 10)

torch.autograd.graph.register_multi_grad_hook, , , 
Register a multi-grad backward hook.
There are two supported modes:  and .
Under the  mode, the hook will be called after gradients with respect to every tensor in
 have been computed. If a tensor is in  but
is not part of the graph, or if a tensor is not needed to compute the gradients
for any  specified for the current .backward() or  call,
this tensor will be ignored and the hook will not wait for its gradient to be
computed.
After every non-ignored tensor’s gradient has been computed,  will be
called with those gradients.  will be passed for tensors that did not
have their gradients computed.
Under the  mode, the hook will be called after the first gradient
with respect to a tensor in  has been computed. The hook
will be called with that gradient as its argument.
The hook should not modify its arguments.
This function returns a handle with a method handle.remove() that removes the hook.


See Backward Hooks execution for more information on how when this hook
is executed, and how its execution is ordered relative to other hooks.


 

    requires_grad
    requires_grad
    
    

 
           

register_multi_grad_hook    

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 11)

retain_graph
[True, True, True, False]
 retain_graph
[True, False, True, False]




Return type
RemovableHandle

Register a multi-grad backward hook.

There are two supported modes:  and .

Under the  mode, the hook will be called after gradients with respect to every tensor in
 have been computed. If a tensor is in  but
is not part of the graph, or if a tensor is not needed to compute the gradients
for any  specified for the current .backward() or  call,
this tensor will be ignored and the hook will not wait for its gradient to be
computed.

After every non-ignored tensor’s gradient has been computed,  will be
called with those gradients.  will be passed for tensors that did not
have their gradients computed.

Under the  mode, the hook will be called after the first gradient
with respect to a tensor in  has been computed. The hook
will be called with that gradient as its argument.

The hook should not modify its arguments.

This function returns a handle with a method handle.remove() that removes the hook.

See Backward Hooks execution for more information on how when this hook
is executed, and how its execution is ordered relative to other hooks.

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 12)

Code example:
requires_grad
    requires_grad
    
    

 
           

register_multi_grad_hook    

retain_graph
[True, True, True, False]
 retain_graph
[True, False, True, False]

Return type
RemovableHandle

torch.autograd.graph.allow_mutation_on_saved_tensors
Context manager under which mutating tensors saved for backward is allowed.
Under this context manager, tensors saved for backward are cloned on mutation,
so the original version can still be used during backward. Normally, mutating a tensor
saved for backward will result in an error raised when it’s used during backward.
To ensure the correct behavior, both the forward and backward should be run under
the same context manager.


An _AllowMutationOnSavedContext object storing the state managed by this
context manager. This object can be useful for debugging purposes. The state
managed by the context manager is automatically cleared upon exiting.

Return type
[_AllowMutationOnSavedContext, None, None]



 
 allow_mutation_on_saved_tensors
    
        requires_grad
      
      
    
    # backward
    

tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 13)

Context manager under which mutating tensors saved for backward is allowed.

Under this context manager, tensors saved for backward are cloned on mutation,
so the original version can still be used during backward. Normally, mutating a tensor
saved for backward will result in an error raised when it’s used during backward.

To ensure the correct behavior, both the forward and backward should be run under
the same context manager.

An _AllowMutationOnSavedContext object storing the state managed by this
context manager. This object can be useful for debugging purposes. The state
managed by the context manager is automatically cleared upon exiting.

Return type
[_AllowMutationOnSavedContext, None, None]

An _AllowMutationOnSavedContext object storing the state managed by this
context manager. This object can be useful for debugging purposes. The state
managed by the context manager is automatically cleared upon exiting.

[_AllowMutationOnSavedContext, None, None]

Code example:
allow_mutation_on_saved_tensors
    
        requires_grad
      
      
    
    # backward
    

tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)

================================================================================

# Automatic differentiation package - torch.autograd - Autograd graph (Part 14)

torch.autograd.graph.GradientEdge, 
Object representing a given gradient edge within the autograd graph.
To get the gradient edge where a given Tensor gradient will be computed,
you can do   autograd.graph.get_gradient_edge(tensor).

Object representing a given gradient edge within the autograd graph.

To get the gradient edge where a given Tensor gradient will be computed,
you can do   autograd.graph.get_gradient_edge(tensor).

torch.autograd.graph.get_gradient_edge
Get the gradient edge for computing the gradient of the given Tensor.
In particular, it is equivalent to call
  autograd.grad(loss,  and   autograd.grad(loss, get_gradient_edge(input)).

Return type
GradientEdge

Get the gradient edge for computing the gradient of the given Tensor.

In particular, it is equivalent to call
  autograd.grad(loss,  and   autograd.grad(loss, get_gradient_edge(input)).

Return type
GradientEdge

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.library

Created On: Jun 13, 2022 | Last Updated On: Jun 07, 2025

torch.library is a collection of APIs for extending PyTorch’s core library
of operators. It contains utilities for testing custom operators, creating new
custom operators, and extending operators defined with PyTorch’s C++ operator
registration APIs (e.g. aten operators).

For a detailed guide on effectively using these APIs, please see
PyTorch Custom Operators Landing Page
for more details on how to effectively use these APIs.

================================================================================

# torch.library - Testing custom ops (Part 1)

Use torch.library.opcheck() to test custom ops for incorrect usage of the
Python torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports
training, use torch.autograd.gradcheck() to test that the gradients are
mathematically correct.

torch.library., , , , test_utils('test_schema', 'test_autograd_registration', 'test_faketensor', 'test_aot_dispatch_dynamic'), raise_exception, , 
Given an operator and some sample arguments, tests if the operator is
registered correctly.
That is, when you use the torch.library/TORCH_LIBRARY APIs to create a
custom op, you specified metadata (e.g. mutability info) about the custom op
and these APIs require that the functions you pass them satisfy certain
properties (e.g. no data pointer access in the fake/meta/abstract kernel)
 tests these metadata and properties.
Concretely, we test the following:

================================================================================

# torch.library - Testing custom ops (Part 2)

test_schema: If the schema matches the implementation of
the operator. For example: if the schema specifies a Tensor is mutated,
then we check the implementation mutates the Tensor. If the schema
specifies that we return a new Tensor, then we check that the
implementation returns a new Tensor (instead of an existing one or
a view of an existing one).
test_autograd_registration: If the operator supports training
(autograd): we check that its autograd formula is registered via
torch.library.register_autograd or a manual registration to one
or more DispatchKey::Autograd keys. Any other DispatchKey-based
registrations may lead to undefined behavior.
test_faketensor: If the operator has a FakeTensor kernel
(and if it is correct). The FakeTensor kernel is necessary (
but not sufficient) for the operator to work with PyTorch compilation
APIs (torch.compile/export/FX). We check that a FakeTensor kernel
(also sometimes known as a meta kernel) was registered for the
operator and that it is correct. This test takes the result of
running the operator on real tensors and the result of running
the operator on FakeTensors and checks that they have the same
Tensor metadata (sizes/strides/dtype/device/etc).
test_aot_dispatch_dynamic: If the operator has correct behavior
with PyTorch compilation APIs (torch.compile/export/FX).
This checks that the outputs (and gradients, if applicable) are the
same under eager-mode PyTorch and torch.compile.
This test is a superset of test_faketensor and is an e2e test;
other things it tests are that the operator supports
functionalization and that the backward pass (if it exists) also
supports FakeTensor and functionalization.

================================================================================

# torch.library - Testing custom ops (Part 3)

For best results, please call  multiple times with a
representative set of inputs. If your operator supports
autograd, please use  with inputs with requires_grad  ;
if your operator supports multiple devices (e.g. CPU and CUDA), please
use  with inputs on all supported devices.

Parameters

================================================================================

# torch.library - Testing custom ops (Part 4)

 (OpOverloadOpOverloadPacketCustomOpDef) – The operator. Must either be a function decorated with
torch.library.custom_op() or an OpOverload/OpOverloadPacket
found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)
 () – The args to the operator
 () – The kwargs to the operator
test_utils () – Tests that we should run. Default: all of them.
Example: (“test_schema”, “test_faketensor”)
raise_exception () – If we should raise an exception on the first
error. If False, we will return a dict with information
on if each test passed or not.
 () – Relative tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).
 () – Absolute tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).


Return type
[, ]



================================================================================

# torch.library - Testing custom ops (Part 5)


opcheck and torch.autograd.gradcheck() test different things;
opcheck tests if your usage of torch.library APIs is correct while
torch.autograd.gradcheck() tests if your autograd formula is
mathematically correct. Use both to test custom ops that support
gradient computation.


"mylib::numpy_mul" mutates_args
      
      
        
     from_numpy

@numpy_mulregister_fake
  
     empty_like

 setup_context  
      
      

  
        

register_autograd setup_contextsetup_context

sample_inputs  
     
       
      requires_grad 
       requires_grad 


   sample_inputs

Given an operator and some sample arguments, tests if the operator is
registered correctly.

That is, when you use the torch.library/TORCH_LIBRARY APIs to create a
custom op, you specified metadata (e.g. mutability info) about the custom op
and these APIs require that the functions you pass them satisfy certain
properties (e.g. no data pointer access in the fake/meta/abstract kernel)
 tests these metadata and properties.

Concretely, we test the following:

================================================================================

# torch.library - Testing custom ops (Part 6)

List:
test_schema: If the schema matches the implementation of
the operator. For example: if the schema specifies a Tensor is mutated,
then we check the implementation mutates the Tensor. If the schema
specifies that we return a new Tensor, then we check that the
implementation returns a new Tensor (instead of an existing one or
a view of an existing one).
test_autograd_registration: If the operator supports training
(autograd): we check that its autograd formula is registered via
torch.library.register_autograd or a manual registration to one
or more DispatchKey::Autograd keys. Any other DispatchKey-based
registrations may lead to undefined behavior.
test_faketensor: If the operator has a FakeTensor kernel
(and if it is correct). The FakeTensor kernel is necessary (
but not sufficient) for the operator to work with PyTorch compilation
APIs (torch.compile/export/FX). We check that a FakeTensor kernel
(also sometimes known as a meta kernel) was registered for the
operator and that it is correct. This test takes the result of
running the operator on real tensors and the result of running
the operator on FakeTensors and checks that they have the same
Tensor metadata (sizes/strides/dtype/device/etc).
test_aot_dispatch_dynamic: If the operator has correct behavior
with PyTorch compilation APIs (torch.compile/export/FX).
This checks that the outputs (and gradients, if applicable) are the
same under eager-mode PyTorch and torch.compile.
This test is a superset of test_faketensor and is an e2e test;
other things it tests are that the operator supports
functionalization and that the backward pass (if it exists) also
supports FakeTensor and functionalization.

================================================================================

# torch.library - Testing custom ops (Part 7)

test_schema: If the schema matches the implementation of
the operator. For example: if the schema specifies a Tensor is mutated,
then we check the implementation mutates the Tensor. If the schema
specifies that we return a new Tensor, then we check that the
implementation returns a new Tensor (instead of an existing one or
a view of an existing one).

test_autograd_registration: If the operator supports training
(autograd): we check that its autograd formula is registered via
torch.library.register_autograd or a manual registration to one
or more DispatchKey::Autograd keys. Any other DispatchKey-based
registrations may lead to undefined behavior.

================================================================================

# torch.library - Testing custom ops (Part 8)

test_faketensor: If the operator has a FakeTensor kernel
(and if it is correct). The FakeTensor kernel is necessary (
but not sufficient) for the operator to work with PyTorch compilation
APIs (torch.compile/export/FX). We check that a FakeTensor kernel
(also sometimes known as a meta kernel) was registered for the
operator and that it is correct. This test takes the result of
running the operator on real tensors and the result of running
the operator on FakeTensors and checks that they have the same
Tensor metadata (sizes/strides/dtype/device/etc).

test_aot_dispatch_dynamic: If the operator has correct behavior
with PyTorch compilation APIs (torch.compile/export/FX).
This checks that the outputs (and gradients, if applicable) are the
same under eager-mode PyTorch and torch.compile.
This test is a superset of test_faketensor and is an e2e test;
other things it tests are that the operator supports
functionalization and that the backward pass (if it exists) also
supports FakeTensor and functionalization.

================================================================================

# torch.library - Testing custom ops (Part 9)

For best results, please call  multiple times with a
representative set of inputs. If your operator supports
autograd, please use  with inputs with requires_grad  ;
if your operator supports multiple devices (e.g. CPU and CUDA), please
use  with inputs on all supported devices.

Parameters

================================================================================

# torch.library - Testing custom ops (Part 10)

 (OpOverloadOpOverloadPacketCustomOpDef) – The operator. Must either be a function decorated with
torch.library.custom_op() or an OpOverload/OpOverloadPacket
found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)
 () – The args to the operator
 () – The kwargs to the operator
test_utils () – Tests that we should run. Default: all of them.
Example: (“test_schema”, “test_faketensor”)
raise_exception () – If we should raise an exception on the first
error. If False, we will return a dict with information
on if each test passed or not.
 () – Relative tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).
 () – Absolute tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).


Return type
[, ]

================================================================================

# torch.library - Testing custom ops (Part 11)

List:
(OpOverloadOpOverloadPacketCustomOpDef) – The operator. Must either be a function decorated with
torch.library.custom_op() or an OpOverload/OpOverloadPacket
found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)
 () – The args to the operator
 () – The kwargs to the operator
test_utils () – Tests that we should run. Default: all of them.
Example: (“test_schema”, “test_faketensor”)
raise_exception () – If we should raise an exception on the first
error. If False, we will return a dict with information
on if each test passed or not.
 () – Relative tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).
 () – Absolute tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).

(OpOverloadOpOverloadPacketCustomOpDef) – The operator. Must either be a function decorated with
torch.library.custom_op() or an OpOverload/OpOverloadPacket
found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)

================================================================================

# torch.library - Testing custom ops (Part 12)

() – The args to the operator

() – The kwargs to the operator

test_utils () – Tests that we should run. Default: all of them.
Example: (“test_schema”, “test_faketensor”)

raise_exception () – If we should raise an exception on the first
error. If False, we will return a dict with information
on if each test passed or not.

() – Relative tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).

() – Absolute tolerance for floating point comparisons.
If specified  must also be specified.
If omitted, default values based on the  are selected
(see the table in torch.testing.assert_close()).

opcheck and torch.autograd.gradcheck() test different things;
opcheck tests if your usage of torch.library APIs is correct while
torch.autograd.gradcheck() tests if your autograd formula is
mathematically correct. Use both to test custom ops that support
gradient computation.

Code example:
"mylib::numpy_mul" mutates_args
      
      
        
     from_numpy

@numpy_mulregister_fake
  
     empty_like

 setup_context  
      
      

  
        

register_autograd setup_contextsetup_context

================================================================================

# torch.library - Testing custom ops (Part 13)

sample_inputs  
     
       
      requires_grad 
       requires_grad 


   sample_inputs

================================================================================

# torch.library - Creating new custom ops in Python (Part 1)

Use torch.library.custom_op() to create new custom ops.

torch.library., , , , mutates_args, device_types, , 
Wraps a function into custom operator.
Reasons why you may want to create a custom op include:
- Wrapping a third-party library or custom kernel to work with PyTorch
subsystems like Autograd.
- Preventing torch.compile/export/FX tracing from peeking inside your function.
This API is used as a decorator around a function (please see examples).
The provided function must have type hints; these are needed to interface
with PyTorch’s various subsystems.

Parameters

================================================================================

# torch.library - Creating new custom ops in Python (Part 2)

 () – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.
device_types () – The device type(s) the function
is valid for. If no device type is provided, then the function
is used as the default implementation for all device types.
Examples: “cpu”, “cuda”.
When registering a device-specific implementation for an operator that accepts no Tensors,
we require the operator to have a “device: torch.device argument”.
 () – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.

================================================================================

# torch.library - Creating new custom ops in Python (Part 3)


Return type
[[[[[…], ]], CustomOpDef], CustomOpDef]




We recommend not passing in a  arg and instead letting us infer
it from the type annotations. It is error-prone to write your own schema.
You may wish to provide your own schema if our interpretation of
the type annotation is not what you want.
For more info on how to write a schema string, see



Examples:: 
   
 torch.library  
   

@custom_op"mylib::numpy_sin" mutates_args
    
      
      
     from_numpy

  
  
  

# Example of a custom op that only works for one device type.
@custom_op"mylib::numpy_sin_cpu" mutates_args device_types
 numpy_sin_cpu   
      
      
     from_numpy

  
  numpy_sin_cpu
  

# Example of a custom op that mutates an input
@custom_op"mylib::numpy_sin_inplace" mutates_args device_types
 numpy_sin_inplace   
      
     

  
  
numpy_sin_inplace
  

# Example of a factory function
"mylib::bar" mutates_args device_types

Wraps a function into custom operator.

Reasons why you may want to create a custom op include:
- Wrapping a third-party library or custom kernel to work with PyTorch
subsystems like Autograd.
- Preventing torch.compile/export/FX tracing from peeking inside your function.

================================================================================

# torch.library - Creating new custom ops in Python (Part 4)

This API is used as a decorator around a function (please see examples).
The provided function must have type hints; these are needed to interface
with PyTorch’s various subsystems.

Parameters

================================================================================

# torch.library - Creating new custom ops in Python (Part 5)

 () – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.
device_types () – The device type(s) the function
is valid for. If no device type is provided, then the function
is used as the default implementation for all device types.
Examples: “cpu”, “cuda”.
When registering a device-specific implementation for an operator that accepts no Tensors,
we require the operator to have a “device: torch.device argument”.
 () – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.

================================================================================

# torch.library - Creating new custom ops in Python (Part 6)


Return type
[[[[[…], ]], CustomOpDef], CustomOpDef]

================================================================================

# torch.library - Creating new custom ops in Python (Part 7)

List:
() – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.
device_types () – The device type(s) the function
is valid for. If no device type is provided, then the function
is used as the default implementation for all device types.
Examples: “cpu”, “cuda”.
When registering a device-specific implementation for an operator that accepts no Tensors,
we require the operator to have a “device: torch.device argument”.
 () – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.

================================================================================

# torch.library - Creating new custom ops in Python (Part 8)

() – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.

mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.

device_types () – The device type(s) the function
is valid for. If no device type is provided, then the function
is used as the default implementation for all device types.
Examples: “cpu”, “cuda”.
When registering a device-specific implementation for an operator that accepts no Tensors,
we require the operator to have a “device: torch.device argument”.

() – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.

================================================================================

# torch.library - Creating new custom ops in Python (Part 9)

[[[[[…], ]], CustomOpDef], CustomOpDef]

We recommend not passing in a  arg and instead letting us infer
it from the type annotations. It is error-prone to write your own schema.
You may wish to provide your own schema if our interpretation of
the type annotation is not what you want.
For more info on how to write a schema string, see

Examples:: 
   
 torch.library  
   

@custom_op"mylib::numpy_sin" mutates_args
    
      
      
     from_numpy

  
  
  

# Example of a custom op that only works for one device type.
@custom_op"mylib::numpy_sin_cpu" mutates_args device_types
 numpy_sin_cpu   
      
      
     from_numpy

  
  numpy_sin_cpu
  

# Example of a custom op that mutates an input
@custom_op"mylib::numpy_sin_inplace" mutates_args device_types
 numpy_sin_inplace   
      
     

  
  
numpy_sin_inplace
  

# Example of a factory function
"mylib::bar" mutates_args device_types

Code example:
torch.library  
   

@custom_op"mylib::numpy_sin" mutates_args
    
      
      
     from_numpy

  
  
  

# Example of a custom op that only works for one device type.
@custom_op"mylib::numpy_sin_cpu" mutates_args device_types
 numpy_sin_cpu   
      
      
     from_numpy

  
  numpy_sin_cpu
  

================================================================================

# torch.library - Creating new custom ops in Python (Part 10)

# Example of a custom op that mutates an input
@custom_op"mylib::numpy_sin_inplace" mutates_args device_types
 numpy_sin_inplace   
      
     

  
  
numpy_sin_inplace
  

# Example of a factory function
"mylib::bar" mutates_args device_types

================================================================================

# torch.library - Creating new custom ops in Python (Part 11)

torch.library., , , , mutates_args, 
Create a custom operator whose implementation is backed by 1+ triton kernels.
This is a more structured way of using triton kernels with PyTorch.
Prefer using triton kernels with no torch.library custom operator wrappers
(like torch.library.custom_op(), torch.library.triton_op()) because
that is simpler;
only use torch.library.custom_op()/torch.library.triton_op() if you
want to create an operator that behaves like PyTorch built-in operators.
For example, you may use a torch.library wrapper API to define the
behavior of the triton kernel when passed a tensor subclass or under
a TorchDispatchMode.
Use torch.library.triton_op() instead of torch.library.custom_op()
when the implementation
consists of 1+ triton kernels. torch.library.custom_op() treats
custom operators as opaque (torch.compile() and
torch.export.export() will never trace into them), but 
makes the implementation visible to these subsystems, allowing them
to optimize the triton kernel(s).
Note that  must only consist of calls to PyTorch-understood
operators and triton kernels. Any triton kernels called inside 
must be wrapped in a call to torch.library.wrap_triton().

Parameters

================================================================================

# torch.library - Creating new custom ops in Python (Part 12)

 () – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.
 () – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.


Return type




 
 torch.library   wrap_triton

 
     


 add_kernel
    
    
    
    n_elements
    BLOCK_SIZE "tl.constexpr"

      program_id
    block_start    BLOCK_SIZE
      block_start   BLOCK_SIZE
        n_elements
         
         
        
        

@triton_op"mylib::add" mutates_args
      
      empty_like
    n_elements  

     
         n_elements "BLOCK_SIZE"

================================================================================

# torch.library - Creating new custom ops in Python (Part 13)

    # NB: we need to wrap the triton kernel in a call to wrap_triton
    wrap_tritonadd_kernel   n_elements

Create a custom operator whose implementation is backed by 1+ triton kernels.

This is a more structured way of using triton kernels with PyTorch.
Prefer using triton kernels with no torch.library custom operator wrappers
(like torch.library.custom_op(), torch.library.triton_op()) because
that is simpler;
only use torch.library.custom_op()/torch.library.triton_op() if you
want to create an operator that behaves like PyTorch built-in operators.
For example, you may use a torch.library wrapper API to define the
behavior of the triton kernel when passed a tensor subclass or under
a TorchDispatchMode.

Use torch.library.triton_op() instead of torch.library.custom_op()
when the implementation
consists of 1+ triton kernels. torch.library.custom_op() treats
custom operators as opaque (torch.compile() and
torch.export.export() will never trace into them), but 
makes the implementation visible to these subsystems, allowing them
to optimize the triton kernel(s).

================================================================================

# torch.library - Creating new custom ops in Python (Part 14)

Note that  must only consist of calls to PyTorch-understood
operators and triton kernels. Any triton kernels called inside 
must be wrapped in a call to torch.library.wrap_triton().

Parameters

 () – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.
 () – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.


Return type

================================================================================

# torch.library - Creating new custom ops in Python (Part 15)

List:
() – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.
mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.
 () – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.

() – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace.

================================================================================

# torch.library - Creating new custom ops in Python (Part 16)

mutates_args () – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated.

() – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”.

Code example:
torch.library   wrap_triton

 
     


 add_kernel
    
    
    
    n_elements
    BLOCK_SIZE "tl.constexpr"

      program_id
    block_start    BLOCK_SIZE
      block_start   BLOCK_SIZE
        n_elements
         
         
        
        

@triton_op"mylib::add" mutates_args
      
      empty_like
    n_elements  

     
         n_elements "BLOCK_SIZE"

    # NB: we need to wrap the triton kernel in a call to wrap_triton
    wrap_tritonadd_kernel   n_elements

================================================================================

# torch.library - Creating new custom ops in Python (Part 17)

torch.library.wrap_tritontriton_kernel, 
Allows capture of a triton kernel into a graph via make_fx or
non-strict torch.export.
These technologies perform Dispatcher-based tracing (via
__torch_dispatch__) and cannot see calls to raw triton kernels.
The wrap_triton API wraps a triton kernel into a callable that
can actually be traced into a graph.
Please use this API together with torch.library.triton_op().

 
 
     
 torch.fx.experimental.proxy_tensor  
 torch.library  wrap_triton


 add_kernel
    
    
    
    n_elements
    BLOCK_SIZE "tl.constexpr"

      program_id
    block_start    BLOCK_SIZE
      block_start   BLOCK_SIZE
        n_elements
         
         
        
        

  
      empty_like
    n_elements  

     
         n_elements "BLOCK_SIZE"

    wrap_tritonadd_kernel   n_elements 
     

   
   
   

================================================================================

# torch.library - Creating new custom ops in Python (Part 18)

# def forward(self, x_1, y_1):
#     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)
#     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(
#         kernel_idx = 0, constant_args_idx = 0,
#         grid = [(1, 1, 1)], kwargs = {
#             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,
#             'n_elements': 3, 'BLOCK_SIZE': 16
#         })
#     return empty_like



Return type

Allows capture of a triton kernel into a graph via make_fx or
non-strict torch.export.

These technologies perform Dispatcher-based tracing (via
__torch_dispatch__) and cannot see calls to raw triton kernels.
The wrap_triton API wraps a triton kernel into a callable that
can actually be traced into a graph.

Please use this API together with torch.library.triton_op().

Code example:
torch.fx.experimental.proxy_tensor  
 torch.library  wrap_triton


 add_kernel
    
    
    
    n_elements
    BLOCK_SIZE "tl.constexpr"

      program_id
    block_start    BLOCK_SIZE
      block_start   BLOCK_SIZE
        n_elements
         
         
        
        

  
      empty_like
    n_elements  

     
         n_elements "BLOCK_SIZE"

================================================================================

# torch.library - Creating new custom ops in Python (Part 19)

    wrap_tritonadd_kernel   n_elements 
     

   
   
   

# def forward(self, x_1, y_1):
#     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)
#     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(
#         kernel_idx = 0, constant_args_idx = 0,
#         grid = [(1, 1, 1)], kwargs = {
#             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,
#             'n_elements': 3, 'BLOCK_SIZE': 16
#         })
#     return empty_like

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 1)

Use the register.* methods, such as torch.library.register_kernel() and
torch.library.register_fake(), to add implementations
for any operators (they may have been created using torch.library.custom_op() or
via PyTorch’s C++ operator registration APIs).

torch.library.register_kernel, device_types, , , , 
Register an implementation for a device type for this operator.
Some valid device_types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”.
This API may be used as a decorator.

Parameters

 (OpOverload) – The operator to register an impl to.
device_types () – The device_types to register an impl to.
If None, we will register to all device types – please only use
this option if your implementation is truly device-type-agnostic.
 () – The function to register as the implementation for
the given device types.
 () – If provided, the lifetime of this registration




Examples:: 
   
 torch.library  
   

# Create a custom op that works on cpu
@custom_op"mylib::numpy_sin" mutates_args device_types
    
      
      
     from_numpy

# Add implementations for the cuda device
register_kernel"mylib::numpy_sin" 
 
      
      
     from_numpy

Register an implementation for a device type for this operator.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 2)

Some valid device_types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”.
This API may be used as a decorator.

Parameters

 (OpOverload) – The operator to register an impl to.
device_types () – The device_types to register an impl to.
If None, we will register to all device types – please only use
this option if your implementation is truly device-type-agnostic.
 () – The function to register as the implementation for
the given device types.
 () – If provided, the lifetime of this registration

List:
(OpOverload) – The operator to register an impl to.
device_types () – The device_types to register an impl to.
If None, we will register to all device types – please only use
this option if your implementation is truly device-type-agnostic.
 () – The function to register as the implementation for
the given device types.
 () – If provided, the lifetime of this registration

(OpOverload) – The operator to register an impl to.

device_types () – The device_types to register an impl to.
If None, we will register to all device types – please only use
this option if your implementation is truly device-type-agnostic.

() – The function to register as the implementation for
the given device types.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 3)

() – If provided, the lifetime of this registration

Examples:: 
   
 torch.library  
   

# Create a custom op that works on cpu
@custom_op"mylib::numpy_sin" mutates_args device_types
    
      
      
     from_numpy

# Add implementations for the cuda device
register_kernel"mylib::numpy_sin" 
 
      
      
     from_numpy

Code example:
torch.library  
   

# Create a custom op that works on cpu
@custom_op"mylib::numpy_sin" mutates_args device_types
    
      
      
     from_numpy

# Add implementations for the cuda device
register_kernel"mylib::numpy_sin" 
 
      
      
     from_numpy

torch.library.register_autocast, device_type, cast_inputs, , , 
Register an autocast dispatch rule for this custom op.
Valid device_type include: “cpu” and “cuda”.

Parameters

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 4)

 (OpOverload) – The operator to register an autocast dispatch rule to.
device_type () – Device type to use. ‘cuda’ or ‘cpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
cast_inputs (torch.dtype) – When custom op runs in an autocast-enabled region,
casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors
are not affected), then executes custom op with autocast disabled.
 () – If provided, the lifetime of this registration




Examples:: 
   
 torch.library  

# Create a custom op that works on cuda
"mylib::my_sin" mutates_args
    
     

# Register autocast dispatch rule for the cuda device
register_autocast"mylib::my_sin"

Register an autocast dispatch rule for this custom op.

Valid device_type include: “cpu” and “cuda”.

Parameters

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 5)

 (OpOverload) – The operator to register an autocast dispatch rule to.
device_type () – Device type to use. ‘cuda’ or ‘cpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
cast_inputs (torch.dtype) – When custom op runs in an autocast-enabled region,
casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors
are not affected), then executes custom op with autocast disabled.
 () – If provided, the lifetime of this registration

List:
(OpOverload) – The operator to register an autocast dispatch rule to.
device_type () – Device type to use. ‘cuda’ or ‘cpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.
cast_inputs (torch.dtype) – When custom op runs in an autocast-enabled region,
casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors
are not affected), then executes custom op with autocast disabled.
 () – If provided, the lifetime of this registration

(OpOverload) – The operator to register an autocast dispatch rule to.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 6)

device_type () – Device type to use. ‘cuda’ or ‘cpu’.
The type is the same as the  attribute of a torch.device.
Thus, you may obtain the device type of a tensor using Tensor.device.type.

cast_inputs (torch.dtype) – When custom op runs in an autocast-enabled region,
casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors
are not affected), then executes custom op with autocast disabled.

() – If provided, the lifetime of this registration

Examples:: 
   
 torch.library  

# Create a custom op that works on cuda
"mylib::my_sin" mutates_args
    
     

# Register autocast dispatch rule for the cuda device
register_autocast"mylib::my_sin"

Code example:
torch.library  

# Create a custom op that works on cuda
"mylib::my_sin" mutates_args
    
     

# Register autocast dispatch rule for the cuda device
register_autocast"mylib::my_sin"

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 7)

torch.library.register_autograd, , , , setup_context, 
Register a backward formula for this custom op.
In order for an operator to work with autograd, you need to register
a backward formula:
1. You must tell us how to compute gradients during the backward pass
by providing us a “backward” function.
2. If you need any values from the forward to compute gradients, you can
use setup_context to save values for backward.
 runs during the backward pass. It accepts  :
-  is one or more gradients. The number of gradients matches
the number of outputs of the operator.
The  object is the same ctx object used by
torch.autograd.Function. The semantics of backward_fn are the
same as torch.autograd.Function.backward().
setup_context(ctx,   runs during the forward pass.
Please save quantities needed for backward onto the  object via
either torch.autograd.function.FunctionCtx.save_for_backward()
or assigning them as attributes of . If your custom op has
kwarg-only arguments, we expect the signature of setup_context
to be setup_context(ctx,  keyword_only_inputs, .
Both setup_context_fn and backward_fn must be traceable. That is,
they may not directly access torch.Tensor.data_ptr() and they must
not depend on or mutate global state. If you need a non-traceable backward,
you can make it a separate custom_op that you call inside backward_fn.
If you need different autograd behavior on different devices, then we
recommend creating two different custom operators, one for each device
that needs different behavior, and switching between them at runtime.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 8)

 
   
   

"mylib::numpy_sin" mutates_args
    
      
      
     from_numpy

 setup_context    
      
    save_for_backward

  
      saved_tensors
       

register_autograd
    "mylib::numpy_sin"  setup_contextsetup_context


   requires_grad
  
    
  

# Example with a keyword-only arg
"mylib::numpy_mul" mutates_args
       
      
        
     from_numpy

 setup_context  keyword_only_inputs   
      keyword_only_inputs

  
       

register_autograd
    "mylib::numpy_mul"  setup_contextsetup_context


   requires_grad

Register a backward formula for this custom op.

In order for an operator to work with autograd, you need to register
a backward formula:
1. You must tell us how to compute gradients during the backward pass
by providing us a “backward” function.
2. If you need any values from the forward to compute gradients, you can
use setup_context to save values for backward.

runs during the backward pass. It accepts  :
-  is one or more gradients. The number of gradients matches
the number of outputs of the operator.
The  object is the same ctx object used by
torch.autograd.Function. The semantics of backward_fn are the
same as torch.autograd.Function.backward().

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 9)

setup_context(ctx,   runs during the forward pass.
Please save quantities needed for backward onto the  object via
either torch.autograd.function.FunctionCtx.save_for_backward()
or assigning them as attributes of . If your custom op has
kwarg-only arguments, we expect the signature of setup_context
to be setup_context(ctx,  keyword_only_inputs, .

Both setup_context_fn and backward_fn must be traceable. That is,
they may not directly access torch.Tensor.data_ptr() and they must
not depend on or mutate global state. If you need a non-traceable backward,
you can make it a separate custom_op that you call inside backward_fn.

If you need different autograd behavior on different devices, then we
recommend creating two different custom operators, one for each device
that needs different behavior, and switching between them at runtime.

Code example:
"mylib::numpy_sin" mutates_args
    
      
      
     from_numpy

 setup_context    
      
    save_for_backward

  
      saved_tensors
       

register_autograd
    "mylib::numpy_sin"  setup_contextsetup_context


   requires_grad
  
    
  

# Example with a keyword-only arg
"mylib::numpy_mul" mutates_args
       
      
        
     from_numpy

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 10)

 setup_context  keyword_only_inputs   
      keyword_only_inputs

  
       

register_autograd
    "mylib::numpy_mul"  setup_contextsetup_context


   requires_grad

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 11)

torch.library.register_fake, , , , , _stacklevel, allow_override
Register a FakeTensor implementation (“fake impl”) for this operator.
Also sometimes known as a “meta kernel”, “abstract impl”.
An “FakeTensor implementation” specifies the behavior of this operator on
Tensors that carry no data (“FakeTensor”). Given some input Tensors with
certain properties (sizes/strides/storage_offset/device), it specifies
what the properties of the output Tensors are.
The FakeTensor implementation has the same signature as the operator.
It is run for both FakeTensors and meta tensors. To write a FakeTensor
implementation, assume that all Tensor inputs to the operator are
regular CPU/CUDA/Meta tensors, but they do not have storage, and
you are trying to return regular CPU/CUDA/Meta tensor(s) as output.
The FakeTensor implementation must consist of only PyTorch operations
(and may not directly access the storage or data of any input or
intermediate Tensors).
This API may be used as a decorator (see examples).
For a detailed guide on custom ops, please see
https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html

Parameters

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 12)

 – Operator name (along with the overload) or OpOverload object.
 () – Fake tensor implementation.
 () – Library to register the fake tensor to.
allow_override () – Flag controlling if we want to override an
existing registered fake impl. This is by default off,
and will error you’re trying to register a fake impl to
an operator that already has a fake impl. This also only
applies if the custom operator was not created via
torch.library.custom_op, as overriding and existing fake
impl is already allowed.




 
   
   

# Example 1: an operator without data-dependent output shape
"mylib::custom_linear" mutates_args
 custom_linear       
     NotImplementedError"Implementation goes here"

register_fake"mylib::custom_linear"
   
       
       
       
       
       
       

         

 _subclassesfake_tensorFakeTensorMode
       
       
      
      custom_linear  

    

# Example 2: an operator with data-dependent output shape
"mylib::custom_nonzero" mutates_args
 custom_nonzero   
      
       
      

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 13)

register_fake"mylib::custom_nonzero"
 
# Number of nonzero-elements is data-dependent.
# Since we cannot peek at the data in an fake impl,
# we use the ctx object to construct a new symint that
# represents the data-dependent size.
      
      new_dynamic_size
       
       
     

 torch.fx.experimental.proxy_tensor  

       
  custom_nonzero tracing_mode"symbolic"
print_readable

  custom_nonzero

Register a FakeTensor implementation (“fake impl”) for this operator.

Also sometimes known as a “meta kernel”, “abstract impl”.

An “FakeTensor implementation” specifies the behavior of this operator on
Tensors that carry no data (“FakeTensor”). Given some input Tensors with
certain properties (sizes/strides/storage_offset/device), it specifies
what the properties of the output Tensors are.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 14)

The FakeTensor implementation has the same signature as the operator.
It is run for both FakeTensors and meta tensors. To write a FakeTensor
implementation, assume that all Tensor inputs to the operator are
regular CPU/CUDA/Meta tensors, but they do not have storage, and
you are trying to return regular CPU/CUDA/Meta tensor(s) as output.
The FakeTensor implementation must consist of only PyTorch operations
(and may not directly access the storage or data of any input or
intermediate Tensors).

This API may be used as a decorator (see examples).

For a detailed guide on custom ops, please see
https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html

Parameters

 – Operator name (along with the overload) or OpOverload object.
 () – Fake tensor implementation.
 () – Library to register the fake tensor to.
allow_override () – Flag controlling if we want to override an
existing registered fake impl. This is by default off,
and will error you’re trying to register a fake impl to
an operator that already has a fake impl. This also only
applies if the custom operator was not created via
torch.library.custom_op, as overriding and existing fake
impl is already allowed.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 15)

List:
– Operator name (along with the overload) or OpOverload object.
 () – Fake tensor implementation.
 () – Library to register the fake tensor to.
allow_override () – Flag controlling if we want to override an
existing registered fake impl. This is by default off,
and will error you’re trying to register a fake impl to
an operator that already has a fake impl. This also only
applies if the custom operator was not created via
torch.library.custom_op, as overriding and existing fake
impl is already allowed.

– Operator name (along with the overload) or OpOverload object.

() – Fake tensor implementation.

() – Library to register the fake tensor to.

allow_override () – Flag controlling if we want to override an
existing registered fake impl. This is by default off,
and will error you’re trying to register a fake impl to
an operator that already has a fake impl. This also only
applies if the custom operator was not created via
torch.library.custom_op, as overriding and existing fake
impl is already allowed.

Code example:
# Example 1: an operator without data-dependent output shape
"mylib::custom_linear" mutates_args
 custom_linear       
     NotImplementedError"Implementation goes here"

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 16)

register_fake"mylib::custom_linear"
   
       
       
       
       
       
       

         

 _subclassesfake_tensorFakeTensorMode
       
       
      
      custom_linear  

    

# Example 2: an operator with data-dependent output shape
"mylib::custom_nonzero" mutates_args
 custom_nonzero   
      
       
      

register_fake"mylib::custom_nonzero"
 
# Number of nonzero-elements is data-dependent.
# Since we cannot peek at the data in an fake impl,
# we use the ctx object to construct a new symint that
# represents the data-dependent size.
      
      new_dynamic_size
       
       
     

 torch.fx.experimental.proxy_tensor  

       
  custom_nonzero tracing_mode"symbolic"
print_readable

  custom_nonzero

torch.library.register_vmap, , , , 
Register a vmap implementation to support torch.vmap() for this custom op.
This API may be used as a decorator (see examples).
In order for an operator to work with torch.vmap(), you may need to register a
vmap implementation in the following signature:

vmap_func(info,  Tuple[Optional[int]],  ,

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 17)

where  and  are the arguments and kwargs for .
We do not support kwarg-only Tensor args.
It specifies how do we compute the batched version of  given inputs with an additional
dimension (specified by ).
For each arg in ,  has a corresponding Optional[int]. It is 
if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.
 is a collection of additional metadata that may be helpful:
info.batch_size specifies the size of the dimension being vmapped over, while
info.randomness is the randomness option that was passed to torch.vmap().
The return of the function  is a tuple of  . Similar to ,
 should be of the same structure as  and contain one 
per output that specifies if the output has the vmapped dimension and what index it is in.

 
   
   
   

 
     

   "FRAGMENT"
"mylib::numpy_cube" mutates_args
 numpy_cube    
      
           
         

 numpy_cube_vmap  
      numpy_cube
       

register_vmapnumpy_cube numpy_cube_vmap

  
numpy_cube

"mylib::numpy_mul" mutates_args
      
        

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 18)

register_vmap"mylib::numpy_mul"
 numpy_mul_vmap   
       
              
              
        
       
      


  
  
 




The vmap function should aim to preserve the semantics of the entire custom operator.
That is, grad(vmap(op)) should be replaceable with a grad(map(op)).
If your custom operator has any custom behavior in the backward pass, please
keep this in mind.

Register a vmap implementation to support torch.vmap() for this custom op.

This API may be used as a decorator (see examples).

In order for an operator to work with torch.vmap(), you may need to register a
vmap implementation in the following signature:

Quote:
vmap_func(info,  Tuple[Optional[int]],  ,

vmap_func(info,  Tuple[Optional[int]],  ,

where  and  are the arguments and kwargs for .
We do not support kwarg-only Tensor args.

It specifies how do we compute the batched version of  given inputs with an additional
dimension (specified by ).

For each arg in ,  has a corresponding Optional[int]. It is 
if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 19)

is a collection of additional metadata that may be helpful:
info.batch_size specifies the size of the dimension being vmapped over, while
info.randomness is the randomness option that was passed to torch.vmap().

The return of the function  is a tuple of  . Similar to ,
 should be of the same structure as  and contain one 
per output that specifies if the output has the vmapped dimension and what index it is in.

Code example:
"FRAGMENT"
"mylib::numpy_cube" mutates_args
 numpy_cube    
      
           
         

 numpy_cube_vmap  
      numpy_cube
       

register_vmapnumpy_cube numpy_cube_vmap

  
numpy_cube

"mylib::numpy_mul" mutates_args
      
        

register_vmap"mylib::numpy_mul"
 numpy_mul_vmap

The vmap function should aim to preserve the semantics of the entire custom operator.
That is, grad(vmap(op)) should be replaceable with a grad(map(op)).

If your custom operator has any custom behavior in the backward pass, please
keep this in mind.

torch.library.impl_abstract, , , , _stacklevel
This API was renamed to torch.library.register_fake() in PyTorch 2.4.
Please use that instead.

This API was renamed to torch.library.register_fake() in PyTorch 2.4.
Please use that instead.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 20)

torch.library.
get_ctx() returns the current AbstractImplCtx object.
Calling  is only valid inside of an fake impl
(see torch.library.register_fake() for more usage details.

Return type
FakeImplCtx

get_ctx() returns the current AbstractImplCtx object.

Calling  is only valid inside of an fake impl
(see torch.library.register_fake() for more usage details.

Return type
FakeImplCtx

torch.library.register_torch_dispatch, torch_dispatch_class, , , , 
Registers a torch_dispatch rule for the given operator and torch_dispatch_class.
This allows for open registration to specify the behavior between the operator
and the torch_dispatch_class without needing to modify the torch_dispatch_class
or the operator directly.
The torch_dispatch_class is either a Tensor subclass with __torch_dispatch__ or a
TorchDispatchMode.
If it is a Tensor subclass, we expect  to have the following signature:
  OpOverload,  Tuple[type,     
If it is a TorchDispatchMode, we expect  to have the following signature:
  OpOverload,  Tuple[type,     
 and  will have been normalized the same way they are
in __torch_dispatch__ (see __torch_dispatch__ calling convention).

 

"mylib::foo" mutates_args
    
     

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 21)

 _python_dispatchTorchDispatchMode
     __torch_dispatch__    
          

register_torch_dispatch"mylib::foo"

Registers a torch_dispatch rule for the given operator and torch_dispatch_class.

This allows for open registration to specify the behavior between the operator
and the torch_dispatch_class without needing to modify the torch_dispatch_class
or the operator directly.

The torch_dispatch_class is either a Tensor subclass with __torch_dispatch__ or a
TorchDispatchMode.

If it is a Tensor subclass, we expect  to have the following signature:
  OpOverload,  Tuple[type,

If it is a TorchDispatchMode, we expect  to have the following signature:
  OpOverload,  Tuple[type,

and  will have been normalized the same way they are
in __torch_dispatch__ (see __torch_dispatch__ calling convention).

Code example:
"mylib::foo" mutates_args
    
     

 _python_dispatchTorchDispatchMode
     __torch_dispatch__    
          

register_torch_dispatch"mylib::foo"

torch.library.infer_schemaprototype_function, , , mutates_args, 
Parses the schema of a given function with type hints. The schema is inferred from the
function’s type hints, and can be used to define a new operator.
We make the following assumptions:

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 22)

None of the outputs alias any of the inputs or each other.

String type annotations “device, dtype, Tensor, types” without library specification are
assumed to be torch.*. Similarly, string type annotations “Optional, List, Sequence, Union”
without library specification are assumed to be typing.*.



Only the args listed in mutates_args are being mutated. If mutates_args is “unknown”,
it assumes that all inputs to the operator are being mutates.



Callers (e.g. the custom ops API) are responsible for checking these assumptions.

Parameters

prototype_function () – The function from which to infer a schema for from its type annotations.
 () – The name of the operator in the schema. If  is None, then the
name is not included in the inferred schema. Note that the input schema to
torch.library.Library.define requires a operator name.
mutates_args () – The arguments that are mutated in the function.



The inferred schema.

Return type




    
     

infer_schema  mutates_args
foo(Tensor x) -> Tensor

infer_schema mutates_args
(Tensor x) -> Tensor

Parses the schema of a given function with type hints. The schema is inferred from the
function’s type hints, and can be used to define a new operator.

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 23)

We make the following assumptions:

List:
None of the outputs alias any of the inputs or each other.

String type annotations “device, dtype, Tensor, types” without library specification are
assumed to be torch.*. Similarly, string type annotations “Optional, List, Sequence, Union”
without library specification are assumed to be typing.*.



Only the args listed in mutates_args are being mutated. If mutates_args is “unknown”,
it assumes that all inputs to the operator are being mutates.

None of the outputs alias any of the inputs or each other.

Callers (e.g. the custom ops API) are responsible for checking these assumptions.

Parameters

prototype_function () – The function from which to infer a schema for from its type annotations.
 () – The name of the operator in the schema. If  is None, then the
name is not included in the inferred schema. Note that the input schema to
torch.library.Library.define requires a operator name.
mutates_args () – The arguments that are mutated in the function.



The inferred schema.

Return type

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 24)

List:
prototype_function () – The function from which to infer a schema for from its type annotations.
 () – The name of the operator in the schema. If  is None, then the
name is not included in the inferred schema. Note that the input schema to
torch.library.Library.define requires a operator name.
mutates_args () – The arguments that are mutated in the function.

prototype_function () – The function from which to infer a schema for from its type annotations.

() – The name of the operator in the schema. If  is None, then the
name is not included in the inferred schema. Note that the input schema to
torch.library.Library.define requires a operator name.

mutates_args () – The arguments that are mutated in the function.

Code example:
infer_schema  mutates_args
foo(Tensor x) -> Tensor

infer_schema mutates_args
(Tensor x) -> Tensor

torch._library.custom_ops.CustomOpDef, , , , 
CustomOpDef is a wrapper around a function that turns it into a custom op.
It has various methods for registering additional behavior for this
custom op.
You should not instantiate CustomOpDef directly; instead, use the
torch.library.custom_op() API.



================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 25)

set_kernel_enableddevice_type, 
Disable or re-enable an already registered kernel for this custom operator.
If the kernel is already disabled/enabled, this is a no-op.


If a kernel is first disabled and then registered, it is disabled until enabled again.


Parameters

device_type () – The device type to disable/enable the kernel for.
 () – Whether to disable or enable the kernel.




  

# define custom op `f`.
@custom_op"mylib::f" mutates_args
    
     

  # tensor([0.]), default kernel

register_kernel
 
     

  # tensor([1.]), CPU kernel

# temporarily disable the CPU kernel
 set_kernel_enabled   
      # tensor([0.]) with CPU kernel disabled

CustomOpDef is a wrapper around a function that turns it into a custom op.

It has various methods for registering additional behavior for this
custom op.

You should not instantiate CustomOpDef directly; instead, use the
torch.library.custom_op() API.

set_kernel_enableddevice_type, 
Disable or re-enable an already registered kernel for this custom operator.
If the kernel is already disabled/enabled, this is a no-op.


If a kernel is first disabled and then registered, it is disabled until enabled again.


Parameters

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 26)

device_type () – The device type to disable/enable the kernel for.
 () – Whether to disable or enable the kernel.




  

# define custom op `f`.
@custom_op"mylib::f" mutates_args
    
     

  # tensor([0.]), default kernel

register_kernel
 
     

  # tensor([1.]), CPU kernel

# temporarily disable the CPU kernel
 set_kernel_enabled   
      # tensor([0.]) with CPU kernel disabled

Disable or re-enable an already registered kernel for this custom operator.

If the kernel is already disabled/enabled, this is a no-op.

If a kernel is first disabled and then registered, it is disabled until enabled again.

Parameters

device_type () – The device type to disable/enable the kernel for.
 () – Whether to disable or enable the kernel.

List:
device_type () – The device type to disable/enable the kernel for.
 () – Whether to disable or enable the kernel.

device_type () – The device type to disable/enable the kernel for.

() – Whether to disable or enable the kernel.

Code example:
# define custom op `f`.
@custom_op"mylib::f" mutates_args
    
     

  # tensor([0.]), default kernel

register_kernel
 
     

  # tensor([1.]), CPU kernel

================================================================================

# torch.library - Extending custom ops (created from Python or C++) (Part 27)

# temporarily disable the CPU kernel
 set_kernel_enabled   
      # tensor([0.]) with CPU kernel disabled

================================================================================

# torch.library - Low-level APIs (Part 1)

The following APIs are direct bindings to PyTorch’s C++ low-level
operator registration APIs.

The low-level operator registration APIs and the PyTorch Dispatcher are a
complicated PyTorch concept. We recommend you use the higher level APIs above
(that do not require a torch.library.Library object) when possible.
This blog post
is a good starting point to learn about the PyTorch Dispatcher.

A tutorial that walks you through some examples on how to use this API is available on Google Colab.

torch.library., , dispatch_key
A class to create libraries that can be used to register new operators or
override operators in existing libraries from Python.
A user can optionally pass in a dispatch keyname if they only want to register
kernels corresponding to only one specific dispatch key.
To create a library to override operators in an existing library (with name ns), set the kind to “IMPL”.
To create a new library (with name ns) to register new operators, set the kind to “DEF”.
To create a fragment of a possibly existing library to register operators (and bypass
the limitation that there is only one library for a given namespace), set the kind to
“FRAGMENT”.

Parameters

================================================================================

# torch.library - Low-level APIs (Part 2)

 – library name
 – “DEF”, “IMPL”, “FRAGMENT”
dispatch_key – PyTorch dispatch key (default: “”)





, alias_analysis, , 
Defines a new operator and its semantics in the ns namespace.

Parameters

 – function schema to define a new operator.
alias_analysis () – Indicates if the aliasing properties of the operator arguments can be
inferred from the schema (default behavior) or not (“CONSERVATIVE”).
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.



name of the operator as inferred from the schema.



   
"sum(Tensor self) -> Tensor"





, dispatch_key, , with_keyset
Registers the function implementation as the fallback for the given key.
This function only works for a library with global namespace (“_”).

Parameters

================================================================================

# torch.library - Low-level APIs (Part 3)

 – function used as fallback for the given dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.




   
 fallback_kernel  
    # Handle all autocast ops generically
    
fallback_kernel "Autocast"





, , dispatch_key, , with_keyset, allow_override
Registers the function implementation for an operator defined in the library.

Parameters

================================================================================

# torch.library - Low-level APIs (Part 4)

 – operator name (along with the overload) or OpOverload object.
 – function that’s the operator implementation for the input dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.
allow_override – Flag controlling if we want to override an
existing registered kernel implementation. This is by
default off, and will error you’re trying to register a
kernel to a dispatch key with a kernel already
registered.




   
  
         
"div.Tensor"

A class to create libraries that can be used to register new operators or
override operators in existing libraries from Python.
A user can optionally pass in a dispatch keyname if they only want to register
kernels corresponding to only one specific dispatch key.

================================================================================

# torch.library - Low-level APIs (Part 5)

To create a library to override operators in an existing library (with name ns), set the kind to “IMPL”.
To create a new library (with name ns) to register new operators, set the kind to “DEF”.
To create a fragment of a possibly existing library to register operators (and bypass
the limitation that there is only one library for a given namespace), set the kind to
“FRAGMENT”.

Parameters

 – library name
 – “DEF”, “IMPL”, “FRAGMENT”
dispatch_key – PyTorch dispatch key (default: “”)

List:
– library name
 – “DEF”, “IMPL”, “FRAGMENT”
dispatch_key – PyTorch dispatch key (default: “”)

– “DEF”, “IMPL”, “FRAGMENT”

dispatch_key – PyTorch dispatch key (default: “”)

, alias_analysis, , 
Defines a new operator and its semantics in the ns namespace.

Parameters

 – function schema to define a new operator.
alias_analysis () – Indicates if the aliasing properties of the operator arguments can be
inferred from the schema (default behavior) or not (“CONSERVATIVE”).
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.



================================================================================

# torch.library - Low-level APIs (Part 6)

name of the operator as inferred from the schema.



   
"sum(Tensor self) -> Tensor"

Defines a new operator and its semantics in the ns namespace.

Parameters

 – function schema to define a new operator.
alias_analysis () – Indicates if the aliasing properties of the operator arguments can be
inferred from the schema (default behavior) or not (“CONSERVATIVE”).
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.



name of the operator as inferred from the schema.

List:
– function schema to define a new operator.
alias_analysis () – Indicates if the aliasing properties of the operator arguments can be
inferred from the schema (default behavior) or not (“CONSERVATIVE”).
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.

– function schema to define a new operator.

================================================================================

# torch.library - Low-level APIs (Part 7)

alias_analysis () – Indicates if the aliasing properties of the operator arguments can be
inferred from the schema (default behavior) or not (“CONSERVATIVE”).

() – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.

name of the operator as inferred from the schema.

Code example:
"sum(Tensor self) -> Tensor"

, dispatch_key, , with_keyset
Registers the function implementation as the fallback for the given key.
This function only works for a library with global namespace (“_”).

Parameters

 – function used as fallback for the given dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.




   
 fallback_kernel  
    # Handle all autocast ops generically
    
fallback_kernel "Autocast"

================================================================================

# torch.library - Low-level APIs (Part 8)

Registers the function implementation as the fallback for the given key.

This function only works for a library with global namespace (“_”).

Parameters

 – function used as fallback for the given dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.

List:
– function used as fallback for the given dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.

– function used as fallback for the given dispatch key or fallthrough_kernel()
to register a fallthrough.

================================================================================

# torch.library - Low-level APIs (Part 9)

dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.

with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.

Code example:
fallback_kernel  
    # Handle all autocast ops generically
    
fallback_kernel "Autocast"

, , dispatch_key, , with_keyset, allow_override
Registers the function implementation for an operator defined in the library.

Parameters

================================================================================

# torch.library - Low-level APIs (Part 10)

 – operator name (along with the overload) or OpOverload object.
 – function that’s the operator implementation for the input dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.
allow_override – Flag controlling if we want to override an
existing registered kernel implementation. This is by
default off, and will error you’re trying to register a
kernel to a dispatch key with a kernel already
registered.




   
  
         
"div.Tensor"

Registers the function implementation for an operator defined in the library.

Parameters

================================================================================

# torch.library - Low-level APIs (Part 11)

 – operator name (along with the overload) or OpOverload object.
 – function that’s the operator implementation for the input dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.
allow_override – Flag controlling if we want to override an
existing registered kernel implementation. This is by
default off, and will error you’re trying to register a
kernel to a dispatch key with a kernel already
registered.

================================================================================

# torch.library - Low-level APIs (Part 12)

List:
– operator name (along with the overload) or OpOverload object.
 – function that’s the operator implementation for the input dispatch key or fallthrough_kernel()
to register a fallthrough.
dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.
with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.
allow_override – Flag controlling if we want to override an
existing registered kernel implementation. This is by
default off, and will error you’re trying to register a
kernel to a dispatch key with a kernel already
registered.

– operator name (along with the overload) or OpOverload object.

– function that’s the operator implementation for the input dispatch key or fallthrough_kernel()
to register a fallthrough.

dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with.

================================================================================

# torch.library - Low-level APIs (Part 13)

with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to  when calling. This should be used to create the appropriate keyset for redispatch calls.

allow_override – Flag controlling if we want to override an
existing registered kernel implementation. This is by
default off, and will error you’re trying to register a
kernel to a dispatch key with a kernel already
registered.

torch.library.fallthrough_kernel
A dummy function to pass to Library.impl in order to register a fallthrough.

A dummy function to pass to Library.impl in order to register a fallthrough.

torch.library., , , , 

torch.library., , alias_analysis
Defines a new operator.
In PyTorch, defining an op (short for “operator”) is a two step-process:
- we need to define the op (by providing an operator name and schema)
- we need to implement behavior for how the operator interacts with
various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.
This entrypoint defines the custom operator (the first step)
you must then perform the second step by calling various
 APIs, like torch.library.impl() or
torch.library.register_fake().

Parameters

================================================================================

# torch.library - Low-level APIs (Part 14)

 () – The qualified name for the operator. Should be
a string that looks like “namespace::name”, e.g. “aten::sin”.
Operators in PyTorch need a namespace to
avoid name collisions; a given operator may only be created once.
If you are writing a Python library, we recommend the namespace to
be the name of your top-level module.
 () – The schema of the operator. E.g. “(Tensor x) -> Tensor”
for an op that accepts one Tensor and returns one Tensor. It does
not contain the operator name (that is passed in ).
 () – If provided, the lifetime of this operator
will be tied to the lifetime of the Library object.
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.




 
   

# Define the operator
"mylib::sin" "(Tensor x) -> Tensor"

# Add implementations for the operator
"mylib::sin" 
 
     from_numpy

# Call the new operator from torch.ops.

Defines a new operator.

================================================================================

# torch.library - Low-level APIs (Part 15)

In PyTorch, defining an op (short for “operator”) is a two step-process:
- we need to define the op (by providing an operator name and schema)
- we need to implement behavior for how the operator interacts with
various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.

This entrypoint defines the custom operator (the first step)
you must then perform the second step by calling various
 APIs, like torch.library.impl() or
torch.library.register_fake().

Parameters

================================================================================

# torch.library - Low-level APIs (Part 16)

 () – The qualified name for the operator. Should be
a string that looks like “namespace::name”, e.g. “aten::sin”.
Operators in PyTorch need a namespace to
avoid name collisions; a given operator may only be created once.
If you are writing a Python library, we recommend the namespace to
be the name of your top-level module.
 () – The schema of the operator. E.g. “(Tensor x) -> Tensor”
for an op that accepts one Tensor and returns one Tensor. It does
not contain the operator name (that is passed in ).
 () – If provided, the lifetime of this operator
will be tied to the lifetime of the Library object.
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.

================================================================================

# torch.library - Low-level APIs (Part 17)

List:
() – The qualified name for the operator. Should be
a string that looks like “namespace::name”, e.g. “aten::sin”.
Operators in PyTorch need a namespace to
avoid name collisions; a given operator may only be created once.
If you are writing a Python library, we recommend the namespace to
be the name of your top-level module.
 () – The schema of the operator. E.g. “(Tensor x) -> Tensor”
for an op that accepts one Tensor and returns one Tensor. It does
not contain the operator name (that is passed in ).
 () – If provided, the lifetime of this operator
will be tied to the lifetime of the Library object.
 () – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.

() – The qualified name for the operator. Should be
a string that looks like “namespace::name”, e.g. “aten::sin”.
Operators in PyTorch need a namespace to
avoid name collisions; a given operator may only be created once.
If you are writing a Python library, we recommend the namespace to
be the name of your top-level module.

================================================================================

# torch.library - Low-level APIs (Part 18)

() – The schema of the operator. E.g. “(Tensor x) -> Tensor”
for an op that accepts one Tensor and returns one Tensor. It does
not contain the operator name (that is passed in ).

() – If provided, the lifetime of this operator
will be tied to the lifetime of the Library object.

() – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it.

# Define the operator
"mylib::sin" "(Tensor x) -> Tensor"

# Add implementations for the operator
"mylib::sin" 
 
     from_numpy

# Call the new operator from torch.ops.

Code example:
# Define the operator
"mylib::sin" "(Tensor x) -> Tensor"

# Add implementations for the operator
"mylib::sin" 
 
     from_numpy

# Call the new operator from torch.ops.

torch.library., , dispatch_key

torch.library., , , ,   

torch.library., , , ,   

================================================================================

# torch.library - Low-level APIs (Part 19)

torch.library., , dispatch_key  
Register an implementation for a device type for this operator.
You may pass “default” for  to register this implementation as the
default implementation for ALL device types.
Please only use this if the implementation truly supports all device types;
for example, this is true if it is a composition of built-in PyTorch operators.
This API may be used as a decorator. You can use nested decorators
with this API provided they return a function and are placed inside
this API (see Example 2).
Some valid types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”.

Parameters

 () – Should be a string that looks like “namespace::operator_name”.
 () – The device types to register an impl to.
 () – If provided, the lifetime of this registration
will be tied to the lifetime of the Library object.




 
   
# Example 1: Register function.
# Define the operator
"mylib::mysin" "(Tensor x) -> Tensor"

# Add implementations for the cpu device
"mylib::mysin" 
 
     from_numpy

  
  
  

# Example 2: Register function with decorator.
 custom_decorator
      
            
     

# Define the operator
"mylib::sin_plus_one" "(Tensor x) -> Tensor"

================================================================================

# torch.library - Low-level APIs (Part 20)

# Add implementations for the operator
"mylib::sin_plus_one" 
@custom_decorator
 
     from_numpy

# Call the new operator from torch.ops.
  

  sin_plus_one

Register an implementation for a device type for this operator.

You may pass “default” for  to register this implementation as the
default implementation for ALL device types.
Please only use this if the implementation truly supports all device types;
for example, this is true if it is a composition of built-in PyTorch operators.

This API may be used as a decorator. You can use nested decorators
with this API provided they return a function and are placed inside
this API (see Example 2).

Some valid types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”.

Parameters

 () – Should be a string that looks like “namespace::operator_name”.
 () – The device types to register an impl to.
 () – If provided, the lifetime of this registration
will be tied to the lifetime of the Library object.

List:
() – Should be a string that looks like “namespace::operator_name”.
 () – The device types to register an impl to.
 () – If provided, the lifetime of this registration
will be tied to the lifetime of the Library object.

================================================================================

# torch.library - Low-level APIs (Part 21)

() – Should be a string that looks like “namespace::operator_name”.

() – The device types to register an impl to.

() – If provided, the lifetime of this registration
will be tied to the lifetime of the Library object.

Code example:
# Example 1: Register function.
# Define the operator
"mylib::mysin" "(Tensor x) -> Tensor"

# Add implementations for the cpu device
"mylib::mysin" 
 
     from_numpy

  
  
  

# Example 2: Register function with decorator.
 custom_decorator
      
            
     

# Define the operator
"mylib::sin_plus_one" "(Tensor x) -> Tensor"

# Add implementations for the operator
"mylib::sin_plus_one" 
@custom_decorator
 
     from_numpy

# Call the new operator from torch.ops.
  

  sin_plus_one

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.accelerator (Part 1)

Created On: Oct 27, 2024 | Last Updated On: Jul 10, 2025

This package introduces support for the current accelerator in python.

Table:
device_count | Return the number of current accelerator available.
is_available | Check if the current accelerator is available at runtime: it was build, all the required drivers are available and at least one device is visible.
current_accelerator | Return the device of the accelerator available at compilation time.
set_device_index | Set the current device index to a given device.
set_device_idx | Set the current device index to a given device.
current_device_index | Return the index of a currently selected device for the current accelerator.
current_device_idx | Return the index of a currently selected device for the current accelerator.
set_stream | Set the current stream to a given stream.
current_stream | Return the currently selected stream for a given device.
synchronize | Wait for all kernels in all streams on the given device to complete.
device_index | Context manager to set the current device index for the current accelerator.

Return the number of current accelerator available.

================================================================================

# torch.accelerator (Part 2)

Check if the current accelerator is available at runtime: it was build, all the required drivers are available and at least one device is visible.

Return the device of the accelerator available at compilation time.

Set the current device index to a given device.

Set the current device index to a given device.

Return the index of a currently selected device for the current accelerator.

Return the index of a currently selected device for the current accelerator.

Set the current stream to a given stream.

Return the currently selected stream for a given device.

Wait for all kernels in all streams on the given device to complete.

Context manager to set the current device index for the current accelerator.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.cpu

Created On: Jul 11, 2023 | Last Updated On: Oct 11, 2023

This package implements abstractions found in torch.cuda
to facilitate writing device-agnostic code.

Table:
current_device | Returns current device for cpu.
current_stream | Returns the currently selected  for a given device.
is_available | Returns a bool indicating if CPU is currently available.
synchronize | Waits for all kernels in all streams on the CPU device to complete.
 | Wrapper around the Context-manager StreamContext that selects a given stream.
set_device | Sets the current device, in CPU we do nothing.
device_count | Returns number of CPU devices (not cores).
StreamContext | Context-manager that selects a given stream.

Returns current device for cpu.

Returns the currently selected  for a given device.

Returns a bool indicating if CPU is currently available.

Waits for all kernels in all streams on the CPU device to complete.

Wrapper around the Context-manager StreamContext that selects a given stream.

Sets the current device, in CPU we do nothing.

Returns number of CPU devices (not cores).

Context-manager that selects a given stream.

================================================================================

# torch.cpu - Streams and events

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.cuda (Part 1)

Created On: Dec 23, 2016 | Last Updated On: Jun 13, 2025

This package adds support for CUDA tensor types.

It implements the same function as CPU tensors, but they utilize
GPUs for computation.

It is lazily initialized, so you can always import it, and use
is_available() to determine if your system supports CUDA.

CUDA semantics has more details about working with CUDA.

================================================================================

# torch.cuda (Part 2)

Table:
StreamContext | Context-manager that selects a given stream.
can_device_access_peer | Check if peer access between two devices is possible.
current_blas_handle | Return cublasHandle_t pointer to current cuBLAS handle
current_device | Return the index of a currently selected device.
current_stream | Return the currently selected  for a given device.
 | Retrieves the CUDA runtime API module.
default_stream | Return the default  for a given device.
 | Context-manager that changes the selected device.
device_count | Return the number of GPUs available.
device_memory_used | Return used global (device) memory in bytes as given by nvidia-smi or .
 | Context-manager that changes the current device to that of given object.
get_arch_list | Return list CUDA architectures this library was compiled for.
get_device_capability | Get the cuda capability of a device.
get_device_name | Get the name of a device.
get_device_properties | Get the properties of a device.
get_gencode_flags | Return NVCC gencode flags this library was compiled with.
get_stream_from_external | Return a  from an externally allocated CUDA stream.
get_sync_debug_mode | Return current value of debug mode for cuda synchronizing operations.
 | Initialize PyTorch's CUDA state.
ipc_collect | Force collects GPU memory after it has been released by CUDA IPC.
is_available | Return a bool indicating if CUDA is currently available.
is_initialized | Return whether PyTorch's CUDA state has been initialized.
is_tf32_supported | Return a bool indicating if the current CUDA/ROCm device supports dtype tf32.
memory_usage | Return the percent of time over the past sample period during which global (device) memory was being read or written as given by nvidia-smi.
set_device | Set the current device.
set_stream | Set the current stream.This is a wrapper API to set the stream.
set_sync_debug_mode | Set the debug mode for cuda synchronizing operations.
 | Wrap around the Context-manager StreamContext that selects a given stream.
synchronize | Wait for all kernels in all streams on a CUDA device to complete.
utilization | Return the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.
temperature | Return the average temperature of the GPU sensor in Degrees C (Centigrades).
power_draw | Return the average power draw of the GPU sensor in mW (MilliWatts)
clock_rate | Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by nvidia-smi.
AcceleratorError | Exception raised while executing on device
OutOfMemoryError | Exception raised when device is out of memory

================================================================================

# torch.cuda (Part 3)

Context-manager that selects a given stream.

can_device_access_peer

Check if peer access between two devices is possible.

Return cublasHandle_t pointer to current cuBLAS handle

Return the index of a currently selected device.

Return the currently selected  for a given device.

Retrieves the CUDA runtime API module.

Return the default  for a given device.

Context-manager that changes the selected device.

Return the number of GPUs available.

Return used global (device) memory in bytes as given by nvidia-smi or .

Context-manager that changes the current device to that of given object.

Return list CUDA architectures this library was compiled for.

get_device_capability

Get the cuda capability of a device.

Get the name of a device.

get_device_properties

Get the properties of a device.

Return NVCC gencode flags this library was compiled with.

get_stream_from_external

Return a  from an externally allocated CUDA stream.

Return current value of debug mode for cuda synchronizing operations.

Initialize PyTorch's CUDA state.

Force collects GPU memory after it has been released by CUDA IPC.

Return a bool indicating if CUDA is currently available.

Return whether PyTorch's CUDA state has been initialized.

================================================================================

# torch.cuda (Part 4)

Return a bool indicating if the current CUDA/ROCm device supports dtype tf32.

Return the percent of time over the past sample period during which global (device) memory was being read or written as given by nvidia-smi.

Set the current device.

Set the current stream.This is a wrapper API to set the stream.

Set the debug mode for cuda synchronizing operations.

Wrap around the Context-manager StreamContext that selects a given stream.

Wait for all kernels in all streams on a CUDA device to complete.

Return the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.

Return the average temperature of the GPU sensor in Degrees C (Centigrades).

Return the average power draw of the GPU sensor in mW (MilliWatts)

Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by nvidia-smi.

Exception raised while executing on device

Exception raised when device is out of memory

================================================================================

# torch.cuda - Random Number Generator (Part 1)

Table:
get_rng_state | Return the random number generator state of the specified GPU as a ByteTensor.
get_rng_state_all | Return a list of ByteTensor representing the random number states of all devices.
set_rng_state | Set the random number generator state of the specified GPU.
set_rng_state_all | Set the random number generator state of all devices.
manual_seed | Set the seed for generating random numbers for the current GPU.
manual_seed_all | Set the seed for generating random numbers on all GPUs.
 | Set the seed for generating random numbers to a random number for the current GPU.
 | Set the seed for generating random numbers to a random number on all GPUs.
initial_seed | Return the current random seed of the current GPU.

Return the random number generator state of the specified GPU as a ByteTensor.

Return a list of ByteTensor representing the random number states of all devices.

Set the random number generator state of the specified GPU.

Set the random number generator state of all devices.

Set the seed for generating random numbers for the current GPU.

Set the seed for generating random numbers on all GPUs.

================================================================================

# torch.cuda - Random Number Generator (Part 2)

Set the seed for generating random numbers to a random number for the current GPU.

Set the seed for generating random numbers to a random number on all GPUs.

Return the current random seed of the current GPU.

================================================================================

# torch.cuda - Communication collectives

Table:
comm.broadcast | Broadcasts a tensor to specified GPU devices.
comm.broadcast_coalesced | Broadcast a sequence of tensors to the specified GPUs.
comm.reduce_add | Sum tensors from multiple GPUs.
comm.reduce_add_coalesced | Sum tensors from multiple GPUs.
comm.scatter | Scatters tensor across multiple GPUs.
comm.gather | Gathers tensors from multiple GPU devices.

Broadcasts a tensor to specified GPU devices.

comm.broadcast_coalesced

Broadcast a sequence of tensors to the specified GPUs.

Sum tensors from multiple GPUs.

comm.reduce_add_coalesced

Sum tensors from multiple GPUs.

Scatters tensor across multiple GPUs.

Gathers tensors from multiple GPU devices.

================================================================================

# torch.cuda - Streams and events

Table:
 | Wrapper around a CUDA stream.
ExternalStream | Wrapper around an externally allocated CUDA stream.
 | Wrapper around a CUDA event.

Wrapper around a CUDA stream.

Wrapper around an externally allocated CUDA stream.

Wrapper around a CUDA event.

================================================================================

# torch.cuda - Graphs (beta)

Table:
is_current_stream_capturing | Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.
graph_pool_handle | Return an opaque token representing the id of a graph memory pool.
 | Wrapper around a CUDA graph.
 | Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.
make_graphed_callables | Accept callables (functions or s) and returns graphed versions.

is_current_stream_capturing

Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.

Return an opaque token representing the id of a graph memory pool.

Wrapper around a CUDA graph.

Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.

make_graphed_callables

Accept callables (functions or s) and returns graphed versions.

This package adds support for device memory management implemented in CUDA.

================================================================================

# torch.cuda - Memory management (Part 1)

Table:
empty_cache | Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.
get_per_process_memory_fraction | Get memory fraction for a process.
list_gpu_processes | Return a human-readable printout of the running processes and their GPU memory use for a given device.
mem_get_info | Return the global free and total GPU memory for a given device using cudaMemGetInfo.
memory_stats | Return a dictionary of CUDA memory allocator statistics for a given device.
memory_stats_as_nested_dict | Return the result of memory_stats() as a nested dictionary.
reset_accumulated_memory_stats | Reset the "accumulated" (historical) stats tracked by the CUDA memory allocator.
host_memory_stats | Return a dictionary of CUDA memory allocator statistics for a given device.
host_memory_stats_as_nested_dict | Return the result of host_memory_stats() as a nested dictionary.
reset_accumulated_host_memory_stats | Reset the "accumulated" (historical) stats tracked by the host memory allocator.
memory_summary | Return a human-readable printout of the current memory allocator statistics for a given device.
memory_snapshot | Return a snapshot of the CUDA memory allocator state across all devices.
memory_allocated | Return the current GPU memory occupied by tensors in bytes for a given device.
max_memory_allocated | Return the maximum GPU memory occupied by tensors in bytes for a given device.
reset_max_memory_allocated | Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.
memory_reserved | Return the current GPU memory managed by the caching allocator in bytes for a given device.
max_memory_reserved | Return the maximum GPU memory managed by the caching allocator in bytes for a given device.
set_per_process_memory_fraction | Set memory fraction for a process.
memory_cached | Deprecated; see memory_reserved().
max_memory_cached | Deprecated; see max_memory_reserved().
reset_max_memory_cached | Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.
reset_peak_memory_stats | Reset the "peak" stats tracked by the CUDA memory allocator.
reset_peak_host_memory_stats | Reset the "peak" stats tracked by the host memory allocator.
caching_allocator_alloc | Perform a memory allocation using the CUDA memory allocator.
caching_allocator_delete | Delete memory allocated using the CUDA memory allocator.
get_allocator_backend | Return a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF.
CUDAPluggableAllocator | CUDA memory allocator loaded from a so file.
change_current_allocator | Change the currently used memory allocator to be the one provided.
 | MemPool represents a pool of memory in a caching allocator.

================================================================================

# torch.cuda - Memory management (Part 2)

Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.

get_per_process_memory_fraction

Get memory fraction for a process.

Return a human-readable printout of the running processes and their GPU memory use for a given device.

Return the global free and total GPU memory for a given device using cudaMemGetInfo.

Return a dictionary of CUDA memory allocator statistics for a given device.

memory_stats_as_nested_dict

Return the result of memory_stats() as a nested dictionary.

reset_accumulated_memory_stats

Reset the "accumulated" (historical) stats tracked by the CUDA memory allocator.

Return a dictionary of CUDA memory allocator statistics for a given device.

host_memory_stats_as_nested_dict

Return the result of host_memory_stats() as a nested dictionary.

reset_accumulated_host_memory_stats

Reset the "accumulated" (historical) stats tracked by the host memory allocator.

Return a human-readable printout of the current memory allocator statistics for a given device.

Return a snapshot of the CUDA memory allocator state across all devices.

================================================================================

# torch.cuda - Memory management (Part 3)

Return the current GPU memory occupied by tensors in bytes for a given device.

Return the maximum GPU memory occupied by tensors in bytes for a given device.

reset_max_memory_allocated

Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device.

Return the current GPU memory managed by the caching allocator in bytes for a given device.

Return the maximum GPU memory managed by the caching allocator in bytes for a given device.

set_per_process_memory_fraction

Set memory fraction for a process.

Deprecated; see memory_reserved().

Deprecated; see max_memory_reserved().

reset_max_memory_cached

Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.

reset_peak_memory_stats

Reset the "peak" stats tracked by the CUDA memory allocator.

reset_peak_host_memory_stats

Reset the "peak" stats tracked by the host memory allocator.

caching_allocator_alloc

Perform a memory allocation using the CUDA memory allocator.

caching_allocator_delete

Delete memory allocated using the CUDA memory allocator.

get_allocator_backend

Return a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF.

CUDAPluggableAllocator

================================================================================

# torch.cuda - Memory management (Part 4)

CUDA memory allocator loaded from a so file.

change_current_allocator

Change the currently used memory allocator to be the one provided.

MemPool represents a pool of memory in a caching allocator.

Table:
caching_allocator_enable | Enable or disable the CUDA memory allocator.

caching_allocator_enable

Enable or disable the CUDA memory allocator.

torch.cuda.use_mem_pool, 
A context manager that routes allocations to a given pool.

Parameters

 (torch.cuda.MemPool) – a MemPool object to be made active so that
allocations route to this pool.
 (torch.device) – selected device. Uses MemPool on
the current device, given by current_device(),
if  is  (default).





This context manager makes only current thread’s allocations route to
the given pool. If a new thread is spawned inside the context manager
(e.g. by calling backward) the allocations in that thread will not
route to the given pool.

A context manager that routes allocations to a given pool.

Parameters

 (torch.cuda.MemPool) – a MemPool object to be made active so that
allocations route to this pool.
 (torch.device) – selected device. Uses MemPool on
the current device, given by current_device(),
if  is  (default).

================================================================================

# torch.cuda - Memory management (Part 5)

List:
(torch.cuda.MemPool) – a MemPool object to be made active so that
allocations route to this pool.
 (torch.device) – selected device. Uses MemPool on
the current device, given by current_device(),
if  is  (default).

(torch.cuda.MemPool) – a MemPool object to be made active so that
allocations route to this pool.

(torch.device) – selected device. Uses MemPool on
the current device, given by current_device(),
if  is  (default).

This context manager makes only current thread’s allocations route to
the given pool. If a new thread is spawned inside the context manager
(e.g. by calling backward) the allocations in that thread will not
route to the given pool.

================================================================================

# torch.cuda - NVIDIA Tools Extension (NVTX)

Table:
Describe an instantaneous event that occurred at some point.
nvtx.range_push | Push a range onto a stack of nested range span.
nvtx.range_pop | Pop a range off of a stack of nested range spans.
nvtx.range | Context manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end.

Describe an instantaneous event that occurred at some point.

Push a range onto a stack of nested range span.

Pop a range off of a stack of nested range spans.

Context manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end.

================================================================================

# torch.cuda - Jiterator (beta)

Table:
jiterator._create_jit_fn | Create a jiterator-generated cuda kernel for an elementwise op.
jiterator._create_multi_output_jit_fn | Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.

jiterator._create_jit_fn

Create a jiterator-generated cuda kernel for an elementwise op.

jiterator._create_multi_output_jit_fn

Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.

================================================================================

# torch.cuda - TunableOp

Some operations could be implemented using more than one library or more than
one technique. For example, a GEMM could be implemented for CUDA or ROCm using
either the cublas/cublasLt libraries or hipblas/hipblasLt libraries,
respectively. How does one know which implementation is the fastest and should
be chosen? That’s what TunableOp provides. Certain operators have been
implemented using multiple strategies as Tunable Operators. At runtime, all
strategies are profiled and the fastest is selected for all subsequent
operations.

See the documentation for information on how to use it.

================================================================================

# torch.cuda - Stream Sanitizer (prototype)

CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch.
See the documentation for information on how to use it.

================================================================================

# torch.cuda - GPUDirect Storage (prototype)

The APIs in torch.cuda.gds provide thin wrappers around certain cuFile APIs that allow
direct memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the
cufile api documentation
for more details.

These APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must
ensure that their system is appropriately configured to use GPUDirect Storage per the
GPUDirect Storage documentation.

See the docs for  for an example of how to use these.

Table:
gds_register_buffer | Registers a storage on a CUDA device as a cufile buffer.
gds_deregister_buffer | Deregisters a previously registered storage on a CUDA device as a cufile buffer.
 | Wrapper around cuFile.

Registers a storage on a CUDA device as a cufile buffer.

gds_deregister_buffer

Deregisters a previously registered storage on a CUDA device as a cufile buffer.

Wrapper around cuFile.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Understanding CUDA Memory Usage

Created On: Aug 23, 2023 | Last Updated On: Jun 10, 2025

To debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory
at any point in time, and optionally record the history of allocation events that led up to that snapshot.

The generated snapshots can then be drag and dropped onto the interactiver viewer hosted at pytorch.org/memory_viz which
can be used to explore the snapshot.

The memory profiler and visualizer described in this document only have visibility into the CUDA memory that is
allocated and managed through the PyTorch allocator.  Any memory allocated directly from CUDA APIs will not be
visible in the PyTorch memory profiler.

NCCL (used for distributed communication on CUDA devices) is a common example of a library that allocates some
GPU memory that is invisible to the PyTorch memory profiler.  See Identifying Non-PyTorch allocations for more info.

================================================================================

# Understanding CUDA Memory Usage - Generating a Snapshot

The common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:

Code example:
# enable memory history, which will
# add tracebacks and event history to snapshots
_record_memory_history

run_your_code
_dump_snapshot"my_snapshot.pickle"

================================================================================

# Understanding CUDA Memory Usage - Using the visualizer

Open pytorch.org/memory_viz and drag/drop the pickled snapshot file into the visualizer.
The visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.

================================================================================

# Understanding CUDA Memory Usage - Active Memory Timeline

The Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations.
Mouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to
render fewer allocations and improve performance when there is a lot of data.

================================================================================

# Understanding CUDA Memory Usage - Allocator State History

The Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the
allocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations
or free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred,
such as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why
an allocation failed even though reserved memory still exists.

The stack trace information also reports the address at which an allocation occurred.
The address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the “_0”th time this address was allocated.
This unique string can be looked up in the Active Memory Timeline and searched
in the Active State History to examine the memory state when a tensor was allocated or freed.

================================================================================

# Understanding CUDA Memory Usage - Identifying Non-PyTorch allocations

If you suspect CUDA memory is being allocated outside of PyTorch, you can collect the raw CUDA allocation info using
the pynvml package, and compare that to the allocation reported by pytorch.

To collect raw memory usage outside pytorch, use device_memory_used()

Code example:
device_idx  
device_memory_useddevice_idx

================================================================================

# Understanding CUDA Memory Usage - Snapshot API Reference (Part 1)

torch.cuda.memory._record_memory_history, , , max_entries9223372036854775807, , clear_history, compile_context, global_record_annotations
Enable recording of stack traces associated with memory
allocations, so you can tell what allocated any piece of memory in
torch.cuda.memory._snapshot().
In addition to keeping stack traces with each current allocation and free,
this will also enable recording of a history of all alloc/free events.
Use torch.cuda.memory._snapshot() to retrieve this information,
and the tools in _memory_viz.py to visualize snapshots.

Buffer behavior
This will store up to max_entries instances of TraceEntry when enabled.
Python trace collection defaults to sys.maxsize, meaning long-running
or indefinitely running jobs should set a reasonable limit to avoid excessive
memory use. Expect each entry to be several KB.
Longer running workflows or those with smaller max_entries values will only
store the last accumulated max_entries entries, meaning new entries overwrite
older entries.
C++ implementation for reference to ring buffer implemenation:
record_history
alloc_tracealloc_trace_max_entries_
alloc_traceemplace_back

================================================================================

# Understanding CUDA Memory Usage - Snapshot API Reference (Part 2)

alloc_tracealloc_trace_next
alloc_trace_nextalloc_trace_max_entries_
alloc_trace_next







Latency impact
The Python trace collection is fast (2us per trace), so you may consider
enabling this on production jobs if you anticipate ever having to debug
memory issues.
C++ trace collection is also fast (~50ns/frame), which for many typical programs
works out to ~2us per trace, but can vary depending on stack depth.

param enabled
, disable recording memory history.
, keep information for currenly allocated memory.
, additionally keep a history of all alloc/free calls.
Defaults to “all”.

type enabled
Literal[None, “state”, “all”], optional

param context
, Do not record any tracebacks.
, Record tracebacks for currently allocated memory.
, additionally keep tracebacks for alloc calls.
, additionally keep tracebacks for free calls.
Defaults to “all”.

type context
Literal[None, “state”, “alloc”, “all”], optional

param stacks
, include Python, TorchScript, and inductor frames in tracebacks
, additionally include C++ frames
Defaults to “all”.

type stacks
Literal[“python”, “all”], optional

param max_entries
Keep a maximum of max_entries
alloc/free events in the recorded history recorded.

================================================================================

# Understanding CUDA Memory Usage - Snapshot API Reference (Part 3)

type max_entries
int, optional

Enable recording of stack traces associated with memory
allocations, so you can tell what allocated any piece of memory in
torch.cuda.memory._snapshot().

In addition to keeping stack traces with each current allocation and free,
this will also enable recording of a history of all alloc/free events.

Use torch.cuda.memory._snapshot() to retrieve this information,
and the tools in _memory_viz.py to visualize snapshots.

================================================================================

# Understanding CUDA Memory Usage - Buffer behavior

This will store up to max_entries instances of TraceEntry when enabled.
Python trace collection defaults to sys.maxsize, meaning long-running
or indefinitely running jobs should set a reasonable limit to avoid excessive
memory use. Expect each entry to be several KB.

Longer running workflows or those with smaller max_entries values will only
store the last accumulated max_entries entries, meaning new entries overwrite
older entries.

C++ implementation for reference to ring buffer implemenation:

Code example:
record_history
alloc_tracealloc_trace_max_entries_
alloc_traceemplace_back

alloc_tracealloc_trace_next
alloc_trace_nextalloc_trace_max_entries_
alloc_trace_next

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 1)

The Python trace collection is fast (2us per trace), so you may consider
enabling this on production jobs if you anticipate ever having to debug
memory issues.

C++ trace collection is also fast (~50ns/frame), which for many typical programs
works out to ~2us per trace, but can vary depending on stack depth.

param enabled
, disable recording memory history.
, keep information for currenly allocated memory.
, additionally keep a history of all alloc/free calls.
Defaults to “all”.

type enabled
Literal[None, “state”, “all”], optional

param context
, Do not record any tracebacks.
, Record tracebacks for currently allocated memory.
, additionally keep tracebacks for alloc calls.
, additionally keep tracebacks for free calls.
Defaults to “all”.

type context
Literal[None, “state”, “alloc”, “all”], optional

param stacks
, include Python, TorchScript, and inductor frames in tracebacks
, additionally include C++ frames
Defaults to “all”.

type stacks
Literal[“python”, “all”], optional

param max_entries
Keep a maximum of max_entries
alloc/free events in the recorded history recorded.

type max_entries
int, optional

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 2)

, disable recording memory history.
, keep information for currenly allocated memory.
, additionally keep a history of all alloc/free calls.
Defaults to “all”.

Literal[None, “state”, “all”], optional

, Do not record any tracebacks.
, Record tracebacks for currently allocated memory.
, additionally keep tracebacks for alloc calls.
, additionally keep tracebacks for free calls.
Defaults to “all”.

Literal[None, “state”, “alloc”, “all”], optional

, include Python, TorchScript, and inductor frames in tracebacks
, additionally include C++ frames
Defaults to “all”.

Literal[“python”, “all”], optional

Keep a maximum of max_entries
alloc/free events in the recorded history recorded.

torch.cuda.memory.
Save a snapshot of CUDA memory state at the time it was called.
The state is represented as a dictionary with the following structure.
 
      
    device_traces TraceEntry

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 3)

 
    # Segments are memory returned from a cudaMalloc call.
    # The size of reserved memory is the sum of all Segments.
    # Segments are cached and reused for future allocations.
    # If the reuse is smaller than the segment, the segment
    # is split into more then one Block.
    # empty_cache() frees Segments that are entirely inactive.
     
    total_size  #  cudaMalloc'd size of segment
     
    segment_type   # 'large' (>1MB)
    allocated_size  # size of memory in use
    active_size  # size of memory in use or in active_awaiting_free state
      

 
    # A piece of memory returned from the allocator, or
    # current cached but inactive.
     
    requested_size  # size requested during malloc, may be smaller than
                        # size due to rounding
     
     'active_allocated' # used by a tensor
                'active_awaiting_free' # waiting for another stream to finish using
                                        # this, then it will become free
                'inactive' # free for reuse
      # stack trace from where the allocation occurred

 
         
         
         

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 4)

 TraceEntry
    # When `torch.cuda.memory._record_memory_history()` is enabled,
    # the snapshot will contain TraceEntry objects that record each
    # action the allocator took.
     
      # memory allocated
    'free_requested' # the allocated received a call to free memory
    'free_completed' # the memory that was requested to be freed is now
                    # able to be used in future allocation calls
    'segment_alloc' # the caching allocator ask cudaMalloc for more memory
                    # and added it as a segment in its cache
    'segment_free'  # the caching allocator called cudaFree to return memory
                    # to cuda possibly trying free up memory to
                    # allocate more segments or because empty_caches was called
              # the allocator threw an OOM exception. 'size' is
                    # the requested number of bytes that did not succeed
    'snapshot'      # the allocator generated a memory snapshot
                    # useful to coorelate a previously taken
                    # snapshot with this trace
    
      # not present for OOM
     
     
     
    device_free  # only present for OOM, the amount of
                    # memory cuda still reports to be free

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 5)




The Snapshot dictionary object

Save a snapshot of CUDA memory state at the time it was called.

The state is represented as a dictionary with the following structure.

Code example:
device_traces TraceEntry

 
    # Segments are memory returned from a cudaMalloc call.
    # The size of reserved memory is the sum of all Segments.
    # Segments are cached and reused for future allocations.
    # If the reuse is smaller than the segment, the segment
    # is split into more then one Block.
    # empty_cache() frees Segments that are entirely inactive.
     
    total_size  #  cudaMalloc'd size of segment
     
    segment_type   # 'large' (>1MB)
    allocated_size  # size of memory in use
    active_size  # size of memory in use or in active_awaiting_free state
      

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 6)

 
    # A piece of memory returned from the allocator, or
    # current cached but inactive.
     
    requested_size  # size requested during malloc, may be smaller than
                        # size due to rounding
     
     'active_allocated' # used by a tensor
                'active_awaiting_free' # waiting for another stream to finish using
                                        # this, then it will become free
                'inactive' # free for reuse
      # stack trace from where the allocation occurred

 
         
         
         

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 7)

 TraceEntry
    # When `torch.cuda.memory._record_memory_history()` is enabled,
    # the snapshot will contain TraceEntry objects that record each
    # action the allocator took.
     
      # memory allocated
    'free_requested' # the allocated received a call to free memory
    'free_completed' # the memory that was requested to be freed is now
                    # able to be used in future allocation calls
    'segment_alloc' # the caching allocator ask cudaMalloc for more memory
                    # and added it as a segment in its cache
    'segment_free'  # the caching allocator called cudaFree to return memory
                    # to cuda possibly trying free up memory to
                    # allocate more segments or because empty_caches was called
              # the allocator threw an OOM exception. 'size' is
                    # the requested number of bytes that did not succeed
    'snapshot'      # the allocator generated a memory snapshot
                    # useful to coorelate a previously taken
                    # snapshot with this trace
    
      # not present for OOM
     
     
     
    device_free  # only present for OOM, the amount of
                    # memory cuda still reports to be free

================================================================================

# Understanding CUDA Memory Usage - Latency impact (Part 8)

The Snapshot dictionary object

The Snapshot dictionary object

torch.cuda.memory._dump_snapshot'dump_snapshot.pickle'
Save a pickled version of the torch.memory._snapshot() dictionary to a file.
This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz
Snapshot file sizes scale with max_entries and stack trace depth per entry,
with several KB per entry. These can easily be in the GB range for longer running
workflows with large max_entries.

Parameters
 () – Name of the file to create. Defaults to “dump_snapshot.pickle”.

Save a pickled version of the torch.memory._snapshot() dictionary to a file.

This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz

Snapshot file sizes scale with max_entries and stack trace depth per entry,
with several KB per entry. These can easily be in the GB range for longer running
workflows with large max_entries.

Parameters
 () – Name of the file to create. Defaults to “dump_snapshot.pickle”.

() – Name of the file to create. Defaults to “dump_snapshot.pickle”.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.mps (Part 1)

Created On: Feb 10, 2023 | Last Updated On: Jun 08, 2025

This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python.
Metal is Apple’s API for programming metal GPU (graphics processor unit). Using MPS means that increased
performance can be achieved, by running work on the metal GPU(s).
See https://developer.apple.com/documentation/metalperformanceshaders for more details.

================================================================================

# torch.mps (Part 2)

Table:
device_count | Returns the number of available MPS devices.
synchronize | Waits for all kernels in all streams on a MPS device to complete.
get_rng_state | Returns the random number generator state as a ByteTensor.
set_rng_state | Sets the random number generator state.
manual_seed | Sets the seed for generating random numbers.
 | Sets the seed for generating random numbers to a random number.
empty_cache | Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.
set_per_process_memory_fraction | Set memory fraction for limiting process's memory allocation on MPS device.
current_allocated_memory | Returns the current GPU memory occupied by tensors in bytes.
driver_allocated_memory | Returns total GPU memory allocated by Metal driver for the process in bytes.
recommended_max_memory | Returns recommended max Working set size for GPU memory in bytes.
compile_shader | Compiles compute shader from source and allows one to invoke kernels defined there from the comfort of Python runtime Example.

Returns the number of available MPS devices.

Waits for all kernels in all streams on a MPS device to complete.

================================================================================

# torch.mps (Part 3)

Returns the random number generator state as a ByteTensor.

Sets the random number generator state.

Sets the seed for generating random numbers.

Sets the seed for generating random numbers to a random number.

Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.

set_per_process_memory_fraction

Set memory fraction for limiting process's memory allocation on MPS device.

current_allocated_memory

Returns the current GPU memory occupied by tensors in bytes.

driver_allocated_memory

Returns total GPU memory allocated by Metal driver for the process in bytes.

recommended_max_memory

Returns recommended max Working set size for GPU memory in bytes.

Compiles compute shader from source and allows one to invoke kernels defined there from the comfort of Python runtime Example.

================================================================================

# torch.mps - MPS Profiler

Table:
profiler.start | Start OS Signpost tracing from MPS backend.
profiler.stop | Stops generating OS Signpost tracing from MPS backend.
profiler.profile | Context Manager to enabling generating OS Signpost tracing from MPS backend.
profiler.is_capturing_metal | Cheks if metal capture is in progress
profiler.is_metal_capture_enabled | Checks if metal_capture context manager is usable To enable metal capture, set MTL_CAPTURE_ENABLED envvar
profiler.metal_capture | Conext manager that enables capturing of Metal calls into gputrace

Start OS Signpost tracing from MPS backend.

Stops generating OS Signpost tracing from MPS backend.

Context Manager to enabling generating OS Signpost tracing from MPS backend.

profiler.is_capturing_metal

Cheks if metal capture is in progress

profiler.is_metal_capture_enabled

Checks if metal_capture context manager is usable To enable metal capture, set MTL_CAPTURE_ENABLED envvar

profiler.metal_capture

Conext manager that enables capturing of Metal calls into gputrace

================================================================================

# torch.mps - MPS Event

Table:
event.Event | Wrapper around an MPS event.

Wrapper around an MPS event.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.xpu (Part 1)

Created On: Feb 01, 2024 | Last Updated On: Jun 06, 2025

This package introduces support for the XPU backend, specifically tailored for
Intel GPU optimization.

This package is lazily initialized, so you can always import it, and use
is_available() to determine if your system supports XPU.

================================================================================

# torch.xpu (Part 2)

Table:
StreamContext | Context-manager that selects a given stream.
current_device | Return the index of a currently selected device.
current_stream | Return the currently selected  for a given device.
 | Context-manager that changes the selected device.
device_count | Return the number of XPU device available.
 | Context-manager that changes the current device to that of given object.
get_arch_list | Return list XPU architectures this library was compiled for.
get_device_capability | Get the xpu capability of a device.
get_device_name | Get the name of a device.
get_device_properties | Get the properties of a device.
get_gencode_flags | Return XPU AOT(ahead-of-time) build flags this library was compiled with.
get_stream_from_external | Return a  from an external SYCL queue.
 | Initialize PyTorch's XPU state.
is_available | Return a bool indicating if XPU is currently available.
is_initialized | Return whether PyTorch's XPU state has been initialized.
set_device | Set the current device.
set_stream | Set the current stream.This is a wrapper API to set the stream.
 | Wrap around the Context-manager StreamContext that selects a given stream.
synchronize | Wait for all kernels in all streams on a XPU device to complete.

================================================================================

# torch.xpu (Part 3)

Context-manager that selects a given stream.

Return the index of a currently selected device.

Return the currently selected  for a given device.

Context-manager that changes the selected device.

Return the number of XPU device available.

Context-manager that changes the current device to that of given object.

Return list XPU architectures this library was compiled for.

get_device_capability

Get the xpu capability of a device.

Get the name of a device.

get_device_properties

Get the properties of a device.

Return XPU AOT(ahead-of-time) build flags this library was compiled with.

get_stream_from_external

Return a  from an external SYCL queue.

Initialize PyTorch's XPU state.

Return a bool indicating if XPU is currently available.

Return whether PyTorch's XPU state has been initialized.

Set the current device.

Set the current stream.This is a wrapper API to set the stream.

Wrap around the Context-manager StreamContext that selects a given stream.

Wait for all kernels in all streams on a XPU device to complete.

================================================================================

# torch.xpu - Random Number Generator (Part 1)

Table:
get_rng_state | Return the random number generator state of the specified GPU as a ByteTensor.
get_rng_state_all | Return a list of ByteTensor representing the random number states of all devices.
initial_seed | Return the current random seed of the current GPU.
manual_seed | Set the seed for generating random numbers for the current GPU.
manual_seed_all | Set the seed for generating random numbers on all GPUs.
 | Set the seed for generating random numbers to a random number for the current GPU.
 | Set the seed for generating random numbers to a random number on all GPUs.
set_rng_state | Set the random number generator state of the specified GPU.
set_rng_state_all | Set the random number generator state of all devices.

Return the random number generator state of the specified GPU as a ByteTensor.

Return a list of ByteTensor representing the random number states of all devices.

Return the current random seed of the current GPU.

Set the seed for generating random numbers for the current GPU.

Set the seed for generating random numbers on all GPUs.

Set the seed for generating random numbers to a random number for the current GPU.

================================================================================

# torch.xpu - Random Number Generator (Part 2)

Set the seed for generating random numbers to a random number on all GPUs.

Set the random number generator state of the specified GPU.

Set the random number generator state of all devices.

================================================================================

# torch.xpu - Streams and events

Table:
 | Wrapper around a XPU event.
 | Wrapper around a XPU stream.

Wrapper around a XPU event.

Wrapper around a XPU stream.

================================================================================

# torch.xpu - Memory management (Part 1)

Table:
empty_cache | Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application.
max_memory_allocated | Return the maximum GPU memory occupied by tensors in bytes for a given device.
max_memory_reserved | Return the maximum GPU memory managed by the caching allocator in bytes for a given device.
mem_get_info | Return the global free and total GPU memory for a given device.
memory_allocated | Return the current GPU memory occupied by tensors in bytes for a given device.
memory_reserved | Return the current GPU memory managed by the caching allocator in bytes for a given device.
memory_stats | Return a dictionary of XPU memory allocator statistics for a given device.
memory_stats_as_nested_dict | Return the result of memory_stats() as a nested dictionary.
reset_accumulated_memory_stats | Reset the "accumulated" (historical) stats tracked by the XPU memory allocator.
reset_peak_memory_stats | Reset the "peak" stats tracked by the XPU memory allocator.

Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application.

================================================================================

# torch.xpu - Memory management (Part 2)

Return the maximum GPU memory occupied by tensors in bytes for a given device.

Return the maximum GPU memory managed by the caching allocator in bytes for a given device.

Return the global free and total GPU memory for a given device.

Return the current GPU memory occupied by tensors in bytes for a given device.

Return the current GPU memory managed by the caching allocator in bytes for a given device.

Return a dictionary of XPU memory allocator statistics for a given device.

memory_stats_as_nested_dict

Return the result of memory_stats() as a nested dictionary.

reset_accumulated_memory_stats

Reset the "accumulated" (historical) stats tracked by the XPU memory allocator.

reset_peak_memory_stats

Reset the "peak" stats tracked by the XPU memory allocator.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.mtia (Part 1)

Created On: Jul 11, 2023 | Last Updated On: Jun 08, 2025

The MTIA backend is implemented out of the tree, only interfaces are be defined here.

This package enables an interface for accessing MTIA backend in python

================================================================================

# torch.mtia (Part 2)

Table:
StreamContext | Context-manager that selects a given stream.
current_device | Return the index of a currently selected device.
current_stream | Return the currently selected  for a given device.
default_stream | Return the default  for a given device.
device_count | Return the number of MTIA devices available.
is_available | Return true if MTIA device is available
is_initialized | Return whether PyTorch's MTIA state has been initialized.
memory_stats | Return a dictionary of MTIA memory allocator statistics for a given device.
get_device_capability | Return capability of a given device as a tuple of (major version, minor version).
empty_cache | Empty the MTIA device cache.
record_memory_history | Enable/Disable the memory profiler on MTIA allocator
 | Return a dictionary of MTIA memory allocator history
attach_out_of_memory_observer | Attach an out-of-memory observer to MTIA memory allocator
set_device | Set the current device.
set_stream | Set the current stream.This is a wrapper API to set the stream.
 | Wrap around the Context-manager StreamContext that selects a given stream.
synchronize | Waits for all jobs in all streams on a MTIA device to complete.
 | Context-manager that changes the selected device.
set_rng_state | Sets the random number generator state.
get_rng_state | Returns the random number generator state as a ByteTensor.
DeferredMtiaCallError | 

================================================================================

# torch.mtia (Part 3)

Context-manager that selects a given stream.

Return the index of a currently selected device.

Return the currently selected  for a given device.

Return the default  for a given device.

Return the number of MTIA devices available.

Return true if MTIA device is available

Return whether PyTorch's MTIA state has been initialized.

Return a dictionary of MTIA memory allocator statistics for a given device.

get_device_capability

Return capability of a given device as a tuple of (major version, minor version).

Empty the MTIA device cache.

record_memory_history

Enable/Disable the memory profiler on MTIA allocator

Return a dictionary of MTIA memory allocator history

attach_out_of_memory_observer

Attach an out-of-memory observer to MTIA memory allocator

Set the current device.

Set the current stream.This is a wrapper API to set the stream.

Wrap around the Context-manager StreamContext that selects a given stream.

Waits for all jobs in all streams on a MTIA device to complete.

Context-manager that changes the selected device.

Sets the random number generator state.

Returns the random number generator state as a ByteTensor.

DeferredMtiaCallError

================================================================================

# torch.mtia - Streams and events

Table:
 | Query and record Stream status to identify or control dependencies across Stream and measure timing.
 | An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.

Query and record Stream status to identify or control dependencies across Stream and measure timing.

An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.mtia.memory

Created On: Dec 09, 2024 | Last Updated On: Jun 08, 2025

The MTIA backend is implemented out of the tree, only interfaces are be defined here.

This package adds support for device memory management implemented in MTIA.

Table:
memory_stats | Return a dictionary of MTIA memory allocator statistics for a given device.

Return a dictionary of MTIA memory allocator statistics for a given device.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Meta device (Part 1)

Created On: Jun 17, 2025 | Last Updated On: Jun 17, 2025

The “meta” device is an abstract device which denotes a tensor which records
only metadata, but no actual data.  Meta tensors have two primary use cases:

List:
Models can be loaded on the meta device, allowing you to load a
representation of the model without actually loading the actual parameters
into memory.  This can be helpful if you need to make transformations on
the model before you load the actual data.
Most operations can be performed on meta tensors, producing new meta
tensors that describe what the result would have been if you performed
the operation on a real tensor.  You can use this to perform abstract
analysis without needing to spend time on compute or space to represent
the actual tensors.  Because meta tensors do not have real data, you cannot
perform data-dependent operations like torch.nonzero() or
.  In some cases, not all device types (e.g., CPU
and CUDA) have exactly the same output metadata for an operation; we
typically prefer representing the CUDA behavior faithfully in this
situation.

================================================================================

# Meta device (Part 2)

Models can be loaded on the meta device, allowing you to load a
representation of the model without actually loading the actual parameters
into memory.  This can be helpful if you need to make transformations on
the model before you load the actual data.

Most operations can be performed on meta tensors, producing new meta
tensors that describe what the result would have been if you performed
the operation on a real tensor.  You can use this to perform abstract
analysis without needing to spend time on compute or space to represent
the actual tensors.  Because meta tensors do not have real data, you cannot
perform data-dependent operations like torch.nonzero() or
.  In some cases, not all device types (e.g., CPU
and CUDA) have exactly the same output metadata for an operation; we
typically prefer representing the CUDA behavior faithfully in this
situation.

Although in principle meta tensor computation should always be faster than
an equivalent CPU/CUDA computation, many meta tensor implementations are
implemented in Python and have not been ported to C++ for speed, so you
may find that you get lower absolute framework latency with small CPU tensors.

================================================================================

# Meta device - Idioms for working with meta tensors (Part 1)

An object can be loaded with torch.load() onto meta device by specifying
map_location='meta':

Code example:
map_location
tensor(..., device='meta', size=(2,))

If you have some arbitrary code which performs some tensor construction without
explicitly specifying a device, you can override it to instead construct on meta device by using
the torch.device() context manager:

Code example:
tensor(..., device='meta', size=(30, 30))

This is especially helpful NN module construction, where you often are not
able to explicitly pass in a device for initialization:

Code example:
torch.nn.modules  
 
     

Linear(in_features=20, out_features=30, bias=True)

You cannot convert a meta tensor directly to a CPU/CUDA tensor, because the
meta tensor stores no data and we do not know what the correct data values for
your new tensor are:

Code example:
Traceback (most recent call last):
  File , line , in 
NotImplementedError: Cannot copy out of meta tensor; no data!

Use a factory function like torch.empty_like() to explicitly specify how
you would like the missing data to be filled in.

================================================================================

# Meta device - Idioms for working with meta tensors (Part 2)

NN modules have a convenience method torch.nn.Module.to_empty() that
allows you to move the module to another device, leaving all parameters
uninitialized.  You are expected to explicitly reinitialize the parameters
manually:

Code example:
torch.nn.modules  
 
       

Linear(in_features=20, out_features=30, bias=True)

torch._subclasses.meta_utils contains undocumented utilities for taking
an arbitrary Tensor and constructing an equivalent meta Tensor with high
fidelity.  These APIs are experimental and may be changed in a BC breaking way
at any time.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.backends

Created On: Sep 16, 2020 | Last Updated On: Jun 12, 2025

torch.backends controls the behavior of various backends that PyTorch supports.

These backends include:

List:
torch.backends.cpu
torch.backends.cuda
torch.backends.cudnn
torch.backends.cusparselt
torch.backends.mha
torch.backends.mps
torch.backends.mkl
torch.backends.mkldnn
torch.backends.nnpack
torch.backends.openmp
torch.backends.opt_einsum
torch.backends.xeon

torch.backends.cusparselt

torch.backends.mkldnn

torch.backends.nnpack

torch.backends.openmp

torch.backends.opt_einsum

================================================================================

# torch.backends - torch.backends.cpu

torch.backends.cpu.get_cpu_capability
Return cpu capability as a string value.
Possible values:
- “DEFAULT”
- “VSX”
- “Z VECTOR”
- “NO AVX”
- “AVX2”
- “AVX512”
- “SVE256”

Return type

Return cpu capability as a string value.

Possible values:
- “DEFAULT”
- “VSX”
- “Z VECTOR”
- “NO AVX”
- “AVX2”
- “AVX512”
- “SVE256”

================================================================================

# torch.backends - torch.backends.cuda (Part 1)

torch.backends.cuda.
Return whether PyTorch is built with CUDA support.
Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch
binary were run on a machine with working CUDA drivers and devices, we would be able to use it.

Return whether PyTorch is built with CUDA support.

Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch
binary were run on a machine with working CUDA drivers and devices, we would be able to use it.

torch.backends.cuda.matmul.allow_tf32
A  that controls whether TensorFloat-32 tensor cores may be used in matrix
multiplications on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices.

A  that controls whether TensorFloat-32 tensor cores may be used in matrix
multiplications on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices.

torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction
A  that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.

A  that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.

================================================================================

# torch.backends - torch.backends.cuda (Part 2)

torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction
A  that controls whether reduced precision reductions are allowed with bf16 GEMMs.

A  that controls whether reduced precision reductions are allowed with bf16 GEMMs.

torch.backends.cuda.cufft_plan_cache
cufft_plan_cache contains the cuFFT plan caches for each CUDA device.
Query a specific device ’s cache via torch.backends.cuda.cufft_plan_cache[i].


torch.backends.cuda.cufft_plan_cache.
A readonly  that shows the number of plans currently in a cuFFT plan cache.



torch.backends.cuda.cufft_plan_cache.
A  that controls the capacity of a cuFFT plan cache.



torch.backends.cuda.cufft_plan_cache.
Clears a cuFFT plan cache.

cufft_plan_cache contains the cuFFT plan caches for each CUDA device.
Query a specific device ’s cache via torch.backends.cuda.cufft_plan_cache[i].

torch.backends.cuda.cufft_plan_cache.
A readonly  that shows the number of plans currently in a cuFFT plan cache.

A readonly  that shows the number of plans currently in a cuFFT plan cache.

torch.backends.cuda.cufft_plan_cache.
A  that controls the capacity of a cuFFT plan cache.

A  that controls the capacity of a cuFFT plan cache.

================================================================================

# torch.backends - torch.backends.cuda (Part 3)

torch.backends.cuda.cufft_plan_cache.
Clears a cuFFT plan cache.

Clears a cuFFT plan cache.

torch.backends.cuda.preferred_blas_library
Override the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].


This flag is experimental and subject to change.

When PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available.
For PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance.
This flag (a ) allows overriding which BLAS library to use.

If  is set then cuBLAS will be used wherever possible.
If “cublaslt” is set then cuBLASLt will be used wherever possible.
If  is set then CK will be used wherever possible.
If  (the default) is set then heuristics will be used to pick between the other options.
When no input is given, this function returns the currently preferred library.
User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.

================================================================================

# torch.backends - torch.backends.cuda (Part 4)

Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s library selection is incorrect
for your application’s inputs.

Return type
_BlasBackend

Override the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].

This flag is experimental and subject to change.

When PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available.
For PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance.
This flag (a ) allows overriding which BLAS library to use.

================================================================================

# torch.backends - torch.backends.cuda (Part 5)

List:
If  is set then cuBLAS will be used wherever possible.
If “cublaslt” is set then cuBLASLt will be used wherever possible.
If  is set then CK will be used wherever possible.
If  (the default) is set then heuristics will be used to pick between the other options.
When no input is given, this function returns the currently preferred library.
User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.

If  is set then cuBLAS will be used wherever possible.

If “cublaslt” is set then cuBLASLt will be used wherever possible.

If  is set then CK will be used wherever possible.

If  (the default) is set then heuristics will be used to pick between the other options.

When no input is given, this function returns the currently preferred library.

================================================================================

# torch.backends - torch.backends.cuda (Part 6)

User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.

Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s library selection is incorrect
for your application’s inputs.

Return type
_BlasBackend

torch.backends.cuda.preferred_rocm_fa_library
[ROCm-only]
Override the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK


This flag is experimeental and subject to change.

When Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend.
This flag (a ) allows users to override this backend to use composable_kernel

================================================================================

# torch.backends - torch.backends.cuda (Part 7)

If  is set then the default backend will be used wherever possible. Currently AOTriton.
If “aotriton” is set then AOTriton will be used wherever possible.
If  is set then CK will be used wherever possible.
When no input is given, this function returns the currently preferred library.
User may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK
globally.

Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s library selection is incorrect
for your application’s inputs.

Return type
_ROCmFABackend

[ROCm-only]
Override the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK

This flag is experimeental and subject to change.

When Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend.
This flag (a ) allows users to override this backend to use composable_kernel

================================================================================

# torch.backends - torch.backends.cuda (Part 8)

List:
If  is set then the default backend will be used wherever possible. Currently AOTriton.
If “aotriton” is set then AOTriton will be used wherever possible.
If  is set then CK will be used wherever possible.
When no input is given, this function returns the currently preferred library.
User may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK
globally.

If  is set then the default backend will be used wherever possible. Currently AOTriton.

If “aotriton” is set then AOTriton will be used wherever possible.

If  is set then CK will be used wherever possible.

When no input is given, this function returns the currently preferred library.

User may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK
globally.

Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s library selection is incorrect
for your application’s inputs.

Return type
_ROCmFABackend

================================================================================

# torch.backends - torch.backends.cuda (Part 9)

torch.backends.cuda.preferred_linalg_library
Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.


This flag is experimental and subject to change.

When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries,
and if both are available it decides which to use with a heuristic.
This flag (a ) allows overriding those heuristics.

If “cusolver” is set then cuSOLVER will be used wherever possible.
If  is set then MAGMA will be used wherever possible.
If  (the default) is set then heuristics will be used to pick between
cuSOLVER and MAGMA if both are available.
When no input is given, this function returns the currently preferred library.
User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.

================================================================================

# torch.backends - torch.backends.cuda (Part 10)

Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect
for your application’s inputs.
Currently supported linalg operators:

torch.linalg.inv()
torch.linalg.inv_ex()
torch.linalg.cholesky()
torch.linalg.cholesky_ex()
torch.cholesky_solve()
torch.cholesky_inverse()
torch.linalg.lu_factor()
torch.linalg.lu()
torch.linalg.lu_solve()
torch.linalg.qr()
torch.linalg.eigh()
torch.linalg.eighvals()
torch.linalg.svd()
torch.linalg.svdvals()


Return type
_LinalgBackend

Override the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.

This flag is experimental and subject to change.

When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries,
and if both are available it decides which to use with a heuristic.
This flag (a ) allows overriding those heuristics.

================================================================================

# torch.backends - torch.backends.cuda (Part 11)

List:
If “cusolver” is set then cuSOLVER will be used wherever possible.
If  is set then MAGMA will be used wherever possible.
If  (the default) is set then heuristics will be used to pick between
cuSOLVER and MAGMA if both are available.
When no input is given, this function returns the currently preferred library.
User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.

If “cusolver” is set then cuSOLVER will be used wherever possible.

If  is set then MAGMA will be used wherever possible.

If  (the default) is set then heuristics will be used to pick between
cuSOLVER and MAGMA if both are available.

When no input is given, this function returns the currently preferred library.

User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER
globally.
This flag only sets the initial value of the preferred library and the preferred library
may still be overridden by this function call later in your script.

================================================================================

# torch.backends - torch.backends.cuda (Part 12)

Note: When a library is preferred other libraries may still be used if the preferred library
doesn’t implement the operation(s) called.
This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect
for your application’s inputs.

Currently supported linalg operators:

List:
torch.linalg.inv()
torch.linalg.inv_ex()
torch.linalg.cholesky()
torch.linalg.cholesky_ex()
torch.cholesky_solve()
torch.cholesky_inverse()
torch.linalg.lu_factor()
torch.linalg.lu()
torch.linalg.lu_solve()
torch.linalg.qr()
torch.linalg.eigh()
torch.linalg.eighvals()
torch.linalg.svd()
torch.linalg.svdvals()

torch.linalg.inv_ex()

torch.linalg.cholesky()

torch.linalg.cholesky_ex()

torch.cholesky_solve()

torch.cholesky_inverse()

torch.linalg.lu_factor()

torch.linalg.lu_solve()

torch.linalg.eighvals()

torch.linalg.svdvals()

Return type
_LinalgBackend

torch.backends.cuda.SDPAParams

torch.backends.cuda.flash_sdp_enabled


This flag is beta and subject to change.

Returns whether flash scaled dot product attention is enabled or not.

This flag is beta and subject to change.

Returns whether flash scaled dot product attention is enabled or not.

torch.backends.cuda.enable_mem_efficient_sdp

================================================================================

# torch.backends - torch.backends.cuda (Part 13)


This flag is beta and subject to change.

Enables or disables memory efficient scaled dot product attention.

This flag is beta and subject to change.

Enables or disables memory efficient scaled dot product attention.

torch.backends.cuda.mem_efficient_sdp_enabled


This flag is beta and subject to change.

Returns whether memory efficient scaled dot product attention is enabled or not.

This flag is beta and subject to change.

Returns whether memory efficient scaled dot product attention is enabled or not.

torch.backends.cuda.enable_flash_sdp


This flag is beta and subject to change.

Enables or disables flash scaled dot product attention.

This flag is beta and subject to change.

Enables or disables flash scaled dot product attention.

torch.backends.cuda.math_sdp_enabled


This flag is beta and subject to change.

Returns whether math scaled dot product attention is enabled or not.

This flag is beta and subject to change.

Returns whether math scaled dot product attention is enabled or not.

torch.backends.cuda.enable_math_sdp


This flag is beta and subject to change.

Enables or disables math scaled dot product attention.

This flag is beta and subject to change.

================================================================================

# torch.backends - torch.backends.cuda (Part 14)

Enables or disables math scaled dot product attention.

torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed


This flag is beta and subject to change.

Returns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not.

This flag is beta and subject to change.

Returns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not.

torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp


This flag is beta and subject to change.

Enables or disables fp16/bf16 reduction in math scaled dot product attention.

This flag is beta and subject to change.

Enables or disables fp16/bf16 reduction in math scaled dot product attention.

torch.backends.cuda.cudnn_sdp_enabled


This flag is beta and subject to change.

Returns whether cuDNN scaled dot product attention is enabled or not.

This flag is beta and subject to change.

Returns whether cuDNN scaled dot product attention is enabled or not.

torch.backends.cuda.enable_cudnn_sdp


This flag is beta and subject to change.

Enables or disables cuDNN scaled dot product attention.

This flag is beta and subject to change.

Enables or disables cuDNN scaled dot product attention.

================================================================================

# torch.backends - torch.backends.cuda (Part 15)

torch.backends.cuda.is_flash_attention_available
Check if PyTorch was built with FlashAttention for scaled_dot_product_attention.


True if FlashAttention is built and available; otherwise, False.

Return type





This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

Check if PyTorch was built with FlashAttention for scaled_dot_product_attention.

True if FlashAttention is built and available; otherwise, False.

Return type

True if FlashAttention is built and available; otherwise, False.

This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

torch.backends.cuda.can_use_flash_attention, 
Check if FlashAttention can be utilized in scaled_dot_product_attention.

Parameters

 (_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn debug information as to why FlashAttention could not be run.
Defaults to False.



True if FlashAttention can be used with the given parameters; otherwise, False.

Return type





================================================================================

# torch.backends - torch.backends.cuda (Part 16)

This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

Check if FlashAttention can be utilized in scaled_dot_product_attention.

Parameters

 (_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn debug information as to why FlashAttention could not be run.
Defaults to False.



True if FlashAttention can be used with the given parameters; otherwise, False.

Return type

List:
(_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn debug information as to why FlashAttention could not be run.
Defaults to False.

(_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.

() – Whether to logging.warn debug information as to why FlashAttention could not be run.
Defaults to False.

================================================================================

# torch.backends - torch.backends.cuda (Part 17)

True if FlashAttention can be used with the given parameters; otherwise, False.

This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

torch.backends.cuda.can_use_efficient_attention, 
Check if efficient_attention can be utilized in scaled_dot_product_attention.

Parameters

 (_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn with information as to why efficient_attention could not be run.
Defaults to False.



True if efficient_attention can be used with the given parameters; otherwise, False.

Return type





This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

Check if efficient_attention can be utilized in scaled_dot_product_attention.

Parameters

================================================================================

# torch.backends - torch.backends.cuda (Part 18)

 (_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn with information as to why efficient_attention could not be run.
Defaults to False.



True if efficient_attention can be used with the given parameters; otherwise, False.

Return type

List:
(_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn with information as to why efficient_attention could not be run.
Defaults to False.

(_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.

() – Whether to logging.warn with information as to why efficient_attention could not be run.
Defaults to False.

True if efficient_attention can be used with the given parameters; otherwise, False.

This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

================================================================================

# torch.backends - torch.backends.cuda (Part 19)

torch.backends.cuda.can_use_cudnn_attention, 
Check if cudnn_attention can be utilized in scaled_dot_product_attention.

Parameters

 (_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn with information as to why cuDNN attention could not be run.
Defaults to False.



True if cuDNN can be used with the given parameters; otherwise, False.

Return type





This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

Check if cudnn_attention can be utilized in scaled_dot_product_attention.

Parameters

 (_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn with information as to why cuDNN attention could not be run.
Defaults to False.



True if cuDNN can be used with the given parameters; otherwise, False.

Return type

================================================================================

# torch.backends - torch.backends.cuda (Part 20)

List:
(_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.
 () – Whether to logging.warn with information as to why cuDNN attention could not be run.
Defaults to False.

(_SDPAParams) – An instance of SDPAParams containing the tensors for query,
key, value, an optional attention mask, dropout rate, and
a flag indicating if the attention is causal.

() – Whether to logging.warn with information as to why cuDNN attention could not be run.
Defaults to False.

True if cuDNN can be used with the given parameters; otherwise, False.

This function is dependent on a CUDA-enabled build of PyTorch. It will return False
in non-CUDA environments.

torch.backends.cuda.sdp_kernelenable_flash, enable_math, enable_mem_efficient, enable_cudnn


This flag is beta and subject to change.

This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention.
Upon exiting the context manager, the previous state of the flags will be restored.

This flag is beta and subject to change.

================================================================================

# torch.backends - torch.backends.cuda (Part 21)

This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention.
Upon exiting the context manager, the previous state of the flags will be restored.

================================================================================

# torch.backends - torch.backends.cudnn (Part 1)

torch.backends.cudnn.
Return the version of cuDNN.

Return the version of cuDNN.

torch.backends.cudnn.is_available
Return a bool indicating if CUDNN is currently available.

Return a bool indicating if CUDNN is currently available.

torch.backends.cudnn.
A  that controls whether cuDNN is enabled.

A  that controls whether cuDNN is enabled.

torch.backends.cudnn.allow_tf32
A  that controls where TensorFloat-32 tensor cores may be used in cuDNN
convolutions on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices.

A  that controls where TensorFloat-32 tensor cores may be used in cuDNN
convolutions on Ampere or newer GPUs. See TensorFloat-32 (TF32) on Ampere (and later) devices.

torch.backends.cudnn.deterministic
A  that, if True, causes cuDNN to only use deterministic convolution algorithms.
See also torch.are_deterministic_algorithms_enabled() and
torch.use_deterministic_algorithms().

A  that, if True, causes cuDNN to only use deterministic convolution algorithms.
See also torch.are_deterministic_algorithms_enabled() and
torch.use_deterministic_algorithms().

================================================================================

# torch.backends - torch.backends.cudnn (Part 2)

torch.backends.cudnn.
A  that, if True, causes cuDNN to benchmark multiple convolution algorithms
and select the fastest.

A  that, if True, causes cuDNN to benchmark multiple convolution algorithms
and select the fastest.

torch.backends.cudnn.benchmark_limit
A  that specifies the maximum number of cuDNN convolution algorithms to try when
torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every
available algorithm. Note that this setting only affects convolutions dispatched via the
cuDNN v8 API.

A  that specifies the maximum number of cuDNN convolution algorithms to try when
torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every
available algorithm. Note that this setting only affects convolutions dispatched via the
cuDNN v8 API.

================================================================================

# torch.backends - torch.backends.cusparselt

torch.backends.cusparselt.
Return the version of cuSPARSELt

Return type
[]

Return the version of cuSPARSELt

torch.backends.cusparselt.is_available
Return a bool indicating if cuSPARSELt is currently available.

Return type

Return a bool indicating if cuSPARSELt is currently available.

================================================================================

# torch.backends - torch.backends.mha

torch.backends.mha.get_fastpath_enabled
Returns whether fast path for TransformerEncoder and MultiHeadAttention
is enabled, or  if jit is scripting.


The fastpath might not be run even if get_fastpath_enabled returns
 unless all conditions on inputs are met.


Return type

Returns whether fast path for TransformerEncoder and MultiHeadAttention
is enabled, or  if jit is scripting.

The fastpath might not be run even if get_fastpath_enabled returns
 unless all conditions on inputs are met.

torch.backends.mha.set_fastpath_enabled
Sets whether fast path is enabled

Sets whether fast path is enabled

================================================================================

# torch.backends - torch.backends.mps

torch.backends.mps.is_available
Return a bool indicating if MPS is currently available.

Return type

Return a bool indicating if MPS is currently available.

torch.backends.mps.
Return whether PyTorch is built with MPS support.
Note that this doesn’t necessarily mean MPS is available; just that
if this PyTorch binary were run a machine with working MPS drivers
and devices, we would be able to use it.

Return type

Return whether PyTorch is built with MPS support.

Note that this doesn’t necessarily mean MPS is available; just that
if this PyTorch binary were run a machine with working MPS drivers
and devices, we would be able to use it.

================================================================================

# torch.backends - torch.backends.mkl (Part 1)

torch.backends.mkl.is_available
Return whether PyTorch is built with MKL support.

Return whether PyTorch is built with MKL support.

torch.backends.mkl.
On-demand oneMKL verbosing functionality.
To make it easier to debug performance issues, oneMKL can dump verbose
messages containing execution information like duration while executing
the kernel. The verbosing functionality can be invoked via an environment
variable named MKL_VERBOSE. However, this methodology dumps messages in
all steps. Those are a large amount of verbose messages. Moreover, for
investigating the performance issues, generally taking verbose messages
for one single iteration is enough. This on-demand verbosing functionality
makes it possible to control scope for verbose message dumping. In the
following example, verbose messages will be dumped out for the second
inference only.
 

 VERBOSE_ON
    



Parameters
 – Verbose level
- VERBOSE_OFF: Disable verbosing
- VERBOSE_ON:  Enable verbosing

On-demand oneMKL verbosing functionality.

================================================================================

# torch.backends - torch.backends.mkl (Part 2)

To make it easier to debug performance issues, oneMKL can dump verbose
messages containing execution information like duration while executing
the kernel. The verbosing functionality can be invoked via an environment
variable named MKL_VERBOSE. However, this methodology dumps messages in
all steps. Those are a large amount of verbose messages. Moreover, for
investigating the performance issues, generally taking verbose messages
for one single iteration is enough. This on-demand verbosing functionality
makes it possible to control scope for verbose message dumping. In the
following example, verbose messages will be dumped out for the second
inference only.

Parameters
 – Verbose level
- VERBOSE_OFF: Disable verbosing
- VERBOSE_ON:  Enable verbosing

– Verbose level
- VERBOSE_OFF: Disable verbosing
- VERBOSE_ON:  Enable verbosing

================================================================================

# torch.backends - torch.backends.mkldnn (Part 1)

torch.backends.mkldnn.is_available

torch.backends.mkldnn.
On-demand oneDNN (former MKL-DNN) verbosing functionality.
To make it easier to debug performance issues, oneDNN can dump verbose
messages containing information like kernel size, input data size and
execution duration while executing the kernel. The verbosing functionality
can be invoked via an environment variable named DNNL_VERBOSE. However,
this methodology dumps messages in all steps. Those are a large amount of
verbose messages. Moreover, for investigating the performance issues,
generally taking verbose messages for one single iteration is enough.
This on-demand verbosing functionality makes it possible to control scope
for verbose message dumping. In the following example, verbose messages
will be dumped out for the second inference only.
 

 VERBOSE_ON
    



Parameters
 – Verbose level
- VERBOSE_OFF: Disable verbosing
- VERBOSE_ON:  Enable verbosing
- VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation

On-demand oneDNN (former MKL-DNN) verbosing functionality.

================================================================================

# torch.backends - torch.backends.mkldnn (Part 2)

To make it easier to debug performance issues, oneDNN can dump verbose
messages containing information like kernel size, input data size and
execution duration while executing the kernel. The verbosing functionality
can be invoked via an environment variable named DNNL_VERBOSE. However,
this methodology dumps messages in all steps. Those are a large amount of
verbose messages. Moreover, for investigating the performance issues,
generally taking verbose messages for one single iteration is enough.
This on-demand verbosing functionality makes it possible to control scope
for verbose message dumping. In the following example, verbose messages
will be dumped out for the second inference only.

Parameters
 – Verbose level
- VERBOSE_OFF: Disable verbosing
- VERBOSE_ON:  Enable verbosing
- VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation

– Verbose level
- VERBOSE_OFF: Disable verbosing
- VERBOSE_ON:  Enable verbosing
- VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation

================================================================================

# torch.backends - torch.backends.nnpack

torch.backends.nnpack.is_available
Return whether PyTorch is built with NNPACK support.

Return whether PyTorch is built with NNPACK support.

torch.backends.nnpack.
Context manager for setting if nnpack is enabled globally

Context manager for setting if nnpack is enabled globally

torch.backends.nnpack.
Set if nnpack is enabled globally

Set if nnpack is enabled globally

================================================================================

# torch.backends - torch.backends.openmp

torch.backends.openmp.is_available
Return whether PyTorch is built with OpenMP support.

Return whether PyTorch is built with OpenMP support.

================================================================================

# torch.backends - torch.backends.opt_einsum (Part 1)

torch.backends.opt_einsum.is_available
Return a bool indicating if opt_einsum is currently available.
You must install opt-einsum in order for torch to automatically optimize einsum. To
make opt-einsum available, you can install it along with torch:   torch[opt-einsum]
or by itself:   opt-einsum. If the package is installed, torch will import
it automatically and use it accordingly. Use this function to check whether opt-einsum
was installed and properly imported by torch.

Return type

Return a bool indicating if opt_einsum is currently available.

You must install opt-einsum in order for torch to automatically optimize einsum. To
make opt-einsum available, you can install it along with torch:   torch[opt-einsum]
or by itself:   opt-einsum. If the package is installed, torch will import
it automatically and use it accordingly. Use this function to check whether opt-einsum
was installed and properly imported by torch.

torch.backends.opt_einsum.get_opt_einsum
Return the opt_einsum package if opt_einsum is currently available, else None.

Return type

Return the opt_einsum package if opt_einsum is currently available, else None.

================================================================================

# torch.backends - torch.backends.opt_einsum (Part 2)

torch.backends.opt_einsum.
A  that controls whether opt_einsum is enabled ( by default). If so,
torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)
if available to calculate an optimal path of contraction for faster performance.
If opt_einsum is not available, torch.einsum will fall back to the default contraction path
of left to right.

A  that controls whether opt_einsum is enabled ( by default). If so,
torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)
if available to calculate an optimal path of contraction for faster performance.

If opt_einsum is not available, torch.einsum will fall back to the default contraction path
of left to right.

torch.backends.opt_einsum.
A  that specifies which strategies to try when torch.backends.opt_einsum.enabled
is . By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal”
strategies are also supported. Note that the “optimal” strategy is factorial on the number of
inputs as it tries all possible paths. See more details in opt_einsum’s docs
(https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).

================================================================================

# torch.backends - torch.backends.opt_einsum (Part 3)

A  that specifies which strategies to try when torch.backends.opt_einsum.enabled
is . By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal”
strategies are also supported. Note that the “optimal” strategy is factorial on the number of
inputs as it tries all possible paths. See more details in opt_einsum’s docs
(https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).

================================================================================

# torch.backends - torch.backends.xeon

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.export

Created On: Jun 12, 2025 | Last Updated On: Jun 12, 2025

This feature is a prototype under active development and there WILL BE
BREAKING CHANGES in the future.

================================================================================

# torch.export - Overview (Part 1)

torch.export.export() takes a torch.nn.Module and produces a traced graph
representing only the Tensor computation of the function in an Ahead-of-Time
(AOT) fashion, which can subsequently be executed with different outputs or
serialized.

Code example:
torch.export  

 
           
          
          
           

example_args     

exported_program ExportedProgram  
     example_args

exported_program

Code example:
ExportedProgram
     GraphModule
           "f32[10, 10]"  "f32[10, 10]"
            # code: a = torch.sin(x)
             "f32[10, 10]"  

            # code: b = torch.cos(y)
             "f32[10, 10]"  

            # code: return a + b
                 
             

================================================================================

# torch.export - Overview (Part 2)

     
        ExportGraphSignature
            input_specs
                
                    USER_INPUT 
                    TensorArgument
                    
                    persistent
                
                
                    USER_INPUT 
                    TensorArgument
                    
                    persistent
                
            
            output_specs
                OutputSpec
                    OutputKindUSER_OUTPUT 
                    TensorArgument
                    
                
            
        
     constraints

torch.export produces a clean intermediate representation (IR) with the
following invariants. More specifications about the IR can be found
.

================================================================================

# torch.export - Overview (Part 3)

List:
: It is guaranteed to be a sound representation of the original
program, and maintains the same calling conventions of the original program.
Normalized: There are no Python semantics within the graph. Submodules
from the original programs are inlined to form one fully flattened
computational graph.
Graph properties: The graph is purely functional, meaning it does not
contain operations with side effects such as mutations or aliasing. It does
not mutate any intermediate values, parameters, or buffers.
: The graph contains metadata captured during tracing, such as a
stacktrace from user’s code.

: It is guaranteed to be a sound representation of the original
program, and maintains the same calling conventions of the original program.

Normalized: There are no Python semantics within the graph. Submodules
from the original programs are inlined to form one fully flattened
computational graph.

Graph properties: The graph is purely functional, meaning it does not
contain operations with side effects such as mutations or aliasing. It does
not mutate any intermediate values, parameters, or buffers.

================================================================================

# torch.export - Overview (Part 4)

: The graph contains metadata captured during tracing, such as a
stacktrace from user’s code.

Under the hood, torch.export leverages the following latest technologies:

List:
TorchDynamo (torch._dynamo) is an internal API that uses a CPython feature
called the Frame Evaluation API to safely trace PyTorch graphs. This
provides a massively improved graph capturing experience, with much fewer
rewrites needed in order to fully trace the PyTorch code.
AOT Autograd provides a functionalized PyTorch graph and ensures the graph
is decomposed/lowered to the ATen operator set.
Torch FX (torch.fx) is the underlying representation of the graph,
allowing flexible Python-based transformations.

TorchDynamo (torch._dynamo) is an internal API that uses a CPython feature
called the Frame Evaluation API to safely trace PyTorch graphs. This
provides a massively improved graph capturing experience, with much fewer
rewrites needed in order to fully trace the PyTorch code.

AOT Autograd provides a functionalized PyTorch graph and ensures the graph
is decomposed/lowered to the ATen operator set.

Torch FX (torch.fx) is the underlying representation of the graph,
allowing flexible Python-based transformations.

================================================================================

# torch.export - Existing frameworks (Part 1)

torch.compile() also utilizes the same PT2 stack as torch.export, but
is slightly different:

List:
JIT vs. AOT: torch.compile() is a JIT compiler whereas
which is not intended to be used to produce compiled artifacts outside of
deployment.
Partial vs. Full Graph Capture: When torch.compile() runs into an
untraceable part of a model, it will “graph break” and fall back to running
the program in the eager Python runtime. In comparison, torch.export aims
to get a full graph representation of a PyTorch model, so it will error out
when something untraceable is reached. Since torch.export produces a full
graph disjoint from any Python features or runtime, this graph can then be
saved, loaded, and run in different environments and languages.
Usability tradeoff: Since torch.compile() is able to fallback to the
Python runtime whenever it reaches something untraceable, it is a lot more
flexible. torch.export will instead require users to provide more
information or rewrite their code to make it traceable.

JIT vs. AOT: torch.compile() is a JIT compiler whereas
which is not intended to be used to produce compiled artifacts outside of
deployment.

================================================================================

# torch.export - Existing frameworks (Part 2)

Partial vs. Full Graph Capture: When torch.compile() runs into an
untraceable part of a model, it will “graph break” and fall back to running
the program in the eager Python runtime. In comparison, torch.export aims
to get a full graph representation of a PyTorch model, so it will error out
when something untraceable is reached. Since torch.export produces a full
graph disjoint from any Python features or runtime, this graph can then be
saved, loaded, and run in different environments and languages.

Usability tradeoff: Since torch.compile() is able to fallback to the
Python runtime whenever it reaches something untraceable, it is a lot more
flexible. torch.export will instead require users to provide more
information or rewrite their code to make it traceable.

================================================================================

# torch.export - Existing frameworks (Part 3)

Compared to torch.fx.symbolic_trace(), torch.export traces using
TorchDynamo which operates at the Python bytecode level, giving it the ability
to trace arbitrary Python constructs not limited by what Python operator
overloading supports. Additionally, torch.export keeps fine-grained track of
tensor metadata, so that conditionals on things like tensor shapes do not
fail tracing. In general, torch.export is expected to work on more user
programs, and produce lower-level graphs (at the torch.ops.aten operator
level). Note that users can still use torch.fx.symbolic_trace() as a
preprocessing step before torch.export.

Compared to torch.jit.script(), torch.export does not capture Python
control flow or data structures, but it supports more Python language features
than TorchScript (as it is easier to have comprehensive coverage over Python
bytecodes). The resulting graphs are simpler and only have straight line control
flow (except for explicit control flow operators).

================================================================================

# torch.export - Existing frameworks (Part 4)

Compared to torch.jit.trace(), torch.export is sound: it is able to
trace code that performs integer computation on sizes and records all of the
side-conditions necessary to show that a particular trace is valid for other
inputs.

================================================================================

# torch.export - An Example (Part 1)

The main entrypoint is through torch.export.export(), which takes a
callable (torch.nn.Module, function, or method) and sample inputs, and
captures the computation graph into an torch.export.ExportedProgram. An
example:

Code example:
torch.export  

# Simple module for demonstration
 
       
        
          
            in_channels out_channels kernel_size 
        
          
          kernel_size

           
          
        
         

example_args     
example_kwargs  "constant"    

exported_program ExportedProgram  
     example_args example_kwargs

exported_program

Code example:
ExportedProgram
     GraphModule
      p_conv_weight "f32[16, 3, 3, 3]" p_conv_bias   "f32[1, 3, 256, 256]"  "f32[1, 16, 256, 256]"
            # code: a = self.conv(x)
             "f32[1, 16, 256, 256]"   p_conv_weight p_conv_bias    

            # code: a.add_(constant)
             "f32[1, 16, 256, 256]"   

            # code: return self.maxpool(self.relu(a))
             "f32[1, 16, 256, 256]"  
            max_pool2d "f32[1, 16, 85, 85]"  max_pool2d    
             max_pool2d

================================================================================

# torch.export - An Example (Part 2)

 
    ExportGraphSignature
        input_specs
            
                 
                TensorArgument'p_conv_weight'
                'conv.weight'
                persistent
            
            
                 
                TensorArgument'p_conv_bias'
                'conv.bias'
                persistent
            
            
                USER_INPUT 
                TensorArgument
                
                persistent
            
            
                USER_INPUT 
                TensorArgument'constant'
                
                persistent
            
        
        output_specs
            OutputSpec
                OutputKindUSER_OUTPUT 
                TensorArgument'max_pool2d'
                
            
        
    
 constraints

Inspecting the ExportedProgram, we can note the following:

================================================================================

# torch.export - An Example (Part 3)

List:
The torch.fx.Graph contains the computation graph of the original
program, along with records of the original code for easy debugging.
The graph contains only torch.ops.aten operators found 
and custom operators, and is fully functional, without any inplace operators
such as torch.add_.
The parameters (weight and bias to conv) are lifted as inputs to the graph,
resulting in no  nodes in the graph, which previously existed in
the result of torch.fx.symbolic_trace().
The torch.export.ExportGraphSignature models the input and output
signature, along with specifying which inputs are parameters.
The resulting shape and dtype of tensors produced by each node in the graph is
noted. For example, the convolution node will result in a tensor of dtype
torch.float32 and shape (1, 16, 256, 256).

The torch.fx.Graph contains the computation graph of the original
program, along with records of the original code for easy debugging.

The graph contains only torch.ops.aten operators found 
and custom operators, and is fully functional, without any inplace operators
such as torch.add_.

================================================================================

# torch.export - An Example (Part 4)

The parameters (weight and bias to conv) are lifted as inputs to the graph,
resulting in no  nodes in the graph, which previously existed in
the result of torch.fx.symbolic_trace().

The torch.export.ExportGraphSignature models the input and output
signature, along with specifying which inputs are parameters.

The resulting shape and dtype of tensors produced by each node in the graph is
noted. For example, the convolution node will result in a tensor of dtype
torch.float32 and shape (1, 16, 256, 256).

================================================================================

# torch.export - Non-Strict Export (Part 1)

In PyTorch 2.3, we introduced a new mode of tracing called non-strict mode.
It’s still going through hardening, so if you run into any issues, please file
them to Github with the “oncall: export” tag.

In non-strict mode, we trace through the program using the Python interpreter.
Your code will execute exactly as it would in eager mode; the only difference is
that all Tensor objects will be replaced by ProxyTensors, which will record all
their operations into a graph.

In  mode, which is currently the default, we first trace through the
program using TorchDynamo, a bytecode analysis engine. TorchDynamo does not
actually execute your Python code. Instead, it symbolically analyzes it and
builds a graph based on the results. This analysis allows torch.export to
provide stronger guarantees about safety, but not all Python code is supported.

An example of a case where one might want to use non-strict mode is if you run
into a unsupported TorchDynamo feature that might not be easily solved, and you
know the python code is not exactly needed for computation. For example:

Code example:
contextlib
 

 ContextManager
     
          
     
          
        
          

================================================================================

# torch.export - Non-Strict Export (Part 2)

 
      
         ContextManager
               

     # Non-strict traces successfully
    # Strict mode fails with torch._dynamo.exc.Unsupported: ContextManager

In this example, the first call using non-strict mode (through the
strict=False flag) traces successfully whereas the second call using strict
mode (default) results with a failure, where TorchDynamo is unable to support
context managers. One option is to rewrite the code (see Limitations of torch.export),
but seeing as the context manager does not affect the tensor
computations in the model, we can go with the non-strict mode’s result.

================================================================================

# torch.export - Export for Training and Inference (Part 1)

In PyTorch 2.5, we introduced a new API called export_for_training().
It’s still going through hardening, so if you run into any issues, please file
them to Github with the “oncall: export” tag.

In this API, we produce the most generic IR that contains all ATen operators
(including both functional and non-functional) which can be used to train in
eager PyTorch Autograd. This API is intended for eager training use cases such as PT2 Quantization
and will soon be the default IR of torch.export.export. To read further about
the motivation behind this change, please refer to
https://dev-discuss.pytorch.org/t/why-pytorch-does-not-need-a-new-standardized-operator-set/2206

When this API is combined with run_decompositions(), you should be able to get inference IR with
any desired decomposition behavior.

To show some examples:

Code example:
ConvBatchnorm
       
        
             
          BatchNorm2d

      
          
          
         

  ConvBatchnorm
     

ep_for_training  export_for_training 
ep_for_training

================================================================================

# torch.export - Export for Training and Inference (Part 2)

Code example:
ExportedProgram
     GraphModule
          p_conv_weight "f32[3, 1, 1, 1]" p_conv_bias  p_bn_weight    b_bn_running_mean  b_bn_running_var  b_bn_num_batches_tracked   "f32[1, 1, 3, 3]"
             "f32[1, 3, 3, 3]"   p_conv_weight p_conv_bias
               b_bn_num_batches_tracked 
            batch_norm "f32[1, 3, 3, 3]"  batch_norm p_bn_weight  b_bn_running_mean b_bn_running_var    
             batch_norm

From the above output, you can see that export_for_training() produces pretty much the same ExportedProgram
as  except for the operators in the graph. You can see that we captured batch_norm in the most general
form. This op is non-functional and will be lowered to different ops when running inference.

You can also go from this IR to an inference IR via run_decompositions() with arbitrary customizations.

Code example:
# Lower to core aten inference IR, but keep conv2d
decomp_table  default_decompositions
 decomp_table
ep_for_inference  ep_for_trainingrun_decompositionsdecomp_table

ep_for_inference

================================================================================

# torch.export - Export for Training and Inference (Part 3)

Code example:
ExportedProgram
     GraphModule
          p_conv_weight "f32[3, 1, 1, 1]" p_conv_bias  p_bn_weight    b_bn_running_mean  b_bn_running_var  b_bn_num_batches_tracked   "f32[1, 1, 3, 3]"
             "f32[1, 3, 3, 3]"   p_conv_weight p_conv_bias
               b_bn_num_batches_tracked 
            _native_batch_norm_legit_functional  _native_batch_norm_legit_functional p_bn_weight  b_bn_running_mean b_bn_running_var   
             "f32[1, 3, 3, 3]"  _native_batch_norm_legit_functional
               _native_batch_norm_legit_functional
               _native_batch_norm_legit_functional

Here you can see that we kept  op in the IR while decomposing the rest. Now the IR is a functional IR
containing core aten operators except for .

You can do even more customization by directly registering your chosen decomposition behaviors.

You can do even more customizations by directly registering custom decomp behaviour

Code example:
# Lower to core aten inference IR, but customize conv2d
decomp_table  default_decompositions

 my_awesome_custom_conv2d_function         
       convolution         

================================================================================

# torch.export - Export for Training and Inference (Part 4)

decomp_table  my_awesome_conv2d_function
ep_for_inference  ep_for_trainingrun_decompositionsdecomp_table

ep_for_inference

Code example:
ExportedProgram
     GraphModule
          p_conv_weight "f32[3, 1, 1, 1]" p_conv_bias  p_bn_weight    b_bn_running_mean  b_bn_running_var  b_bn_num_batches_tracked   "f32[1, 1, 3, 3]"
            convolution "f32[1, 3, 3, 3]"  convolution p_conv_weight p_conv_bias          
             "f32[1, 3, 3, 3]"  convolution 
               b_bn_num_batches_tracked 
            _native_batch_norm_legit_functional  _native_batch_norm_legit_functional p_bn_weight  b_bn_running_mean b_bn_running_var   
             "f32[1, 3, 3, 3]"  _native_batch_norm_legit_functional
               _native_batch_norm_legit_functional
               _native_batch_norm_legit_functional

================================================================================

# torch.export - Expressing Dynamism (Part 1)

By default torch.export will trace the program assuming all input shapes are
, and specializing the exported program to those dimensions. However,
some dimensions, such as a batch dimension, can be dynamic and vary from run to
run. Such dimensions must be specified by using the
torch.export.Dim() API to create them and by passing them into
torch.export.export() through the dynamic_shapes argument. An example:

Code example:
torch.export   

 
     
        

          Sequential
              
        
          Sequential
              
        
          

       
          
          
            

example_args     

# Create a dynamic batch size
  
# Specify that the first dimension of each input is that batch size
dynamic_shapes       

exported_program ExportedProgram  
     example_args dynamic_shapesdynamic_shapes

exported_program

Code example:
ExportedProgram
 GraphModule
      p_branch1_0_weight "f32[32, 64]" p_branch1_0_bias  p_branch2_0_weight "f32[64, 128]" p_branch2_0_bias     "f32[s0, 64]"  "f32[s0, 128]"

         # code: out1 = self.branch1(x1)
         "f32[s0, 32]"   p_branch1_0_weight p_branch1_0_bias
         "f32[s0, 32]"  

================================================================================

# torch.export - Expressing Dynamism (Part 2)

         # code: out2 = self.branch2(x2)
         "f32[s0, 64]"   p_branch2_0_weight p_branch2_0_bias
         "f32[s0, 64]"  

         # code: return (out1 + self.buffer, out2)
         "f32[s0, 32]"   
          

 constraints

Some additional things to note:

List:
Through the torch.export.Dim() API and the dynamic_shapes argument, we specified the first
dimension of each input to be dynamic. Looking at the inputs  and
, they have a symbolic shape of (s0, 64) and (s0, 128), instead of
the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs.
 is a symbol representing that this dimension can be a range
of values.
exported_program.range_constraints describes the ranges of each symbol
appearing in the graph. In this case, we see that  has the range
[0, int_oo]. For technical reasons that are difficult to explain here, they are
assumed to be not 0 or 1. This is not a bug, and does not necessarily mean
that the exported program will not work for dimensions 0 or 1. See
The 0/1 Specialization Problem
for an in-depth discussion of this topic.

================================================================================

# torch.export - Expressing Dynamism (Part 3)

Through the torch.export.Dim() API and the dynamic_shapes argument, we specified the first
dimension of each input to be dynamic. Looking at the inputs  and
, they have a symbolic shape of (s0, 64) and (s0, 128), instead of
the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs.
 is a symbol representing that this dimension can be a range
of values.

exported_program.range_constraints describes the ranges of each symbol
appearing in the graph. In this case, we see that  has the range
[0, int_oo]. For technical reasons that are difficult to explain here, they are
assumed to be not 0 or 1. This is not a bug, and does not necessarily mean
that the exported program will not work for dimensions 0 or 1. See
The 0/1 Specialization Problem
for an in-depth discussion of this topic.

We can also specify more expressive relationships between input shapes, such as
where a pair of shapes might differ by one, a shape might be double of
another, or a shape is even. An example:

Code example:
exported_program  
       dynamic_shapes   

exported_program

================================================================================

# torch.export - Expressing Dynamism (Part 4)

Code example:
ExportedProgram
 GraphModule
         "f32[s0 + 1]"
        # code: return x + y[1:]
              9223372036854775807
            
         

 constraints

List:
By specifying   for the first input, we see that the resulting
shape of the first input is now dynamic, being . And now by specifying
  for the second input, we see that the resulting shape of the
second input is also dynamic. However, because we expressed     ,
instead of ’s shape containing a new symbol, we see that it is
now being represented with the same symbol used in , . We can
see that relationship of      is being shown through   .
Looking at the range constraints, we see that  has the range [3, 6],
which is specified initially, and we can see that    has the solved
range of [4, 7].

================================================================================

# torch.export - Expressing Dynamism (Part 5)

By specifying   for the first input, we see that the resulting
shape of the first input is now dynamic, being . And now by specifying
  for the second input, we see that the resulting shape of the
second input is also dynamic. However, because we expressed     ,
instead of ’s shape containing a new symbol, we see that it is
now being represented with the same symbol used in , . We can
see that relationship of      is being shown through   .

Looking at the range constraints, we see that  has the range [3, 6],
which is specified initially, and we can see that    has the solved
range of [4, 7].

================================================================================

# torch.export - Serialization

To save the ExportedProgram, users can use the torch.export.save() and
torch.export.load() APIs. A convention is to save the ExportedProgram
using a  file extension.

Code example:
exported_program   

exported_program 'exported_program.pt2'
saved_exported_program  'exported_program.pt2'

================================================================================

# torch.export - Specializations

A key concept in understanding the behavior of torch.export is the
difference between  and  values.

A  value is one that can change from run to run. These behave like
normal arguments to a Python function—you can pass different values for an
argument and expect your function to do the right thing. Tensor  is
treated as dynamic.

A  value is a value that is fixed at export time and cannot change
between executions of the exported program. When the value is encountered during
tracing, the exporter will treat it as a constant and hard-code it into the
graph.

When an operation is performed (e.g.   ) and all inputs are static, then
the output of the operation will be directly hard-coded into the graph, and the
operation won’t show up (i.e. it will get constant-folded).

When a value has been hard-coded into the graph, we say that the graph has been
specialized to that value.

The following values are static:

================================================================================

# torch.export - Input Tensor Shapes (Part 1)

By default, torch.export will trace the program specializing on the input
tensors’ shapes, unless a dimension is specified as dynamic via the
dynamic_shapes argument to torch.export. This means that if there exists
shape-dependent control flow, torch.export will specialize on the branch
that is being taken with the given sample inputs. For example:

Code example:
torch.export  

 
      
           
               
        
               

example_inputs   
exported_program   example_inputs
exported_program

Code example:
ExportedProgram
 GraphModule
       "f32[10, 2]"
        # code: return x + 1
         "f32[10, 2]"

The conditional of (x.shape[0]  ) does not appear in the
ExportedProgram because the example inputs have the static
shape of (10, 2). Since torch.export specializes on the inputs’ static
shapes, the else branch (  ) will never be reached. To preserve the dynamic
branching behavior based on the shape of a tensor in the traced graph,
torch.export.Dim() will need to be used to specify the dimension
of the input tensor (x.shape[0]) to be dynamic, and the source code will
need to be .

================================================================================

# torch.export - Input Tensor Shapes (Part 2)

Note that tensors that are part of the module state (e.g. parameters and
buffers) always have static shapes.

================================================================================

# torch.export - Python Primitives

torch.export also specializes on Python primitives,
such as , , , and . However they do have dynamic
variants such as , , and .

Code example:
torch.export  

 
           
           
                
         

example_inputs     
exported_program   example_inputs
exported_program

Code example:
ExportedProgram
     GraphModule
           "f32[2, 2]"  
            # code: x = x + const
             "f32[2, 2]"   
             "f32[2, 2]"   
             "f32[2, 2]"

Because integers are specialized, the torch.ops.aten.add.Tensor operations
are all computed with the hard-coded constant , rather than . If
a user passes a different value for  at runtime, like 2, than the one used
during export time, 1, this will result in an error.
Additionally, the  iterator used in the  loop is also “inlined”
in the graph through the 3 repeated torch.ops.aten.add.Tensor calls, and the
input  is never used.

================================================================================

# torch.export - Python Containers

Python containers (, , NamedTuple, etc.) are considered to
have static structure.

================================================================================

# torch.export - Graph Breaks

As torch.export is a one-shot process for capturing a computation graph from
a PyTorch program, it might ultimately run into untraceable parts of programs as
it is nearly impossible to support tracing all PyTorch and Python features. In
the case of torch.compile, an unsupported operation will cause a “graph
break” and the unsupported operation will be run with default Python evaluation.
In contrast, torch.export will require users to provide additional
information or rewrite parts of their code to make it traceable. As the
tracing is based on TorchDynamo, which evaluates at the Python
bytecode level, there will be significantly fewer rewrites required compared to
previous tracing frameworks.

When a graph break is encountered,  is a great
resource for learning about the kinds of programs that are supported and
unsupported, along with ways to rewrite programs to make them traceable.

An option to get past dealing with this graph breaks is by using
non-strict export

================================================================================

# torch.export - Data/Shape-Dependent Control Flow

Graph breaks can also be encountered on data-dependent control flow ( x.shape[0]  ) when shapes are not being specialized, as a tracing compiler cannot
possibly deal with without generating code for a combinatorially exploding
number of paths. In such cases, users will need to rewrite their code using
special control flow operators. Currently, we support torch.cond
to express if-else like control flow (more coming soon!).

================================================================================

# torch.export - Missing Fake/Meta/Abstract Kernels for Operators

When tracing, a FakeTensor kernel (aka meta kernel, abstract impl) is
required for all operators. This is used to reason about the input/output shapes
for this operator.

Please see torch.library.register_fake() for more details.

In the unfortunate case where your model uses an ATen operator that is does not
have a FakeTensor kernel implementation yet, please file an issue.

================================================================================

# torch.export - Read More

Additional Links for Export Users

List:
torch.export Programming Model
torch.export IR Specification
Draft Export
Writing Graph Transformations on ATen IR


Control Flow - Cond

Deep Dive for PyTorch Developers

List:
Dynamo Overview
Dynamo Deep-Dive
Dynamic Shapes
Fake tensor

================================================================================

# torch.export - API Reference (Part 1)

torch.export., , , , dynamic_shapes, , preserve_module_call_signature
 takes any nn.Module along with example inputs, and produces a traced graph representing
only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,
which can subsequently be executed with different inputs or serialized.  The
traced graph (1) produces normalized operators in the functional ATen operator set
(as well as any user-specified custom operators), (2) has eliminated all Python control
flow and data structures (with certain exceptions), and (3) records the set of
shape constraints needed to show that this normalization and control-flow elimination
is sound for future inputs.
Soundness Guarantee
While tracing,  takes note of shape-related assumptions
made by the user program and the underlying PyTorch operator kernels.
The output ExportedProgram is considered valid only when these
assumptions hold true.
Tracing makes assumptions on the shapes (not values) of input tensors.
Such assumptions must be validated at graph capture time for 
to succeed. Specifically:

================================================================================

# torch.export - API Reference (Part 2)

Assumptions on static shapes of input tensors are automatically validated without additional effort.
Assumptions on dynamic shape of input tensors require explicit specification
by using the  API to construct dynamic dimensions and by associating
them with example inputs through the dynamic_shapes argument.

If any assumption can not be validated, a fatal error will be raised. When that happens,
the error message will include suggested fixes to the specification that are needed
to validate the assumptions. For example  might suggest the
following fix to the definition of a dynamic dimension , say appearing in the
shape associated with input , that was previously defined as Dim("dim0_x"):
   


This example means the generated code requires dimension 0 of input  to be less
than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension
definitions and then copy them verbatim into your code without needing to change the
dynamic_shapes argument to your  call.

Parameters

================================================================================

# torch.export - API Reference (Part 3)

 () – We will trace the forward method of this module.
 () – Example positional inputs.
 () – Optional example keyword inputs.
dynamic_shapes () – An optional argument where the type should either be:
1) a dict from argument names of  to their dynamic shape specifications,
2) a tuple that specifies dynamic shape specifications for each input in original order.
If you are specifying dynamism on keyword args, you will need to pass them in the order that
is defined in the original function signature.
The dynamic shape of a tensor argument can be specified as either
(1) a dict from dynamic dimension indices to  types, where it is
not required to include static dimension indices in this dict, but when they are,
they should be mapped to None; or (2) a tuple / list of  types or None,
where the  types correspond to dynamic dimensions, and static dimensions
are denoted by None. Arguments that are dicts or tuples / lists of tensors are
recursively specified by using mappings or sequences of contained specifications.

================================================================================

# torch.export - API Reference (Part 4)

 () – When disabled (default), the export function will trace the program through
Python runtime, which by itself will not validate some of the implicit assumptions
baked into the graph. It will still validate most critical assumptions like shape
safety. When enabled (by setting strict=True), the export function will trace
the program through TorchDynamo which will ensure the soundness of the resulting
graph. TorchDynamo has limited Python feature coverage, thus you may experience more
errors. Note that toggling this argument does not affect the resulting IR spec to be
different and the model will be serialized in the same way regardless of what value
is passed here.
preserve_module_call_signature () – A list of submodule paths for which the original
calling conventions are preserved as metadata. The metadata will be used when calling
torch.export.unflatten to preserve the original calling conventions of modules.



An ExportedProgram containing the traced callable.

Return type
ExportedProgram


Acceptable input/output types
Acceptable types of inputs (for  and ) and outputs include:

================================================================================

# torch.export - API Reference (Part 5)

Primitive types, i.e. torch.Tensor, , ,  and .
Dataclasses, but they must be registered by calling register_dataclass() first.
(Nested) Data structures comprising of , , , namedtuple and
OrderedDict containing all above types.

takes any nn.Module along with example inputs, and produces a traced graph representing
only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,
which can subsequently be executed with different inputs or serialized.  The
traced graph (1) produces normalized operators in the functional ATen operator set
(as well as any user-specified custom operators), (2) has eliminated all Python control
flow and data structures (with certain exceptions), and (3) records the set of
shape constraints needed to show that this normalization and control-flow elimination
is sound for future inputs.

While tracing,  takes note of shape-related assumptions
made by the user program and the underlying PyTorch operator kernels.
The output ExportedProgram is considered valid only when these
assumptions hold true.

================================================================================

# torch.export - API Reference (Part 6)

Tracing makes assumptions on the shapes (not values) of input tensors.
Such assumptions must be validated at graph capture time for 
to succeed. Specifically:

List:
Assumptions on static shapes of input tensors are automatically validated without additional effort.
Assumptions on dynamic shape of input tensors require explicit specification
by using the  API to construct dynamic dimensions and by associating
them with example inputs through the dynamic_shapes argument.

Assumptions on static shapes of input tensors are automatically validated without additional effort.

Assumptions on dynamic shape of input tensors require explicit specification
by using the  API to construct dynamic dimensions and by associating
them with example inputs through the dynamic_shapes argument.

If any assumption can not be validated, a fatal error will be raised. When that happens,
the error message will include suggested fixes to the specification that are needed
to validate the assumptions. For example  might suggest the
following fix to the definition of a dynamic dimension , say appearing in the
shape associated with input , that was previously defined as Dim("dim0_x"):

================================================================================

# torch.export - API Reference (Part 7)

This example means the generated code requires dimension 0 of input  to be less
than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension
definitions and then copy them verbatim into your code without needing to change the
dynamic_shapes argument to your  call.

Parameters

================================================================================

# torch.export - API Reference (Part 8)

 () – We will trace the forward method of this module.
 () – Example positional inputs.
 () – Optional example keyword inputs.
dynamic_shapes () – An optional argument where the type should either be:
1) a dict from argument names of  to their dynamic shape specifications,
2) a tuple that specifies dynamic shape specifications for each input in original order.
If you are specifying dynamism on keyword args, you will need to pass them in the order that
is defined in the original function signature.
The dynamic shape of a tensor argument can be specified as either
(1) a dict from dynamic dimension indices to  types, where it is
not required to include static dimension indices in this dict, but when they are,
they should be mapped to None; or (2) a tuple / list of  types or None,
where the  types correspond to dynamic dimensions, and static dimensions
are denoted by None. Arguments that are dicts or tuples / lists of tensors are
recursively specified by using mappings or sequences of contained specifications.

================================================================================

# torch.export - API Reference (Part 9)

 () – When disabled (default), the export function will trace the program through
Python runtime, which by itself will not validate some of the implicit assumptions
baked into the graph. It will still validate most critical assumptions like shape
safety. When enabled (by setting strict=True), the export function will trace
the program through TorchDynamo which will ensure the soundness of the resulting
graph. TorchDynamo has limited Python feature coverage, thus you may experience more
errors. Note that toggling this argument does not affect the resulting IR spec to be
different and the model will be serialized in the same way regardless of what value
is passed here.
preserve_module_call_signature () – A list of submodule paths for which the original
calling conventions are preserved as metadata. The metadata will be used when calling
torch.export.unflatten to preserve the original calling conventions of modules.



An ExportedProgram containing the traced callable.

Return type
ExportedProgram

================================================================================

# torch.export - API Reference (Part 10)

List:
() – We will trace the forward method of this module.
 () – Example positional inputs.
 () – Optional example keyword inputs.
dynamic_shapes () – An optional argument where the type should either be:
1) a dict from argument names of  to their dynamic shape specifications,
2) a tuple that specifies dynamic shape specifications for each input in original order.
If you are specifying dynamism on keyword args, you will need to pass them in the order that
is defined in the original function signature.
The dynamic shape of a tensor argument can be specified as either
(1) a dict from dynamic dimension indices to  types, where it is
not required to include static dimension indices in this dict, but when they are,
they should be mapped to None; or (2) a tuple / list of  types or None,
where the  types correspond to dynamic dimensions, and static dimensions
are denoted by None. Arguments that are dicts or tuples / lists of tensors are
recursively specified by using mappings or sequences of contained specifications.

================================================================================

# torch.export - API Reference (Part 11)

 () – When disabled (default), the export function will trace the program through
Python runtime, which by itself will not validate some of the implicit assumptions
baked into the graph. It will still validate most critical assumptions like shape
safety. When enabled (by setting strict=True), the export function will trace
the program through TorchDynamo which will ensure the soundness of the resulting
graph. TorchDynamo has limited Python feature coverage, thus you may experience more
errors. Note that toggling this argument does not affect the resulting IR spec to be
different and the model will be serialized in the same way regardless of what value
is passed here.
preserve_module_call_signature () – A list of submodule paths for which the original
calling conventions are preserved as metadata. The metadata will be used when calling
torch.export.unflatten to preserve the original calling conventions of modules.

() – We will trace the forward method of this module.

() – Example positional inputs.

() – Optional example keyword inputs.

================================================================================

# torch.export - API Reference (Part 12)

dynamic_shapes () – An optional argument where the type should either be:
1) a dict from argument names of  to their dynamic shape specifications,
2) a tuple that specifies dynamic shape specifications for each input in original order.
If you are specifying dynamism on keyword args, you will need to pass them in the order that
is defined in the original function signature.
The dynamic shape of a tensor argument can be specified as either
(1) a dict from dynamic dimension indices to  types, where it is
not required to include static dimension indices in this dict, but when they are,
they should be mapped to None; or (2) a tuple / list of  types or None,
where the  types correspond to dynamic dimensions, and static dimensions
are denoted by None. Arguments that are dicts or tuples / lists of tensors are
recursively specified by using mappings or sequences of contained specifications.

================================================================================

# torch.export - API Reference (Part 13)

An optional argument where the type should either be:
1) a dict from argument names of  to their dynamic shape specifications,
2) a tuple that specifies dynamic shape specifications for each input in original order.
If you are specifying dynamism on keyword args, you will need to pass them in the order that
is defined in the original function signature.

The dynamic shape of a tensor argument can be specified as either
(1) a dict from dynamic dimension indices to  types, where it is
not required to include static dimension indices in this dict, but when they are,
they should be mapped to None; or (2) a tuple / list of  types or None,
where the  types correspond to dynamic dimensions, and static dimensions
are denoted by None. Arguments that are dicts or tuples / lists of tensors are
recursively specified by using mappings or sequences of contained specifications.

================================================================================

# torch.export - API Reference (Part 14)

() – When disabled (default), the export function will trace the program through
Python runtime, which by itself will not validate some of the implicit assumptions
baked into the graph. It will still validate most critical assumptions like shape
safety. When enabled (by setting strict=True), the export function will trace
the program through TorchDynamo which will ensure the soundness of the resulting
graph. TorchDynamo has limited Python feature coverage, thus you may experience more
errors. Note that toggling this argument does not affect the resulting IR spec to be
different and the model will be serialized in the same way regardless of what value
is passed here.

preserve_module_call_signature () – A list of submodule paths for which the original
calling conventions are preserved as metadata. The metadata will be used when calling
torch.export.unflatten to preserve the original calling conventions of modules.

An ExportedProgram containing the traced callable.

Acceptable input/output types

Acceptable types of inputs (for  and ) and outputs include:

================================================================================

# torch.export - API Reference (Part 15)

List:
Primitive types, i.e. torch.Tensor, , ,  and .
Dataclasses, but they must be registered by calling register_dataclass() first.
(Nested) Data structures comprising of , , , namedtuple and
OrderedDict containing all above types.

Primitive types, i.e. torch.Tensor, , ,  and .

Dataclasses, but they must be registered by calling register_dataclass() first.

(Nested) Data structures comprising of , , , namedtuple and
OrderedDict containing all above types.

torch.export., , , extra_files, opset_version, pickle_protocol


Under active development, saved files may not be usable in newer versions
of PyTorch.

Saves an ExportedProgram to a file-like object. It can then be
loaded using the Python API torch.export.load.

Parameters

 (ExportedProgram) – The exported program to save.
 (os.PathLike) – implement write and flush) or a string containing a file name.
extra_files () – Map from filename to contents
which will be stored as part of f.
opset_version () – A map of opset names
to the version of this opset
pickle_protocol () – can be specified to override the default protocol




 
 


 
      
           


   

# Save to file
 "exported_program.pt2"

# Save to io.BytesIO buffer
  
 

================================================================================

# torch.export - API Reference (Part 16)

# Save with extra files
extra_files   
 "exported_program.pt2" extra_filesextra_files

Under active development, saved files may not be usable in newer versions
of PyTorch.

Saves an ExportedProgram to a file-like object. It can then be
loaded using the Python API torch.export.load.

Parameters

 (ExportedProgram) – The exported program to save.
 (os.PathLike) – implement write and flush) or a string containing a file name.
extra_files () – Map from filename to contents
which will be stored as part of f.
opset_version () – A map of opset names
to the version of this opset
pickle_protocol () – can be specified to override the default protocol

List:
(ExportedProgram) – The exported program to save.
 (os.PathLike) – implement write and flush) or a string containing a file name.
extra_files () – Map from filename to contents
which will be stored as part of f.
opset_version () – A map of opset names
to the version of this opset
pickle_protocol () – can be specified to override the default protocol

(ExportedProgram) – The exported program to save.

(os.PathLike) – implement write and flush) or a string containing a file name.

================================================================================

# torch.export - API Reference (Part 17)

extra_files () – Map from filename to contents
which will be stored as part of f.

opset_version () – A map of opset names
to the version of this opset

pickle_protocol () – can be specified to override the default protocol

Code example:
# Save to file
 "exported_program.pt2"

# Save to io.BytesIO buffer
  
 

# Save with extra files
extra_files   
 "exported_program.pt2" extra_filesextra_files

torch.export., , extra_files, expected_opset_version


Under active development, saved files may not be usable in newer versions
of PyTorch.

Loads an ExportedProgram previously saved with
torch.export.save.

Parameters

 (os.PathLike) – A file-like object (has to
implement write and flush) or a string containing a file name.
extra_files () – The extra filenames given in
this map would be loaded and their content would be stored in the
provided map.
expected_opset_version () – A map of opset names
to expected opset versions



An ExportedProgram object

Return type
ExportedProgram



 
 

# Load ExportedProgram from file
  "exported_program.pt2"

# Load ExportedProgram from io.BytesIO object
 "exported_program.pt2"   
      

  

================================================================================

# torch.export - API Reference (Part 18)

# Load with extra files.
extra_files     # values will be replaced with data
  "exported_program.pt2" extra_filesextra_files
extra_files

Under active development, saved files may not be usable in newer versions
of PyTorch.

Loads an ExportedProgram previously saved with
torch.export.save.

Parameters

 (os.PathLike) – A file-like object (has to
implement write and flush) or a string containing a file name.
extra_files () – The extra filenames given in
this map would be loaded and their content would be stored in the
provided map.
expected_opset_version () – A map of opset names
to expected opset versions



An ExportedProgram object

Return type
ExportedProgram

List:
(os.PathLike) – A file-like object (has to
implement write and flush) or a string containing a file name.
extra_files () – The extra filenames given in
this map would be loaded and their content would be stored in the
provided map.
expected_opset_version () – A map of opset names
to expected opset versions

(os.PathLike) – A file-like object (has to
implement write and flush) or a string containing a file name.

extra_files () – The extra filenames given in
this map would be loaded and their content would be stored in the
provided map.

================================================================================

# torch.export - API Reference (Part 19)

expected_opset_version () – A map of opset names
to expected opset versions

An ExportedProgram object

Code example:
# Load ExportedProgram from file
  "exported_program.pt2"

# Load ExportedProgram from io.BytesIO object
 "exported_program.pt2"   
      

  

# Load with extra files.
extra_files     # values will be replaced with data
  "exported_program.pt2" extra_filesextra_files
extra_files

torch.export.draft_export, , , , dynamic_shapes, preserve_module_call_signature, 
A version of torch.export.export which is designed to consistently produce
an ExportedProgram, even if there are potential soundness issues, and to
generate a report listing the issues found.

Return type
ExportedProgram

A version of torch.export.export which is designed to consistently produce
an ExportedProgram, even if there are potential soundness issues, and to
generate a report listing the issues found.

Return type
ExportedProgram

torch.export.register_dataclass, , serialized_type_name
Registers a dataclass as a valid input/output type for torch.export.export().

Parameters

================================================================================

# torch.export - API Reference (Part 20)

 () – the dataclass type to register
serialized_type_name () – The serialized name for the dataclass. This is
 (required if you want to serialize the pytree TreeSpec containing) – 
dataclass. – 




 
 dataclasses  


@dataclass
 InputDataClass
     
     


@dataclass
 OutputDataClass
     


register_dataclassInputDataClass
register_dataclassOutputDataClass


 
       InputDataClass  OutputDataClass
            
         OutputDataClass


   InputDataClass

Registers a dataclass as a valid input/output type for torch.export.export().

Parameters

 () – the dataclass type to register
serialized_type_name () – The serialized name for the dataclass. This is
 (required if you want to serialize the pytree TreeSpec containing) – 
dataclass. –

List:
() – the dataclass type to register
serialized_type_name () – The serialized name for the dataclass. This is
 (required if you want to serialize the pytree TreeSpec containing) – 
dataclass. –

() – the dataclass type to register

serialized_type_name () – The serialized name for the dataclass. This is

(required if you want to serialize the pytree TreeSpec containing) –

Code example:
dataclasses  


@dataclass
 InputDataClass
     
     

================================================================================

# torch.export - API Reference (Part 21)


@dataclass
 OutputDataClass
     


register_dataclassInputDataClass
register_dataclassOutputDataClass


 
       InputDataClass  OutputDataClass
            
         OutputDataClass


   InputDataClass

torch.export.dynamic_shapes., , , 
The  class allows users to specify dynamism in their exported programs. By marking a dimension with a ,
the compiler associates the dimension with a symbolic integer containing a dynamic range.
The API can be used in 2 ways: Dim hints (i.e. automatic dynamic shapes: , Dim.DYNAMIC, Dim.STATIC),
or named Dims (i.e. Dim(“name”, min=1, max=2)).
Dim hints provide the lowest barrier to exportability, with the user only needing to specify if a dimension
if dynamic, static, or left for the compiler to decide (). The export process will automatically
infer the remaining constraints on min/max ranges and relationships between dimensions.

 
       
           
           
           


   
   
dynamic_shapes  
        
        

     dynamic_shapesdynamic_shapes

================================================================================

# torch.export - API Reference (Part 22)


Here, export would raise an exception if we replaced all uses of  with Dim.DYNAMIC,
as x.shape[0] is constrained to be static by the model.
More complex relations between dimensions may also be codegened as runtime assertion nodes by the compiler,
e.g. (x.shape[0] + y.shape[1]) % 4 == 0, to be raised if runtime inputs do not satisfy such constraints.
You may also specify min-max bounds for Dim hints, e.g. Dim.AUTO(min=16, max=32), Dim.DYNAMIC(max=64),
with the compiler inferring the remaining constraints within the ranges. An exception will be raised if
the valid range is entirely outside the user-specified range.
Named Dims provide a stricter way of specifying dynamism, where exceptions are raised if the compiler
infers constraints that do not match the user specification. For example, exporting the previous
model, the user would need the following dynamic_shapes argument:
  
   
dynamic_shapes  
        
        

     dynamic_shapesdynamic_shapes


Named Dims also allow specification of relationships between dimensions, up to univariate linear relations.
For example, the following indicates one dimension is a multiple of another plus 4:

================================================================================

# torch.export - API Reference (Part 23)

The  class allows users to specify dynamism in their exported programs. By marking a dimension with a ,
the compiler associates the dimension with a symbolic integer containing a dynamic range.

The API can be used in 2 ways: Dim hints (i.e. automatic dynamic shapes: , Dim.DYNAMIC, Dim.STATIC),
or named Dims (i.e. Dim(“name”, min=1, max=2)).

Dim hints provide the lowest barrier to exportability, with the user only needing to specify if a dimension
if dynamic, static, or left for the compiler to decide (). The export process will automatically
infer the remaining constraints on min/max ranges and relationships between dimensions.

Code example:
dynamic_shapes  
        
        

     dynamic_shapesdynamic_shapes

Here, export would raise an exception if we replaced all uses of  with Dim.DYNAMIC,
as x.shape[0] is constrained to be static by the model.

More complex relations between dimensions may also be codegened as runtime assertion nodes by the compiler,
e.g. (x.shape[0] + y.shape[1]) % 4 == 0, to be raised if runtime inputs do not satisfy such constraints.

================================================================================

# torch.export - API Reference (Part 24)

You may also specify min-max bounds for Dim hints, e.g. Dim.AUTO(min=16, max=32), Dim.DYNAMIC(max=64),
with the compiler inferring the remaining constraints within the ranges. An exception will be raised if
the valid range is entirely outside the user-specified range.

Named Dims provide a stricter way of specifying dynamism, where exceptions are raised if the compiler
infers constraints that do not match the user specification. For example, exporting the previous
model, the user would need the following dynamic_shapes argument:

Code example:
dynamic_shapes  
        
        

     dynamic_shapesdynamic_shapes

Named Dims also allow specification of relationships between dimensions, up to univariate linear relations.
For example, the following indicates one dimension is a multiple of another plus 4:

torch.export.dynamic_shapes.ShapesCollection
Builder for dynamic_shapes.
Used to assign dynamic shape specifications to tensors that appear in inputs.
This is useful particularly when  is a nested input structure, and it’s
easier to index the input tensors, than to replicate the structure of  in
the dynamic_shapes() specification.

      

================================================================================

# torch.export - API Reference (Part 25)

  
dynamic_shapes  ShapesCollection
dynamic_shapes      
dynamic_shapes     
# This is equivalent to the following (now auto-generated):
# dynamic_shapes = {"x": (dim, dim + 1, 8), "others": [{0: dim * 2}, None]}

  dynamic_shapesdynamic_shapes


To specify dynamism for integers, we need to first wrap the integers using
_IntWrapper so that we have a “unique identification tag” for each integer.

      
# Wrap all ints with _IntWrapper
mapped_args  tree_map_only   _IntWrapper 

dynamic_shapes  ShapesCollection
dynamic_shapes      
dynamic_shapesmapped_args  

# This is equivalent to the following (now auto-generated):
# dynamic_shapes = {"x": (dim, dim + 1, 8), "others": [Dim.DYNAMIC, None]}

  dynamic_shapesdynamic_shapes




dynamic_shapes, , 
Generates the dynamic_shapes() pytree structure according to  and .

Builder for dynamic_shapes.
Used to assign dynamic shape specifications to tensors that appear in inputs.

This is useful particularly when  is a nested input structure, and it’s
easier to index the input tensors, than to replicate the structure of  in
the dynamic_shapes() specification.

================================================================================

# torch.export - API Reference (Part 26)

Code example:
dynamic_shapes  ShapesCollection
dynamic_shapes      
dynamic_shapes     
# This is equivalent to the following (now auto-generated):
# dynamic_shapes = {"x": (dim, dim + 1, 8), "others": [{0: dim * 2}, None]}

  dynamic_shapesdynamic_shapes

To specify dynamism for integers, we need to first wrap the integers using
_IntWrapper so that we have a “unique identification tag” for each integer.

Code example:
# Wrap all ints with _IntWrapper
mapped_args  tree_map_only   _IntWrapper 

dynamic_shapes  ShapesCollection
dynamic_shapes      
dynamic_shapesmapped_args  

# This is equivalent to the following (now auto-generated):
# dynamic_shapes = {"x": (dim, dim + 1, 8), "others": [Dim.DYNAMIC, None]}

  dynamic_shapesdynamic_shapes

dynamic_shapes, , 
Generates the dynamic_shapes() pytree structure according to  and .

Generates the dynamic_shapes() pytree structure according to  and .

================================================================================

# torch.export - API Reference (Part 27)

torch.export.dynamic_shapes.AdditionalInputs
Infers dynamic_shapes based on additional inputs.
This is useful particularly for deployment engineers who, on the one hand, may
have access to ample testing or profiling data that can provide a fair sense of
representative inputs for a model, but on the other hand, may not know enough
about the model to guess which input shapes should be dynamic.
Input shapes that are different than the original are considered dynamic; conversely,
those that are the same as the original are considered static. Moreover, we verify
that the additional inputs are valid for the exported program. This guarantees that
tracing with them instead of the original would have generated the same graph.

     # example inputs for export

# other representative inputs that the exported program will run on
dynamic_shapes  AdditionalInputs
dynamic_shapes 

dynamic_shapes 

   dynamic_shapesdynamic_shapes




, 
Additional input  and .



dynamic_shapes, , 
Infers a dynamic_shapes() pytree structure by merging shapes of the
original input  and  and of each additional input
args and kwargs.




Verifies that an exported program is valid for each additional input.

================================================================================

# torch.export - API Reference (Part 28)

Infers dynamic_shapes based on additional inputs.

This is useful particularly for deployment engineers who, on the one hand, may
have access to ample testing or profiling data that can provide a fair sense of
representative inputs for a model, but on the other hand, may not know enough
about the model to guess which input shapes should be dynamic.

Input shapes that are different than the original are considered dynamic; conversely,
those that are the same as the original are considered static. Moreover, we verify
that the additional inputs are valid for the exported program. This guarantees that
tracing with them instead of the original would have generated the same graph.

Code example:
# example inputs for export

# other representative inputs that the exported program will run on
dynamic_shapes  AdditionalInputs
dynamic_shapes 

dynamic_shapes 

   dynamic_shapesdynamic_shapes

, 
Additional input  and .

Additional input  and .

dynamic_shapes, , 
Infers a dynamic_shapes() pytree structure by merging shapes of the
original input  and  and of each additional input
args and kwargs.

================================================================================

# torch.export - API Reference (Part 29)

Infers a dynamic_shapes() pytree structure by merging shapes of the
original input  and  and of each additional input
args and kwargs.

Verifies that an exported program is valid for each additional input.

Verifies that an exported program is valid for each additional input.

torch.export.dynamic_shapes.refine_dynamic_shapes_from_suggested_fixes, dynamic_shapes
When exporting with dynamic_shapes(), export may fail with a ConstraintViolation error if the specification
doesn’t match the constraints inferred from tracing the model. The error message may provide suggested fixes -
changes that can be made to dynamic_shapes() to export successfully.
Example ConstraintViolation error message:
 

          # this just refines the dim's range
        # this specializes to a constant
          # dy was specified as an independent dim, but is actually tied to dx with this relation


This is a helper function that takes the ConstraintViolation error message and the original dynamic_shapes() spec,
and returns a new dynamic_shapes() spec that incorporates the suggested fixes.
Example usage:

================================================================================

# torch.export - API Reference (Part 30)

        dynamic_shapesdynamic_shapes
   
    new_shapes  refine_dynamic_shapes_from_suggested_fixes
         dynamic_shapes
    
        dynamic_shapesnew_shapes



Return type
[[, ], [], []]

When exporting with dynamic_shapes(), export may fail with a ConstraintViolation error if the specification
doesn’t match the constraints inferred from tracing the model. The error message may provide suggested fixes -
changes that can be made to dynamic_shapes() to export successfully.

Example ConstraintViolation error message:

Code example:
# this just refines the dim's range
        # this specializes to a constant
          # dy was specified as an independent dim, but is actually tied to dx with this relation

This is a helper function that takes the ConstraintViolation error message and the original dynamic_shapes() spec,
and returns a new dynamic_shapes() spec that incorporates the suggested fixes.

Code example:
dynamic_shapesdynamic_shapes
   
    new_shapes  refine_dynamic_shapes_from_suggested_fixes
         dynamic_shapes
    
        dynamic_shapesnew_shapes

Return type
[[, ], [], []]

================================================================================

# torch.export - API Reference (Part 31)

torch.export.ExportedProgram, , graph_signature, state_dict, range_constraints, module_call_graph, example_inputs, , , 
Package of a program from . It contains
an torch.fx.Graph that represents Tensor computation, a state_dict containing
tensor values of all lifted parameters and buffers, and various metadata.
You can call an ExportedProgram like the original callable traced by
 with the same calling convention.
To perform transformations on the graph, use  property to access
an torch.fx.GraphModule. You can then use
FX transformation
to rewrite the graph. Afterwards, you can simply use 
again to construct a correct ExportedProgram.







graph_signature



state_dict







range_constraints



module_call_graph



example_inputs




Returns a self contained GraphModule with all the parameters/buffers inlined.

Return type






run_decompositionsdecomp_table, decompose_custom_triton_ops
Run a set of decompositions on the exported program and returns a new
exported program. By default we will run the Core ATen decompositions to
get operators in the
Core ATen Operator Set.
For now, we do not decompose joint graphs.

================================================================================

# torch.export - API Reference (Part 32)

Parameters
decomp_table (torch._ops.OperatorBase) – An optional argument that specifies decomp behaviour for Aten ops
(1) If None, we decompose to core aten decompositions
(2) If empty, we don’t decompose any operator

Return type
ExportedProgram


Some examples:
If you don’t want to decompose anything
   
  run_decompositionsdecomp_table


If you want to get a core aten operator set except for certain operator, you can do following:
   
decomp_table  default_decompositions
decomp_table  your_custom_decomp
  run_decompositionsdecomp_tabledecomp_table

Package of a program from . It contains
an torch.fx.Graph that represents Tensor computation, a state_dict containing
tensor values of all lifted parameters and buffers, and various metadata.

You can call an ExportedProgram like the original callable traced by
 with the same calling convention.

To perform transformations on the graph, use  property to access
an torch.fx.GraphModule. You can then use
FX transformation
to rewrite the graph. Afterwards, you can simply use 
again to construct a correct ExportedProgram.

Returns a self contained GraphModule with all the parameters/buffers inlined.

Return type

================================================================================

# torch.export - API Reference (Part 33)

Returns a self contained GraphModule with all the parameters/buffers inlined.

run_decompositionsdecomp_table, decompose_custom_triton_ops
Run a set of decompositions on the exported program and returns a new
exported program. By default we will run the Core ATen decompositions to
get operators in the
Core ATen Operator Set.
For now, we do not decompose joint graphs.

Parameters
decomp_table (torch._ops.OperatorBase) – An optional argument that specifies decomp behaviour for Aten ops
(1) If None, we decompose to core aten decompositions
(2) If empty, we don’t decompose any operator

Return type
ExportedProgram


Some examples:
If you don’t want to decompose anything
   
  run_decompositionsdecomp_table


If you want to get a core aten operator set except for certain operator, you can do following:
   
decomp_table  default_decompositions
decomp_table  your_custom_decomp
  run_decompositionsdecomp_tabledecomp_table

Run a set of decompositions on the exported program and returns a new
exported program. By default we will run the Core ATen decompositions to
get operators in the
Core ATen Operator Set.

For now, we do not decompose joint graphs.

================================================================================

# torch.export - API Reference (Part 34)

Parameters
decomp_table (torch._ops.OperatorBase) – An optional argument that specifies decomp behaviour for Aten ops
(1) If None, we decompose to core aten decompositions
(2) If empty, we don’t decompose any operator

Return type
ExportedProgram

decomp_table (torch._ops.OperatorBase) – An optional argument that specifies decomp behaviour for Aten ops
(1) If None, we decompose to core aten decompositions
(2) If empty, we don’t decompose any operator

If you don’t want to decompose anything

Code example:
run_decompositionsdecomp_table

If you want to get a core aten operator set except for certain operator, you can do following:

Code example:
decomp_table  default_decompositions
decomp_table  your_custom_decomp
  run_decompositionsdecomp_tabledecomp_table

================================================================================

# torch.export - API Reference (Part 35)

torch.export.ExportGraphSignatureinput_specs, output_specs
ExportGraphSignature models the input/output signature of Export Graph,
which is a fx.Graph with stronger invariants gurantees.
Export Graph is functional and does not access “states” like parameters
or buffers within the graph via  nodes. Instead, 
gurantees that parameters, buffers, and constant tensors are lifted out of
the graph as inputs.  Similarly, any mutations to buffers are not included
in the graph either, instead the updated values of mutated buffers are
modeled as additional outputs of Export Graph.
The ordering of all inputs and outputs are:
  parameters_buffers_constant_tensors flattened_user_inputs
  mutated_inputs flattened_user_outputs


e.g. If following module is exported:
 CustomModule
       
        CustomModule 

        # Define a parameter
        my_parameter  

        # Define two buffers
        register_buffer"my_buffer1" 
        register_buffer"my_buffer2" 

       
        # Use the parameter, buffers, and both inputs in the forward method
          
              my_parameter
          my_buffer1    my_buffer2

================================================================================

# torch.export - API Reference (Part 36)

        # Mutate one of the buffers (e.g., increment it by 1)
        my_buffer2  # In-place addition

         


  CustomModule
    


Resulting Graph is non-functional:

    p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2    
     


Resulting ExportGraphSignature of the non-functional Graph would be:

p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 USER_OUTPUT


To get a functional Graph, you can use run_decompositions():
  CustomModule
    
  run_decompositions


Resulting Graph is functional:

================================================================================

# torch.export - API Reference (Part 37)

    p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2    
      


Resulting ExportGraphSignature of the functional Graph would be:

p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 BUFFER_MUTATION 'my_buffer2'
 USER_OUTPUT

ExportGraphSignature models the input/output signature of Export Graph,
which is a fx.Graph with stronger invariants gurantees.

Export Graph is functional and does not access “states” like parameters
or buffers within the graph via  nodes. Instead, 
gurantees that parameters, buffers, and constant tensors are lifted out of
the graph as inputs.  Similarly, any mutations to buffers are not included
in the graph either, instead the updated values of mutated buffers are
modeled as additional outputs of Export Graph.

The ordering of all inputs and outputs are:

================================================================================

# torch.export - API Reference (Part 38)

Code example:
parameters_buffers_constant_tensors flattened_user_inputs
  mutated_inputs flattened_user_outputs

e.g. If following module is exported:

Code example:
CustomModule
       
        CustomModule 

        # Define a parameter
        my_parameter  

        # Define two buffers
        register_buffer"my_buffer1" 
        register_buffer"my_buffer2" 

       
        # Use the parameter, buffers, and both inputs in the forward method
          
              my_parameter
          my_buffer1    my_buffer2

        # Mutate one of the buffers (e.g., increment it by 1)
        my_buffer2  # In-place addition

         


  CustomModule

Resulting Graph is non-functional:

Code example:
p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2

Resulting ExportGraphSignature of the non-functional Graph would be:

================================================================================

# torch.export - API Reference (Part 39)

Code example:
p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 USER_OUTPUT

To get a functional Graph, you can use run_decompositions():

Code example:
CustomModule
    
  run_decompositions

Resulting Graph is functional:

Code example:
p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2

Resulting ExportGraphSignature of the functional Graph would be:

Code example:
p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 BUFFER_MUTATION 'my_buffer2'
 USER_OUTPUT

================================================================================

# torch.export - API Reference (Part 40)

torch.export.ModuleCallSignaturetorch.export.graph_signature.TensorArgumenttorch.export.graph_signature.SymIntArgumenttorch.export.graph_signature.SymFloatArgumenttorch.export.graph_signature.SymBoolArgumenttorch.export.graph_signature.ConstantArgumenttorch.export.graph_signature.CustomObjArgumenttorch.export.graph_signature.TokenArgument, torch.export.graph_signature.TensorArgumenttorch.export.graph_signature.SymIntArgumenttorch.export.graph_signature.SymFloatArgumenttorch.export.graph_signature.SymBoolArgumenttorch.export.graph_signature.ConstantArgumenttorch.export.graph_signature.CustomObjArgumenttorch.export.graph_signature.TokenArgument, torch.utils._pytree.TreeSpec, torch.utils._pytree.TreeSpec, forward_arg_names

torch.export.ModuleCallEntry, torch.export.exported_program.ModuleCallSignature

================================================================================

# torch.export - API Reference (Part 41)

torch.export.decomp_utils.CustomDecompTable
This is a custom dictionary that is specifically used for handling decomp_table in export.
The reason we need this is because in the new world, you can only  an op from decomp
table to preserve it. This is problematic for custom ops because we don’t know when the custom
op will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations
until we really need to materialize it (which is when we run decomposition pass.)

Invariants we hold are:
All aten decomp is loaded at the init time
We materialize ALL ops when user ever reads from the table to make it more likely
that dispatcher picks up the custom op.
If it is write operation, we don’t necessarily materialize
We load the final time during export, right before calling run_decompositions()







Return type
CustomDecompTable













materialize

Return type
[torch._ops.OperatorBase, ]









other_dict

================================================================================

# torch.export - API Reference (Part 42)

This is a custom dictionary that is specifically used for handling decomp_table in export.
The reason we need this is because in the new world, you can only  an op from decomp
table to preserve it. This is problematic for custom ops because we don’t know when the custom
op will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations
until we really need to materialize it (which is when we run decomposition pass.)

Invariants we hold are:
All aten decomp is loaded at the init time
We materialize ALL ops when user ever reads from the table to make it more likely
that dispatcher picks up the custom op.
If it is write operation, we don’t necessarily materialize
We load the final time during export, right before calling run_decompositions()

List:
All aten decomp is loaded at the init time
We materialize ALL ops when user ever reads from the table to make it more likely
that dispatcher picks up the custom op.
If it is write operation, we don’t necessarily materialize
We load the final time during export, right before calling run_decompositions()

All aten decomp is loaded at the init time

================================================================================

# torch.export - API Reference (Part 43)

We materialize ALL ops when user ever reads from the table to make it more likely
that dispatcher picks up the custom op.

If it is write operation, we don’t necessarily materialize

We load the final time during export, right before calling run_decompositions()

Return type
CustomDecompTable

Return type
CustomDecompTable

materialize

Return type
[torch._ops.OperatorBase, ]

Return type
[torch._ops.OperatorBase, ]

[torch._ops.OperatorBase, ]

torch.export.exported_program.default_decompositions
This is the default decomposition table which contains decomposition of
all ATEN operators to core aten opset. Use this API together with
run_decompositions()

Return type
CustomDecompTable

This is the default decomposition table which contains decomposition of
all ATEN operators to core aten opset. Use this API together with
run_decompositions()

Return type
CustomDecompTable

================================================================================

# torch.export - API Reference (Part 44)

torch.export.graph_signature.ExportGraphSignatureinput_specs, output_specs
ExportGraphSignature models the input/output signature of Export Graph,
which is a fx.Graph with stronger invariants gurantees.
Export Graph is functional and does not access “states” like parameters
or buffers within the graph via  nodes. Instead, 
gurantees that parameters, buffers, and constant tensors are lifted out of
the graph as inputs.  Similarly, any mutations to buffers are not included
in the graph either, instead the updated values of mutated buffers are
modeled as additional outputs of Export Graph.
The ordering of all inputs and outputs are:
  parameters_buffers_constant_tensors flattened_user_inputs
  mutated_inputs flattened_user_outputs


e.g. If following module is exported:
 CustomModule
       
        CustomModule 

        # Define a parameter
        my_parameter  

        # Define two buffers
        register_buffer"my_buffer1" 
        register_buffer"my_buffer2" 

       
        # Use the parameter, buffers, and both inputs in the forward method
          
              my_parameter
          my_buffer1    my_buffer2

================================================================================

# torch.export - API Reference (Part 45)

        # Mutate one of the buffers (e.g., increment it by 1)
        my_buffer2  # In-place addition

         


  CustomModule
    


Resulting Graph is non-functional:

    p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2    
     


Resulting ExportGraphSignature of the non-functional Graph would be:

p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 USER_OUTPUT


To get a functional Graph, you can use run_decompositions():
  CustomModule
    
  run_decompositions


Resulting Graph is functional:

================================================================================

# torch.export - API Reference (Part 46)

    p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2    
      


Resulting ExportGraphSignature of the functional Graph would be:

p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 BUFFER_MUTATION 'my_buffer2'
 USER_OUTPUT





replace_all_uses, 
Replace all uses of the old name with new name in the signature.




get_replace_hookreplace_inputs

ExportGraphSignature models the input/output signature of Export Graph,
which is a fx.Graph with stronger invariants gurantees.

================================================================================

# torch.export - API Reference (Part 47)

Export Graph is functional and does not access “states” like parameters
or buffers within the graph via  nodes. Instead, 
gurantees that parameters, buffers, and constant tensors are lifted out of
the graph as inputs.  Similarly, any mutations to buffers are not included
in the graph either, instead the updated values of mutated buffers are
modeled as additional outputs of Export Graph.

The ordering of all inputs and outputs are:

Code example:
parameters_buffers_constant_tensors flattened_user_inputs
  mutated_inputs flattened_user_outputs

e.g. If following module is exported:

Code example:
CustomModule
       
        CustomModule 

        # Define a parameter
        my_parameter  

        # Define two buffers
        register_buffer"my_buffer1" 
        register_buffer"my_buffer2" 

       
        # Use the parameter, buffers, and both inputs in the forward method
          
              my_parameter
          my_buffer1    my_buffer2

        # Mutate one of the buffers (e.g., increment it by 1)
        my_buffer2  # In-place addition

         


  CustomModule

Resulting Graph is non-functional:

================================================================================

# torch.export - API Reference (Part 48)

Code example:
p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2

Resulting ExportGraphSignature of the non-functional Graph would be:

Code example:
p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 USER_OUTPUT

To get a functional Graph, you can use run_decompositions():

Code example:
CustomModule
    
  run_decompositions

Resulting Graph is functional:

Code example:
p_my_parameter    placeholderp_my_parameter
    b_my_buffer1    placeholderb_my_buffer1
    b_my_buffer2    placeholderb_my_buffer2
        placeholder
        placeholder
        call_function   p_my_parameter   
        call_function   b_my_buffer1   
        call_function   b_my_buffer2   
        call_function      
        call_function  b_my_buffer2

Resulting ExportGraphSignature of the functional Graph would be:

================================================================================

# torch.export - API Reference (Part 49)

Code example:
p_my_parameter  'my_parameter'
b_my_buffer1  'my_buffer1' persistent
b_my_buffer2  'my_buffer2' persistent
 USER_INPUT
 USER_INPUT


 BUFFER_MUTATION 'my_buffer2'
 USER_OUTPUT

replace_all_uses, 
Replace all uses of the old name with new name in the signature.

Replace all uses of the old name with new name in the signature.

get_replace_hookreplace_inputs

torch.export.graph_signature.ExportBackwardSignaturegradients_to_parameters, gradients_to_user_inputs, loss_output

torch.export.graph_signature.
An enumeration.

torch.export.graph_signature.torch.export.graph_signature.InputKind, torch.export.graph_signature.TensorArgumenttorch.export.graph_signature.SymIntArgumenttorch.export.graph_signature.SymFloatArgumenttorch.export.graph_signature.SymBoolArgumenttorch.export.graph_signature.ConstantArgumenttorch.export.graph_signature.CustomObjArgumenttorch.export.graph_signature.TokenArgument, , persistent

torch.export.graph_signature.OutputKind
An enumeration.

================================================================================

# torch.export - API Reference (Part 50)

torch.export.graph_signature.OutputSpectorch.export.graph_signature.OutputKind, torch.export.graph_signature.TensorArgumenttorch.export.graph_signature.SymIntArgumenttorch.export.graph_signature.SymFloatArgumenttorch.export.graph_signature.SymBoolArgumenttorch.export.graph_signature.ConstantArgumenttorch.export.graph_signature.CustomObjArgumenttorch.export.graph_signature.TokenArgument,

torch.export.graph_signature.SymIntArgument

torch.export.graph_signature.SymBoolArgument

torch.export.graph_signature.SymFloatArgument

torch.export.graph_signature.CustomObjArgument, , torch._library.fake_class_registry.FakeScriptObject

torch.export.unflatten.FlatArgsAdapter
Adapts input arguments with input_spec to align target_spec.


target_spec, input_spec, input_args, , 
NOTE: This adapter may mutate given input_args_with_path.

Return type
[]

Adapts input arguments with input_spec to align target_spec.

target_spec, input_spec, input_args, , 
NOTE: This adapter may mutate given input_args_with_path.

Return type
[]

NOTE: This adapter may mutate given input_args_with_path.

================================================================================

# torch.export - API Reference (Part 51)

torch.export.unflatten.InterpreterModule, 
A module that uses torch.fx.Interpreter to execute instead of the usual
codegen that GraphModule uses. This provides better stack trace information
and makes it easier to debug execution.

A module that uses torch.fx.Interpreter to execute instead of the usual
codegen that GraphModule uses. This provides better stack trace information
and makes it easier to debug execution.

torch.export.unflatten.InterpreterModuleDispatcher, call_modules
A module that carries a sequence of InterpreterModules corresponding to
a sequence of calls of that module. Each call to the module dispatches
to the next InterpreterModule, and wraps back around after the last.

A module that carries a sequence of InterpreterModules corresponding to
a sequence of calls of that module. Each call to the module dispatches
to the next InterpreterModule, and wraps back around after the last.

================================================================================

# torch.export - API Reference (Part 52)

torch.export.unflatten., flat_args_adapter
Unflatten an ExportedProgram, producing a module with the same module
hierarchy as the original eager module. This can be useful if you are trying
to use torch.export with another system that expects a module
hierachy instead of the flat graph that torch.export usually produces.


The args/kwargs of unflattened modules will not necessarily match
the eager module, so doing a module swap (e.g. self.submod 
) will not necessarily work. If you need to swap a module out, you
need to set the preserve_module_call_signature parameter of
torch.export.export().


Parameters

 (ExportedProgram) – The ExportedProgram to unflatten.
flat_args_adapter (FlatArgsAdapter) – Adapt flat args if input TreeSpec does not match with exported module’s.



An instance of UnflattenedModule, which has the same module
hierarchy as the original eager module pre-export.

Return type
UnflattenedModule

Unflatten an ExportedProgram, producing a module with the same module
hierarchy as the original eager module. This can be useful if you are trying
to use torch.export with another system that expects a module
hierachy instead of the flat graph that torch.export usually produces.

================================================================================

# torch.export - API Reference (Part 53)

The args/kwargs of unflattened modules will not necessarily match
the eager module, so doing a module swap (e.g. self.submod 
) will not necessarily work. If you need to swap a module out, you
need to set the preserve_module_call_signature parameter of
torch.export.export().

Parameters

 (ExportedProgram) – The ExportedProgram to unflatten.
flat_args_adapter (FlatArgsAdapter) – Adapt flat args if input TreeSpec does not match with exported module’s.



An instance of UnflattenedModule, which has the same module
hierarchy as the original eager module pre-export.

Return type
UnflattenedModule

List:
(ExportedProgram) – The ExportedProgram to unflatten.
flat_args_adapter (FlatArgsAdapter) – Adapt flat args if input TreeSpec does not match with exported module’s.

(ExportedProgram) – The ExportedProgram to unflatten.

flat_args_adapter (FlatArgsAdapter) – Adapt flat args if input TreeSpec does not match with exported module’s.

An instance of UnflattenedModule, which has the same module
hierarchy as the original eager module pre-export.

torch.export.passes.move_to_device_pass, 
Move the exported program to the given device.

Parameters

================================================================================

# torch.export - API Reference (Part 54)

 (ExportedProgram) – The exported program to move.
 (torch.device) – The device to move the exported program to.
If a string, it is interpreted as a device name.
If a dict, it is interpreted as a mapping from
the existing device to the intended one



The moved exported program.

Return type
ExportedProgram

Move the exported program to the given device.

Parameters

 (ExportedProgram) – The exported program to move.
 (torch.device) – The device to move the exported program to.
If a string, it is interpreted as a device name.
If a dict, it is interpreted as a mapping from
the existing device to the intended one



The moved exported program.

Return type
ExportedProgram

List:
(ExportedProgram) – The exported program to move.
 (torch.device) – The device to move the exported program to.
If a string, it is interpreted as a device name.
If a dict, it is interpreted as a mapping from
the existing device to the intended one

(ExportedProgram) – The exported program to move.

(torch.device) – The device to move the exported program to.
If a string, it is interpreted as a device name.
If a dict, it is interpreted as a mapping from
the existing device to the intended one

The moved exported program.

================================================================================

# torch.export - API Reference (Part 55)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Distributed communication package - torch.distributed

Created On: Jul 12, 2017 | Last Updated On: Jul 14, 2025

Please refer to PyTorch Distributed Overview
for a brief introduction to all features related to distributed training.

================================================================================

# Distributed communication package - torch.distributed - Backends

torch.distributed supports three built-in backends, each with
different capabilities. The table below shows which functions are available
for use with CPU / CUDA tensors.
MPI supports CUDA only if the implementation used to build PyTorch supports it.

Table:
all_reduce
all_gather
reduce_scatter
all_to_all

================================================================================

# Distributed communication package - torch.distributed - Backends that come with PyTorch

PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype).
By default for Linux, the Gloo and NCCL backends are built and included in PyTorch
distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be
included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI
installed.)

As of PyTorch v1.8, Windows supports all collective communications backend but NCCL,
If the init_method argument of init_process_group() points to a file it must adhere
to the following schema:

List:
Local file system, init_method="file:///d:/tmp/some_file"
Shared file system, init_method="file://////{machine_name}/{share_folder_name}/some_file"

Local file system, init_method="file:///d:/tmp/some_file"

Shared file system, init_method="file://////{machine_name}/{share_folder_name}/some_file"

Same as on Linux platform, you can enable TcpStore by setting environment variables,
MASTER_ADDR and MASTER_PORT.

================================================================================

# Distributed communication package - torch.distributed - Which backend to use? (Part 1)

In the past, we were often asked: “which backend should I use?”.

List:
Rule of thumb

Use the NCCL backend for distributed  training
Use the Gloo backend for distributed  training.


GPU hosts with InfiniBand interconnect

Use NCCL, since it’s the only backend that currently supports
InfiniBand and GPUDirect.


GPU hosts with Ethernet interconnect

Use NCCL, since it currently provides the best distributed GPU
training performance, especially for multiprocess single-node or
multi-node distributed training. If you encounter any problem with
NCCL, use Gloo as the fallback option. (Note that Gloo currently
runs slower than NCCL for GPUs.)


CPU hosts with InfiniBand interconnect

If your InfiniBand has enabled IP over IB, use Gloo, otherwise,
use MPI instead. We are planning on adding InfiniBand support for
Gloo in the upcoming releases.


CPU hosts with Ethernet interconnect

Use Gloo, unless you have specific reasons to use MPI.

List:
Use the NCCL backend for distributed  training
Use the Gloo backend for distributed  training.

Use the NCCL backend for distributed  training

Use the Gloo backend for distributed  training.

GPU hosts with InfiniBand interconnect

================================================================================

# Distributed communication package - torch.distributed - Which backend to use? (Part 2)

List:
Use NCCL, since it’s the only backend that currently supports
InfiniBand and GPUDirect.

Use NCCL, since it’s the only backend that currently supports
InfiniBand and GPUDirect.

GPU hosts with Ethernet interconnect

List:
Use NCCL, since it currently provides the best distributed GPU
training performance, especially for multiprocess single-node or
multi-node distributed training. If you encounter any problem with
NCCL, use Gloo as the fallback option. (Note that Gloo currently
runs slower than NCCL for GPUs.)

Use NCCL, since it currently provides the best distributed GPU
training performance, especially for multiprocess single-node or
multi-node distributed training. If you encounter any problem with
NCCL, use Gloo as the fallback option. (Note that Gloo currently
runs slower than NCCL for GPUs.)

CPU hosts with InfiniBand interconnect

List:
If your InfiniBand has enabled IP over IB, use Gloo, otherwise,
use MPI instead. We are planning on adding InfiniBand support for
Gloo in the upcoming releases.

If your InfiniBand has enabled IP over IB, use Gloo, otherwise,
use MPI instead. We are planning on adding InfiniBand support for
Gloo in the upcoming releases.

================================================================================

# Distributed communication package - torch.distributed - Which backend to use? (Part 3)

CPU hosts with Ethernet interconnect

List:
Use Gloo, unless you have specific reasons to use MPI.

Use Gloo, unless you have specific reasons to use MPI.

================================================================================

# Distributed communication package - torch.distributed - Choosing the network interface to use

By default, both the NCCL and Gloo backends will try to find the right network interface to use.
If the automatically detected interface is not correct, you can override it using the following
environment variables (applicable to the respective backend):

List:
NCCL_SOCKET_IFNAME, for example  NCCL_SOCKET_IFNAME=eth0
GLOO_SOCKET_IFNAME, for example  GLOO_SOCKET_IFNAME=eth0

NCCL_SOCKET_IFNAME, for example  NCCL_SOCKET_IFNAME=eth0

GLOO_SOCKET_IFNAME, for example  GLOO_SOCKET_IFNAME=eth0

If you’re using the Gloo backend, you can specify multiple interfaces by separating
them by a comma, like this:  GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3.
The backend will dispatch operations in a round-robin fashion across these interfaces.
It is imperative that all processes specify the same number of interfaces in this variable.

================================================================================

# Distributed communication package - torch.distributed - Other NCCL environment variables (Part 1)

- in case of NCCL failure, you can set NCCL_DEBUG=INFO to print an explicit
warning message as well as basic NCCL initialization information.

You may also use NCCL_DEBUG_SUBSYS to get more details about a specific
aspect of NCCL. For example, NCCL_DEBUG_SUBSYS=COLL would print logs of
collective calls, which may be helpful when debugging hangs, especially those
caused by collective type or message size mismatch. In case of topology
detection failure, it would be helpful to set NCCL_DEBUG_SUBSYS=GRAPH
to inspect the detailed detection result and save as reference if further help
from NCCL team is needed.

Performance tuning - NCCL performs automatic tuning based on its topology detection to save users’
tuning effort. On some socket-based systems, users may still try tuning
NCCL_SOCKET_NTHREADS and NCCL_NSOCKS_PERTHREAD to increase socket
network bandwidth. These two environment variables have been pre-tuned by NCCL
for some cloud providers, such as AWS or GCP.

For a full list of NCCL environment variables, please refer to
NVIDIA NCCL’s official documentation

================================================================================

# Distributed communication package - torch.distributed - Other NCCL environment variables (Part 2)

You can tune NCCL communicators even further using torch.distributed.ProcessGroupNCCL.NCCLConfig
and torch.distributed.ProcessGroupNCCL.Options. Learn more about them using 
(e.g. help(torch.distributed.ProcessGroupNCCL.NCCLConfig)) in the interpreter.

================================================================================

# Distributed communication package - torch.distributed - Basics (Part 1)

The torch.distributed package provides PyTorch support and communication primitives
for multiprocess parallelism across several computation nodes running on one or more
machines. The class torch.nn.parallel.DistributedDataParallel() builds on this
functionality to provide synchronous distributed training as a wrapper around any
PyTorch model. This differs from the kinds of parallelism provided by
Multiprocessing package - torch.multiprocessing and torch.nn.DataParallel() in that it supports
multiple network-connected machines and in that the user must explicitly launch a separate
copy of the main training script for each process.

In the single-machine synchronous case, torch.distributed or the
torch.nn.parallel.DistributedDataParallel() wrapper may still have advantages over other
approaches to data-parallelism, including torch.nn.DataParallel():

================================================================================

# Distributed communication package - torch.distributed - Basics (Part 2)

List:
Each process maintains its own optimizer and performs a complete optimization step with each
iteration. While this may appear redundant, since the gradients have already been gathered
together and averaged across processes and are thus the same for every process, this means
that no parameter broadcast step is needed, reducing time spent transferring tensors between
nodes.
Each process contains an independent Python interpreter, eliminating the extra interpreter
overhead and “GIL-thrashing” that comes from driving several execution threads, model
replicas, or GPUs from a single Python process. This is especially important for models that
make heavy use of the Python runtime, including models with recurrent layers or many small
components.

Each process maintains its own optimizer and performs a complete optimization step with each
iteration. While this may appear redundant, since the gradients have already been gathered
together and averaged across processes and are thus the same for every process, this means
that no parameter broadcast step is needed, reducing time spent transferring tensors between
nodes.

================================================================================

# Distributed communication package - torch.distributed - Basics (Part 3)

Each process contains an independent Python interpreter, eliminating the extra interpreter
overhead and “GIL-thrashing” that comes from driving several execution threads, model
replicas, or GPUs from a single Python process. This is especially important for models that
make heavy use of the Python runtime, including models with recurrent layers or many small
components.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 1)

The package needs to be initialized using the torch.distributed.init_process_group()
or torch.distributed.device_mesh.init_device_mesh() function before calling any other methods.
Both block until all processes have joined.

Initialization is not thread-safe. Process group creation should be performed from a single thread, to prevent
inconsistent ‘UUID’ assignment across ranks, and to prevent races during initialization that can lead to hangs.

torch.distributed.is_available
Return  if the distributed package is available.
Otherwise,
torch.distributed does not expose any other APIs. Currently,
torch.distributed is available on Linux, MacOS and Windows. Set
USE_DISTRIBUTED=1 to enable it when building PyTorch from source.
Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows,
USE_DISTRIBUTED=0 for MacOS.

Return type

Return  if the distributed package is available.

Otherwise,
torch.distributed does not expose any other APIs. Currently,
torch.distributed is available on Linux, MacOS and Windows. Set
USE_DISTRIBUTED=1 to enable it when building PyTorch from source.
Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows,
USE_DISTRIBUTED=0 for MacOS.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 2)

torch.distributed.init_process_group, init_method, , world_size, , , group_name, pg_options, 
Initialize the default distributed process group.
This will also initialize the distributed package.

There are 2 main ways to initialize a process group:
Specify , , and world_size explicitly.
Specify init_method (a URL string) which indicates where/how
to discover peers. Optionally specify  and world_size,
or encode all required parameters in the URL and omit them.



If neither is specified, init_method is assumed to be “env://”.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 3)

 () – The backend to use. Depending on
build-time configurations, valid values include , ,
, , or one that is registered by a third-party
plugin.
Since 2.6, if  is not provided, c10d will use a backend
registered for the device type indicated by the  kwarg
(if provided). The known default registrations today are: 
for ,  for .
If neither  nor  is provided, c10d will
detect the accelerator on the run-time machine and use a backend
registered for that detected accelerator (or ).
This field can be given as a lowercase string (e.g., ),
which can also be accessed via  attributes (e.g.,
Backend.GLOO).
If using multiple processes per machine with  backend, each
process must have exclusive access to every GPU it uses, as sharing
GPUs between processes can result in deadlock or NCCL invalid usage.
 backend is experimental.
Default backend for the device can be queried with
get_default_backend_for_device().
init_method () – URL specifying how to initialize the
process group. Default is “env://” if no
init_method or  is specified.
Mutually exclusive with .
world_size () – Number of processes participating in
the job. Required if  is specified.
 () – Rank of the current process (it should be a
number between 0 and world_size-1).
Required if  is specified.
 () – Key/value store accessible to all workers, used
to exchange connection/address information.
Mutually exclusive with init_method.
 () – Timeout for operations executed against
the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.
This is the duration after which collectives will be aborted asynchronously and the process will crash.
This is done since CUDA execution is async and it is no longer safe to continue executing user code since
failed async NCCL operations might result in subsequent CUDA operations running on corrupted data.
When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.
group_name (deprecated) – Group name. This argument is ignored
pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. As of now, the only
options we support is ProcessGroupNCCL.Options for the 
backend, is_high_priority_stream can be specified so that
the nccl backend can pick up high priority cuda streams when
there’re compute kernels waiting. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t
 (torch.device) – a single, specific device
this process will work on, allowing for backend-specific
optimizations.  Currently this has two effects, only under
NCCL: the communicator is immediately formed (calling
ncclCommInit* immediately rather than the normal lazy
call) and sub-groups will use ncclCommSplit when
possible to avoid unnecessary overhead of group creation. If you
want to know NCCL initialization error early, you can also use this
field. If an  is provided, the API assumes that the accelerator
type at compile time will be used.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 4)





To enable   Backend.MPI, PyTorch needs to be built from source
on a system that supports MPI.



Support for multiple backends is experimental. Currently when no backend is
specified, both  and  backends will be created. The  backend
will be used for collectives with CPU tensors and the  backend will be used
for collectives with CUDA tensors. A custom backend can be specified by passing in
a string with format “<device_type>:<backend_name>,<device_type>:<backend_name>”, e.g.
“cpu:gloo,cuda:custom_backend”.

Initialize the default distributed process group.

This will also initialize the distributed package.

There are 2 main ways to initialize a process group:
Specify , , and world_size explicitly.
Specify init_method (a URL string) which indicates where/how
to discover peers. Optionally specify  and world_size,
or encode all required parameters in the URL and omit them.

List:
Specify , , and world_size explicitly.
Specify init_method (a URL string) which indicates where/how
to discover peers. Optionally specify  and world_size,
or encode all required parameters in the URL and omit them.

Specify , , and world_size explicitly.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 5)

Specify init_method (a URL string) which indicates where/how
to discover peers. Optionally specify  and world_size,
or encode all required parameters in the URL and omit them.

If neither is specified, init_method is assumed to be “env://”.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 6)

 () – The backend to use. Depending on
build-time configurations, valid values include , ,
, , or one that is registered by a third-party
plugin.
Since 2.6, if  is not provided, c10d will use a backend
registered for the device type indicated by the  kwarg
(if provided). The known default registrations today are: 
for ,  for .
If neither  nor  is provided, c10d will
detect the accelerator on the run-time machine and use a backend
registered for that detected accelerator (or ).
This field can be given as a lowercase string (e.g., ),
which can also be accessed via  attributes (e.g.,
Backend.GLOO).
If using multiple processes per machine with  backend, each
process must have exclusive access to every GPU it uses, as sharing
GPUs between processes can result in deadlock or NCCL invalid usage.
 backend is experimental.
Default backend for the device can be queried with
get_default_backend_for_device().
init_method () – URL specifying how to initialize the
process group. Default is “env://” if no
init_method or  is specified.
Mutually exclusive with .
world_size () – Number of processes participating in
the job. Required if  is specified.
 () – Rank of the current process (it should be a
number between 0 and world_size-1).
Required if  is specified.
 () – Key/value store accessible to all workers, used
to exchange connection/address information.
Mutually exclusive with init_method.
 () – Timeout for operations executed against
the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.
This is the duration after which collectives will be aborted asynchronously and the process will crash.
This is done since CUDA execution is async and it is no longer safe to continue executing user code since
failed async NCCL operations might result in subsequent CUDA operations running on corrupted data.
When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.
group_name (deprecated) – Group name. This argument is ignored
pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. As of now, the only
options we support is ProcessGroupNCCL.Options for the 
backend, is_high_priority_stream can be specified so that
the nccl backend can pick up high priority cuda streams when
there’re compute kernels waiting. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t
 (torch.device) – a single, specific device
this process will work on, allowing for backend-specific
optimizations.  Currently this has two effects, only under
NCCL: the communicator is immediately formed (calling
ncclCommInit* immediately rather than the normal lazy
call) and sub-groups will use ncclCommSplit when
possible to avoid unnecessary overhead of group creation. If you
want to know NCCL initialization error early, you can also use this
field. If an  is provided, the API assumes that the accelerator
type at compile time will be used.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 7)

List:
() – The backend to use. Depending on
build-time configurations, valid values include , ,
, , or one that is registered by a third-party
plugin.
Since 2.6, if  is not provided, c10d will use a backend
registered for the device type indicated by the  kwarg
(if provided). The known default registrations today are: 
for ,  for .
If neither  nor  is provided, c10d will
detect the accelerator on the run-time machine and use a backend
registered for that detected accelerator (or ).
This field can be given as a lowercase string (e.g., ),
which can also be accessed via  attributes (e.g.,
Backend.GLOO).
If using multiple processes per machine with  backend, each
process must have exclusive access to every GPU it uses, as sharing
GPUs between processes can result in deadlock or NCCL invalid usage.
 backend is experimental.
Default backend for the device can be queried with
get_default_backend_for_device().
init_method () – URL specifying how to initialize the
process group. Default is “env://” if no
init_method or  is specified.
Mutually exclusive with .
world_size () – Number of processes participating in
the job. Required if  is specified.
 () – Rank of the current process (it should be a
number between 0 and world_size-1).
Required if  is specified.
 () – Key/value store accessible to all workers, used
to exchange connection/address information.
Mutually exclusive with init_method.
 () – Timeout for operations executed against
the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.
This is the duration after which collectives will be aborted asynchronously and the process will crash.
This is done since CUDA execution is async and it is no longer safe to continue executing user code since
failed async NCCL operations might result in subsequent CUDA operations running on corrupted data.
When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.
group_name (deprecated) – Group name. This argument is ignored
pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. As of now, the only
options we support is ProcessGroupNCCL.Options for the 
backend, is_high_priority_stream can be specified so that
the nccl backend can pick up high priority cuda streams when
there’re compute kernels waiting. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t
 (torch.device) – a single, specific device
this process will work on, allowing for backend-specific
optimizations.  Currently this has two effects, only under
NCCL: the communicator is immediately formed (calling
ncclCommInit* immediately rather than the normal lazy
call) and sub-groups will use ncclCommSplit when
possible to avoid unnecessary overhead of group creation. If you
want to know NCCL initialization error early, you can also use this
field. If an  is provided, the API assumes that the accelerator
type at compile time will be used.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 8)

() – The backend to use. Depending on
build-time configurations, valid values include , ,
, , or one that is registered by a third-party
plugin.
Since 2.6, if  is not provided, c10d will use a backend
registered for the device type indicated by the  kwarg
(if provided). The known default registrations today are: 
for ,  for .
If neither  nor  is provided, c10d will
detect the accelerator on the run-time machine and use a backend
registered for that detected accelerator (or ).
This field can be given as a lowercase string (e.g., ),
which can also be accessed via  attributes (e.g.,
Backend.GLOO).
If using multiple processes per machine with  backend, each
process must have exclusive access to every GPU it uses, as sharing
GPUs between processes can result in deadlock or NCCL invalid usage.
 backend is experimental.
Default backend for the device can be queried with
get_default_backend_for_device().

init_method () – URL specifying how to initialize the
process group. Default is “env://” if no
init_method or  is specified.
Mutually exclusive with .

world_size () – Number of processes participating in
the job. Required if  is specified.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 9)

() – Rank of the current process (it should be a
number between 0 and world_size-1).
Required if  is specified.

() – Key/value store accessible to all workers, used
to exchange connection/address information.
Mutually exclusive with init_method.

() – Timeout for operations executed against
the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.
This is the duration after which collectives will be aborted asynchronously and the process will crash.
This is done since CUDA execution is async and it is no longer safe to continue executing user code since
failed async NCCL operations might result in subsequent CUDA operations running on corrupted data.
When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.

group_name (deprecated) – Group name. This argument is ignored

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 10)

pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. As of now, the only
options we support is ProcessGroupNCCL.Options for the 
backend, is_high_priority_stream can be specified so that
the nccl backend can pick up high priority cuda streams when
there’re compute kernels waiting. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t

(torch.device) – a single, specific device
this process will work on, allowing for backend-specific
optimizations.  Currently this has two effects, only under
NCCL: the communicator is immediately formed (calling
ncclCommInit* immediately rather than the normal lazy
call) and sub-groups will use ncclCommSplit when
possible to avoid unnecessary overhead of group creation. If you
want to know NCCL initialization error early, you can also use this
field. If an  is provided, the API assumes that the accelerator
type at compile time will be used.

To enable   Backend.MPI, PyTorch needs to be built from source
on a system that supports MPI.

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 11)

Support for multiple backends is experimental. Currently when no backend is
specified, both  and  backends will be created. The  backend
will be used for collectives with CPU tensors and the  backend will be used
for collectives with CUDA tensors. A custom backend can be specified by passing in
a string with format “<device_type>:<backend_name>,<device_type>:<backend_name>”, e.g.
“cpu:gloo,cuda:custom_backend”.

torch.distributed.device_mesh.init_device_meshdevice_type, mesh_shape, , mesh_dim_names
Initializes a DeviceMesh based on device_type, mesh_shape, and mesh_dim_names parameters.
This creates a DeviceMesh with an n-dimensional array layout, where  is the length of mesh_shape.
If mesh_dim_names is provided, each dimension is labeled as mesh_dim_names[i].


init_device_mesh follows SPMD programming model, meaning the same PyTorch Python program
runs on all processes/ranks in the cluster. Ensure mesh_shape (the dimensions of the nD array
describing device layout) is identical across all ranks. Inconsistent mesh_shape may lead to hanging.



================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 12)

If no process group is found, init_device_mesh will initialize distributed process group/groups
required for distributed communications behind the scene.


Parameters

device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
Passing in a device type with a GPU index, such as “cuda:0”, is not allowed.
mesh_shape () – A tuple defining the dimensions of the multi-dimensional array
describing the layout of devices.
mesh_dim_names () – A tuple of mesh dimension names to assign to each dimension
of the multi-dimensional array describing the layout of devices. Its length must match the length
of mesh_shape. Each string in mesh_dim_names must be unique.



A DeviceMesh object representing the device layout.

Return type
DeviceMesh



 torch.distributed.device_mesh  init_device_mesh

  init_device_mesh mesh_shape
  init_device_mesh mesh_shape  mesh_dim_names

Initializes a DeviceMesh based on device_type, mesh_shape, and mesh_dim_names parameters.

This creates a DeviceMesh with an n-dimensional array layout, where  is the length of mesh_shape.
If mesh_dim_names is provided, each dimension is labeled as mesh_dim_names[i].

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 13)

init_device_mesh follows SPMD programming model, meaning the same PyTorch Python program
runs on all processes/ranks in the cluster. Ensure mesh_shape (the dimensions of the nD array
describing device layout) is identical across all ranks. Inconsistent mesh_shape may lead to hanging.

If no process group is found, init_device_mesh will initialize distributed process group/groups
required for distributed communications behind the scene.

Parameters

device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
Passing in a device type with a GPU index, such as “cuda:0”, is not allowed.
mesh_shape () – A tuple defining the dimensions of the multi-dimensional array
describing the layout of devices.
mesh_dim_names () – A tuple of mesh dimension names to assign to each dimension
of the multi-dimensional array describing the layout of devices. Its length must match the length
of mesh_shape. Each string in mesh_dim_names must be unique.



A DeviceMesh object representing the device layout.

Return type
DeviceMesh

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 14)

List:
device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
Passing in a device type with a GPU index, such as “cuda:0”, is not allowed.
mesh_shape () – A tuple defining the dimensions of the multi-dimensional array
describing the layout of devices.
mesh_dim_names () – A tuple of mesh dimension names to assign to each dimension
of the multi-dimensional array describing the layout of devices. Its length must match the length
of mesh_shape. Each string in mesh_dim_names must be unique.

device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
Passing in a device type with a GPU index, such as “cuda:0”, is not allowed.

mesh_shape () – A tuple defining the dimensions of the multi-dimensional array
describing the layout of devices.

mesh_dim_names () – A tuple of mesh dimension names to assign to each dimension
of the multi-dimensional array describing the layout of devices. Its length must match the length
of mesh_shape. Each string in mesh_dim_names must be unique.

A DeviceMesh object representing the device layout.

Code example:
torch.distributed.device_mesh  init_device_mesh

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 15)

  init_device_mesh mesh_shape
  init_device_mesh mesh_shape  mesh_dim_names

torch.distributed.is_initialized
Check if the default process group has been initialized.

Return type

Check if the default process group has been initialized.

torch.distributed.is_mpi_available
Check if the MPI backend is available.

Return type

Check if the MPI backend is available.

torch.distributed.is_nccl_available
Check if the NCCL backend is available.

Return type

Check if the NCCL backend is available.

torch.distributed.is_gloo_available
Check if the Gloo backend is available.

Return type

Check if the Gloo backend is available.

torch.distributed.distributed_c10d.is_xccl_available
Check if the XCCL backend is available.

Return type

Check if the XCCL backend is available.

torch.distributed.is_torchelastic_launched
Check whether this process was launched with torch.distributed.elastic (aka torchelastic).
The existence of TORCHELASTIC_RUN_ID environment
variable is used as a proxy to determine whether the current process
was launched with torchelastic. This is a reasonable proxy since
TORCHELASTIC_RUN_ID maps to the rendezvous id which is always a
non-null value indicating the job id for peer discovery purposes..

================================================================================

# Distributed communication package - torch.distributed - Initialization (Part 16)

Return type

Check whether this process was launched with torch.distributed.elastic (aka torchelastic).

The existence of TORCHELASTIC_RUN_ID environment
variable is used as a proxy to determine whether the current process
was launched with torchelastic. This is a reasonable proxy since
TORCHELASTIC_RUN_ID maps to the rendezvous id which is always a
non-null value indicating the job id for peer discovery purposes..

torch.distributed.get_default_backend_for_device
Return the default backend for the given device.

Parameters
 (torch.device) – The device to get the default backend for.


The default backend for the given device as a lower case string.

Return type

Return the default backend for the given device.

Parameters
 (torch.device) – The device to get the default backend for.


The default backend for the given device as a lower case string.

Return type

(torch.device) – The device to get the default backend for.

The default backend for the given device as a lower case string.

Currently three initialization methods are supported:

================================================================================

# Distributed communication package - torch.distributed - TCP initialization

There are two ways to initialize using TCP, both requiring a network address
reachable from all processes and a desired world_size. The first way
requires specifying an address that belongs to the rank 0 process. This
initialization method requires that all processes have manually specified ranks.

Note that multicast address is not supported anymore in the latest distributed
package. group_name is deprecated as well.

Code example:
torch.distributed  

# Use address of one of the machines
init_process_group init_method'tcp://10.1.1.20:23456'
                         world_size

================================================================================

# Distributed communication package - torch.distributed - Shared file-system initialization (Part 1)

Another initialization method makes use of a file system that is shared and
visible from all machines in a group, along with a desired world_size. The URL should start
with  and contain a path to a non-existent file (in an existing
directory) on a shared file system. File-system initialization will automatically
create that file if it doesn’t exist, but will not delete the file. Therefore, it
is your responsibility to make sure that the file is cleaned up before the next
init_process_group() call on the same file path/name.

Note that automatic rank assignment is not supported anymore in the latest
distributed package and group_name is deprecated as well.

This method assumes that the file system supports locking using  - most
local systems and NFS support it.

================================================================================

# Distributed communication package - torch.distributed - Shared file-system initialization (Part 2)

This method will always create the file and try its best to clean up and remove
the file at the end of the program. In other words, each initialization with
the file init method will need a brand new empty file in order for the initialization
to succeed. If the same file used by the previous initialization (which happens not
to get cleaned up) is used again, this is unexpected behavior and can often cause
deadlocks and failures. Therefore, even though this method will try its best to clean up
the file, if the auto-delete happens to be unsuccessful, it is your responsibility
to ensure that the file is removed at the end of the training to prevent the same
file to be reused again during the next time. This is especially important
if you plan to call init_process_group() multiple times on the same file name.
In other words, if the file is not removed/cleaned up and you call
init_process_group() again on that file, failures are expected.
The rule of thumb here is that, make sure that the file is non-existent or
empty every time init_process_group() is called.

Code example:
torch.distributed  

================================================================================

# Distributed communication package - torch.distributed - Shared file-system initialization (Part 3)

# rank should always be specified
init_process_group init_method'file:///mnt/nfs/sharedfile'
                        world_size

================================================================================

# Distributed communication package - torch.distributed - Environment variable initialization

This method will read the configuration from environment variables, allowing
one to fully customize how the information is obtained. The variables to be set
are:

List:
MASTER_PORT - required; has to be a free port on machine with rank 0
MASTER_ADDR - required (except for rank 0); address of rank 0 node
WORLD_SIZE - required; can be set either here, or in a call to init function
 - required; can be set either here, or in a call to init function

MASTER_PORT - required; has to be a free port on machine with rank 0

MASTER_ADDR - required (except for rank 0); address of rank 0 node

WORLD_SIZE - required; can be set either here, or in a call to init function

- required; can be set either here, or in a call to init function

The machine with rank 0 will be used to set up all connections.

This is the default method, meaning that init_method does not have to be specified (or
can be ).

================================================================================

# Distributed communication package - torch.distributed - Improving initialization time

List:
TORCH_GLOO_LAZY_INIT - establishes connections on demand rather than
using a full mesh which can greatly improve initialization time for non all2all
operations.

TORCH_GLOO_LAZY_INIT - establishes connections on demand rather than
using a full mesh which can greatly improve initialization time for non all2all
operations.

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 1)

Once torch.distributed.init_process_group() was run, the following functions can be used. To
check whether the process group has already been initialized use torch.distributed.is_initialized().

torch.distributed.
An enum-like class for backends.
Available backends: GLOO, NCCL, UCC, MPI, XCCL, and other registered backends.
The values of this class are lowercase strings, e.g., . They can
be accessed as attributes, e.g., Backend.NCCL.
This class can be directly called to parse the string, e.g.,
Backend(backend_str) will check if backend_str is valid, and
return the parsed lowercase string if so. It also accepts uppercase strings,
e.g., Backend("GLOO") returns .


The entry Backend.UNDEFINED is present but only used as
initial value of some fields. Users should neither use it directly
nor assume its existence.




classmethodregister_backend, , extended_api, 
Register a new backend with the given name and instantiating function.
This class method is used by 3rd party ProcessGroup extension to
register new backends.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 2)

 () – Backend name of the ProcessGroup extension. It
should match the one in init_process_group().
 () – Function handler that instantiates the backend.
The function should be implemented in the backend
extension and takes four arguments, including
, , world_size, and .
extended_api () – Whether the backend supports extended argument structure.
Default: . If set to , the backend
will get an instance of c10d::DistributedBackendOptions, and
a process group options object as defined by the backend implementation.
 () – device type this backend
supports, e.g. “cpu”, “cuda”, etc. If ,
assuming both “cpu” and “cuda”





This support of 3rd party backend is experimental and subject to change.

An enum-like class for backends.

Available backends: GLOO, NCCL, UCC, MPI, XCCL, and other registered backends.

The values of this class are lowercase strings, e.g., . They can
be accessed as attributes, e.g., Backend.NCCL.

This class can be directly called to parse the string, e.g.,
Backend(backend_str) will check if backend_str is valid, and
return the parsed lowercase string if so. It also accepts uppercase strings,
e.g., Backend("GLOO") returns .

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 3)

The entry Backend.UNDEFINED is present but only used as
initial value of some fields. Users should neither use it directly
nor assume its existence.

classmethodregister_backend, , extended_api, 
Register a new backend with the given name and instantiating function.
This class method is used by 3rd party ProcessGroup extension to
register new backends.

Parameters

 () – Backend name of the ProcessGroup extension. It
should match the one in init_process_group().
 () – Function handler that instantiates the backend.
The function should be implemented in the backend
extension and takes four arguments, including
, , world_size, and .
extended_api () – Whether the backend supports extended argument structure.
Default: . If set to , the backend
will get an instance of c10d::DistributedBackendOptions, and
a process group options object as defined by the backend implementation.
 () – device type this backend
supports, e.g. “cpu”, “cuda”, etc. If ,
assuming both “cpu” and “cuda”





This support of 3rd party backend is experimental and subject to change.

Register a new backend with the given name and instantiating function.

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 4)

This class method is used by 3rd party ProcessGroup extension to
register new backends.

Parameters

 () – Backend name of the ProcessGroup extension. It
should match the one in init_process_group().
 () – Function handler that instantiates the backend.
The function should be implemented in the backend
extension and takes four arguments, including
, , world_size, and .
extended_api () – Whether the backend supports extended argument structure.
Default: . If set to , the backend
will get an instance of c10d::DistributedBackendOptions, and
a process group options object as defined by the backend implementation.
 () – device type this backend
supports, e.g. “cpu”, “cuda”, etc. If ,
assuming both “cpu” and “cuda”

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 5)

List:
() – Backend name of the ProcessGroup extension. It
should match the one in init_process_group().
 () – Function handler that instantiates the backend.
The function should be implemented in the backend
extension and takes four arguments, including
, , world_size, and .
extended_api () – Whether the backend supports extended argument structure.
Default: . If set to , the backend
will get an instance of c10d::DistributedBackendOptions, and
a process group options object as defined by the backend implementation.
 () – device type this backend
supports, e.g. “cpu”, “cuda”, etc. If ,
assuming both “cpu” and “cuda”

() – Backend name of the ProcessGroup extension. It
should match the one in init_process_group().

() – Function handler that instantiates the backend.
The function should be implemented in the backend
extension and takes four arguments, including
, , world_size, and .

extended_api () – Whether the backend supports extended argument structure.
Default: . If set to , the backend
will get an instance of c10d::DistributedBackendOptions, and
a process group options object as defined by the backend implementation.

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 6)

() – device type this backend
supports, e.g. “cpu”, “cuda”, etc. If ,
assuming both “cpu” and “cuda”

This support of 3rd party backend is experimental and subject to change.

torch.distributed.get_backend
Return the backend of the given process group.

Parameters
 (ProcessGroup) – The process group to work on. The
default is the general main process group. If another specific group
is specified, the calling process must be part of .


The backend of the given process group as a lower case string.

Return type

Return the backend of the given process group.

Parameters
 (ProcessGroup) – The process group to work on. The
default is the general main process group. If another specific group
is specified, the calling process must be part of .


The backend of the given process group as a lower case string.

Return type

(ProcessGroup) – The process group to work on. The
default is the general main process group. If another specific group
is specified, the calling process must be part of .

The backend of the given process group as a lower case string.

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 7)

torch.distributed.
Return the rank of the current process in the provided , default otherwise.
Rank is a unique identifier assigned to each process within a distributed
process group. They are always consecutive integers ranging from 0 to
world_size.

Parameters
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.


The rank of the process group
-1, if not part of the group

Return type

Return the rank of the current process in the provided , default otherwise.

Rank is a unique identifier assigned to each process within a distributed
process group. They are always consecutive integers ranging from 0 to
world_size.

Parameters
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.


The rank of the process group
-1, if not part of the group

Return type

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

The rank of the process group
-1, if not part of the group

torch.distributed.get_world_size
Return the number of processes in the current process group.

Parameters
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.

================================================================================

# Distributed communication package - torch.distributed - Post-Initialization (Part 8)


The world size of the process group
-1, if not part of the group

Return type

Return the number of processes in the current process group.

Parameters
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.


The world size of the process group
-1, if not part of the group

Return type

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

The world size of the process group
-1, if not part of the group

================================================================================

# Distributed communication package - torch.distributed - Shutdown

It is important to clean up resources on exit by calling destroy_process_group().

The simplest pattern to follow is to destroy every process group and backend by calling
destroy_process_group() with the default value of None for the  argument, at a
point in the training script where communications are no longer needed, usually near the
end of main(). The call should be made once per trainer-process, not at the outer
process-launcher level.

if destroy_process_group() is not called by all ranks in a pg within the timeout duration,
especially when there are multiple process-groups in the application e.g. for N-D parallelism,
hangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort,
which must be called collectively, but the order of calling ProcessGroupNCCL’s destructor if called
by python’s GC is not deterministic. Calling destroy_process_group() helps by ensuring
ncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort
during ProcessGroupNCCL’s destructor.

================================================================================

# Distributed communication package - torch.distributed - Reinitialization

destroy_process_group can also be used to destroy individual process groups. One use
case could be fault tolerant training, where a process group may be destroyed and then
a new one initialized during runtime. In this case, it’s critical to synchronize the trainer
processes using some means other than torch.distributed primitives _after_ calling destroy and
before subsequently initializing. This behavior is currently unsupported/untested, due to
the difficulty of achieving this synchronization, and is considered a known issue. Please file
a github issue or RFC if this is a use case that’s blocking you.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 1)

By default collectives operate on the default group (also called the world) and
require all processes to enter the distributed function call. However, some workloads can benefit
from more fine-grained communication. This is where distributed groups come
into play. new_group() function can be
used to create new groups, with arbitrary subsets of all processes. It returns
an opaque group handle that can be given as a  argument to all collectives
(collectives are distributed functions to exchange information in certain well-known programming patterns).

torch.distributed., , , pg_options, use_local_synchronization, group_desc, 
Create a new distributed group.
This function requires that all processes in the main group (i.e. all
processes that are part of the distributed job) enter this function, even
if they are not going to be members of the group. Additionally, groups
should be created in the same order in all processes.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 2)


Safe concurrent usage:
When using multiple process groups with the  backend, the user
must ensure a globally consistent execution order of collectives across
ranks.
If multiple threads within a process issue collectives, explicit
synchronization is necessary to ensure consistent ordering.
When using async variants of torch.distributed communication APIs,
a work object is returned and the communication kernel is
enqueued on a separate CUDA stream, allowing overlap of communication
and computation. Once one or more async ops have been issued on one process
group, they must be synchronized with other cuda streams by calling work.wait()
before using another process group.
See Using multiple NCCL communicators concurrently
<https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using-multiple-nccl-communicators-concurrently>
for more details.


Parameters

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 3)

 () – List of ranks of group members. If , will be
set to all ranks. Default is .
 () – see init_process_group for details and default value.
 () – The backend to use. Depending on
build-time configurations, valid values are  and .
By default uses the same backend as the global group. This field
should be given as a lowercase string (e.g., ), which can
also be accessed via  attributes (e.g.,
Backend.GLOO). If  is passed in, the backend
corresponding to the default process group will be used. Default is
.
pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. i.e. for the 
backend, is_high_priority_stream can be specified so that
process group can pick up high priority cuda streams. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-tuse_local_synchronization
(bool, optional): perform a group-local barrier at the end of the process group creation.
This is different in that non-member ranks don’t need to call into API and don’t
join the barrier.
group_desc () – a string to describe the process group.
 (torch.device) – a single, specific device
to “bind” this process to,  The  call will try to initialize
a communication backend immediately for the device if this field is given.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 4)



A handle of distributed group that can be given to collective calls or
GroupMember.NON_GROUP_MEMBER if the rank is not part of .


N.B. use_local_synchronization doesn’t work with MPI.
N.B. While use_local_synchronization=True can be significantly faster with larger
clusters and small process groups, care must be taken since it changes cluster behavior
as non-member ranks don’t join the group barrier().
N.B. use_local_synchronization=True can lead to deadlocks when each rank creates
multiple overlapping process groups. To avoid that, make sure all ranks follow the
same global creation order.

Create a new distributed group.

This function requires that all processes in the main group (i.e. all
processes that are part of the distributed job) enter this function, even
if they are not going to be members of the group. Additionally, groups
should be created in the same order in all processes.

Safe concurrent usage:
When using multiple process groups with the  backend, the user
must ensure a globally consistent execution order of collectives across
ranks.

If multiple threads within a process issue collectives, explicit
synchronization is necessary to ensure consistent ordering.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 5)

When using async variants of torch.distributed communication APIs,
a work object is returned and the communication kernel is
enqueued on a separate CUDA stream, allowing overlap of communication
and computation. Once one or more async ops have been issued on one process
group, they must be synchronized with other cuda streams by calling work.wait()
before using another process group.

See Using multiple NCCL communicators concurrently
<https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using-multiple-nccl-communicators-concurrently>
for more details.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 6)

 () – List of ranks of group members. If , will be
set to all ranks. Default is .
 () – see init_process_group for details and default value.
 () – The backend to use. Depending on
build-time configurations, valid values are  and .
By default uses the same backend as the global group. This field
should be given as a lowercase string (e.g., ), which can
also be accessed via  attributes (e.g.,
Backend.GLOO). If  is passed in, the backend
corresponding to the default process group will be used. Default is
.
pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. i.e. for the 
backend, is_high_priority_stream can be specified so that
process group can pick up high priority cuda streams. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-tuse_local_synchronization
(bool, optional): perform a group-local barrier at the end of the process group creation.
This is different in that non-member ranks don’t need to call into API and don’t
join the barrier.
group_desc () – a string to describe the process group.
 (torch.device) – a single, specific device
to “bind” this process to,  The  call will try to initialize
a communication backend immediately for the device if this field is given.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 7)



A handle of distributed group that can be given to collective calls or
GroupMember.NON_GROUP_MEMBER if the rank is not part of .

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 8)

List:
() – List of ranks of group members. If , will be
set to all ranks. Default is .
 () – see init_process_group for details and default value.
 () – The backend to use. Depending on
build-time configurations, valid values are  and .
By default uses the same backend as the global group. This field
should be given as a lowercase string (e.g., ), which can
also be accessed via  attributes (e.g.,
Backend.GLOO). If  is passed in, the backend
corresponding to the default process group will be used. Default is
.
pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. i.e. for the 
backend, is_high_priority_stream can be specified so that
process group can pick up high priority cuda streams. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-tuse_local_synchronization
(bool, optional): perform a group-local barrier at the end of the process group creation.
This is different in that non-member ranks don’t need to call into API and don’t
join the barrier.
group_desc () – a string to describe the process group.
 (torch.device) – a single, specific device
to “bind” this process to,  The  call will try to initialize
a communication backend immediately for the device if this field is given.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 9)

() – List of ranks of group members. If , will be
set to all ranks. Default is .

() – see init_process_group for details and default value.

() – The backend to use. Depending on
build-time configurations, valid values are  and .
By default uses the same backend as the global group. This field
should be given as a lowercase string (e.g., ), which can
also be accessed via  attributes (e.g.,
Backend.GLOO). If  is passed in, the backend
corresponding to the default process group will be used. Default is
.

pg_options (ProcessGroupOptions) – process group options
specifying what additional options need to be passed in during
the construction of specific process groups. i.e. for the 
backend, is_high_priority_stream can be specified so that
process group can pick up high priority cuda streams. For other available options to config nccl,
See https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-tuse_local_synchronization
(bool, optional): perform a group-local barrier at the end of the process group creation.
This is different in that non-member ranks don’t need to call into API and don’t
join the barrier.

group_desc () – a string to describe the process group.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 10)

(torch.device) – a single, specific device
to “bind” this process to,  The  call will try to initialize
a communication backend immediately for the device if this field is given.

A handle of distributed group that can be given to collective calls or
GroupMember.NON_GROUP_MEMBER if the rank is not part of .

N.B. use_local_synchronization doesn’t work with MPI.

N.B. While use_local_synchronization=True can be significantly faster with larger
clusters and small process groups, care must be taken since it changes cluster behavior
as non-member ranks don’t join the group barrier().

N.B. use_local_synchronization=True can lead to deadlocks when each rank creates
multiple overlapping process groups. To avoid that, make sure all ranks follow the
same global creation order.

torch.distributed.get_group_rank, global_rank
Translate a global rank into a group rank.
global_rank must be part of  otherwise this raises RuntimeError.

Parameters

 (ProcessGroup) – ProcessGroup to find the relative rank.
global_rank () – Global rank to query.



Group rank of global_rank relative to 

Return type



N.B. calling this function on the default process group returns identity

Translate a global rank into a group rank.

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 11)

global_rank must be part of  otherwise this raises RuntimeError.

Parameters

 (ProcessGroup) – ProcessGroup to find the relative rank.
global_rank () – Global rank to query.



Group rank of global_rank relative to 

Return type

List:
(ProcessGroup) – ProcessGroup to find the relative rank.
global_rank () – Global rank to query.

(ProcessGroup) – ProcessGroup to find the relative rank.

global_rank () – Global rank to query.

Group rank of global_rank relative to

N.B. calling this function on the default process group returns identity

torch.distributed.get_global_rank, group_rank
Translate a group rank into a global rank.
group_rank must be part of  otherwise this raises RuntimeError.

Parameters

 (ProcessGroup) – ProcessGroup to find the global rank from.
group_rank () – Group rank to query.



Global rank of group_rank relative to 

Return type



N.B. calling this function on the default process group returns identity

Translate a group rank into a global rank.

group_rank must be part of  otherwise this raises RuntimeError.

Parameters

 (ProcessGroup) – ProcessGroup to find the global rank from.
group_rank () – Group rank to query.



Global rank of group_rank relative to 

Return type

================================================================================

# Distributed communication package - torch.distributed - Groups (Part 12)

List:
(ProcessGroup) – ProcessGroup to find the global rank from.
group_rank () – Group rank to query.

(ProcessGroup) – ProcessGroup to find the global rank from.

group_rank () – Group rank to query.

Global rank of group_rank relative to

N.B. calling this function on the default process group returns identity

torch.distributed.get_process_group_ranks
Get all ranks associated with .

Parameters
 (ProcessGroup) – ProcessGroup to get all ranks from.
If None, the default process group will be used.


List of global ranks ordered by group rank.

Return type
[]

Get all ranks associated with .

Parameters
 (ProcessGroup) – ProcessGroup to get all ranks from.
If None, the default process group will be used.


List of global ranks ordered by group rank.

Return type
[]

(ProcessGroup) – ProcessGroup to get all ranks from.
If None, the default process group will be used.

List of global ranks ordered by group rank.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 1)

DeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators).
It allows user to easily create inter node and intra node process groups without worrying about
how to set up the ranks correctly for different sub process groups, and it helps manage those
distributed process group easily. init_device_mesh() function can be
used to create new DeviceMesh, with a mesh shape describing the device topology.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 2)

torch.distributed.device_mesh.DeviceMeshdevice_type, , , mesh_dim_names, _init_backend
DeviceMesh represents a mesh of devices, where layout of devices could be
represented as a n-d dimension array, and each value of the n-d dimensional
array is the global id of the default process group ranks.
DeviceMesh could be used to setup the N dimensional device connections across the cluster,
and manage the ProcessGroups for N dimensional parallelisms. Communications could happen on
each dimension of the DeviceMesh separately. DeviceMesh respects the device that user selects
already (i.e. if user call torch.cuda.set_device before the DeviceMesh initialization),
and will select/set the device for the current process if user does not set the device
beforehand. Note that manual device selection should happen BEFORE the DeviceMesh initialization.
DeviceMesh can also be used as a context manager when using together with DTensor APIs.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 3)


DeviceMesh follows SPMD programming model, which means the same PyTorch Python program
is running on all processes/ranks in the cluster. Therefore, users need to make sure the
 array (which describes the layout of devices) should be identical across all ranks.
Inconsistent  will lead to silent hang.


Parameters

device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
 () – A multi-dimensional array or an integer tensor describing the layout
of devices, where the IDs are global IDs of the default process group.



A DeviceMesh object representing the device layout.

Return type
DeviceMesh


The following program runs on each process/rank in an SPMD manner. In this example, we have 2
hosts with 4 GPUs each.
A reduction over the first dimension of mesh will reduce across
columns (0, 4), .. and (3, 7), a reduction over the second dimension
of mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7).

 torch.distributed.device_mesh  DeviceMesh

# Initialize device mesh as (2, 4) to represent the topology
# of cross-host(dim 0), and within-host (dim 1).
  DeviceMeshdevice_type       



================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 4)


from_group, device_type, , , mesh_dim_names
Constructs a DeviceMesh with device_type from an
existing ProcessGroup or a list of existing ProcessGroup.
The constructed device mesh has number of dimensions equal to the
number of groups passed. For example, if a single process group is passed in,
the resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in,
the resulted DeviceMesh is a 2D mesh.
If more than one group is passed, then the  and mesh_dim_names arguments
are required. The order of the process groups passed in determines the topology of
the mesh. For example, the first process group will be the 0th dimension of the DeviceMesh.
The  tensor passed in must have the same number of dimensions as the number of process
groups passed in, and the order of the dimensions in the  tensor must match the order
in the process groups passed in.

Parameters

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 5)

 (ProcessGroupProcessGroup) – the existing ProcessGroup
or a list of existing ProcessGroups.
device_type () – The device type of the mesh. Currently supports: “cpu”,
“cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”,
is not allowed.
 (torch.Tensor) – A multi-dimensional array or an
integer tensor describing the layout of devices, where the IDs are global IDs
of the default process group. Default is None.
mesh_dim_names () – A tuple of mesh dimension names to assign
to each dimension of the multi-dimensional array describing the layout of devices.
Its length must match the length of mesh_shape. Each string in mesh_dim_names
must be unique. Default is None.



A DeviceMesh object representing the device layout.

Return type
DeviceMesh





get_all_groups
Returns a list of ProcessGroups for all mesh dimensions.


A list of ProcessGroup object.

Return type
[torch.distributed.distributed_c10d.ProcessGroup]





get_coordinate
Return the relative indices of this rank relative to all
dimensions of the mesh. If this rank is not part of the mesh, return None.

Return type
[[]]





================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 6)


Returns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the
DeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh.

Parameters

 (str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) – 



A ProcessGroup object.

Return type
ProcessGroup





get_local_rank
Returns the local rank of the given mesh_dim of the DeviceMesh.

Parameters

 (str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) – 



An integer denotes the local rank.

Return type



The following program runs on each process/rank in an SPMD manner. In this example, we have 2
hosts with 4 GPUs each.
Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0.
Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.

 torch.distributed.device_mesh  DeviceMesh

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 7)

# Initialize device mesh as (2, 4) to represent the topology
# of cross-host(dim 0), and within-host (dim 1).
  DeviceMeshdevice_type       






Returns the current global rank.

Return type

DeviceMesh represents a mesh of devices, where layout of devices could be
represented as a n-d dimension array, and each value of the n-d dimensional
array is the global id of the default process group ranks.

DeviceMesh could be used to setup the N dimensional device connections across the cluster,
and manage the ProcessGroups for N dimensional parallelisms. Communications could happen on
each dimension of the DeviceMesh separately. DeviceMesh respects the device that user selects
already (i.e. if user call torch.cuda.set_device before the DeviceMesh initialization),
and will select/set the device for the current process if user does not set the device
beforehand. Note that manual device selection should happen BEFORE the DeviceMesh initialization.

DeviceMesh can also be used as a context manager when using together with DTensor APIs.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 8)

DeviceMesh follows SPMD programming model, which means the same PyTorch Python program
is running on all processes/ranks in the cluster. Therefore, users need to make sure the
 array (which describes the layout of devices) should be identical across all ranks.
Inconsistent  will lead to silent hang.

Parameters

device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
 () – A multi-dimensional array or an integer tensor describing the layout
of devices, where the IDs are global IDs of the default process group.



A DeviceMesh object representing the device layout.

Return type
DeviceMesh

List:
device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.
 () – A multi-dimensional array or an integer tensor describing the layout
of devices, where the IDs are global IDs of the default process group.

device_type () – The device type of the mesh. Currently supports: “cpu”, “cuda/cuda-like”.

() – A multi-dimensional array or an integer tensor describing the layout
of devices, where the IDs are global IDs of the default process group.

A DeviceMesh object representing the device layout.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 9)

The following program runs on each process/rank in an SPMD manner. In this example, we have 2
hosts with 4 GPUs each.
A reduction over the first dimension of mesh will reduce across
columns (0, 4), .. and (3, 7), a reduction over the second dimension
of mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7).

Code example:
torch.distributed.device_mesh  DeviceMesh

# Initialize device mesh as (2, 4) to represent the topology
# of cross-host(dim 0), and within-host (dim 1).
  DeviceMeshdevice_type

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 10)

from_group, device_type, , , mesh_dim_names
Constructs a DeviceMesh with device_type from an
existing ProcessGroup or a list of existing ProcessGroup.
The constructed device mesh has number of dimensions equal to the
number of groups passed. For example, if a single process group is passed in,
the resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in,
the resulted DeviceMesh is a 2D mesh.
If more than one group is passed, then the  and mesh_dim_names arguments
are required. The order of the process groups passed in determines the topology of
the mesh. For example, the first process group will be the 0th dimension of the DeviceMesh.
The  tensor passed in must have the same number of dimensions as the number of process
groups passed in, and the order of the dimensions in the  tensor must match the order
in the process groups passed in.

Parameters

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 11)

 (ProcessGroupProcessGroup) – the existing ProcessGroup
or a list of existing ProcessGroups.
device_type () – The device type of the mesh. Currently supports: “cpu”,
“cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”,
is not allowed.
 (torch.Tensor) – A multi-dimensional array or an
integer tensor describing the layout of devices, where the IDs are global IDs
of the default process group. Default is None.
mesh_dim_names () – A tuple of mesh dimension names to assign
to each dimension of the multi-dimensional array describing the layout of devices.
Its length must match the length of mesh_shape. Each string in mesh_dim_names
must be unique. Default is None.



A DeviceMesh object representing the device layout.

Return type
DeviceMesh

Constructs a DeviceMesh with device_type from an
existing ProcessGroup or a list of existing ProcessGroup.

The constructed device mesh has number of dimensions equal to the
number of groups passed. For example, if a single process group is passed in,
the resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in,
the resulted DeviceMesh is a 2D mesh.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 12)

If more than one group is passed, then the  and mesh_dim_names arguments
are required. The order of the process groups passed in determines the topology of
the mesh. For example, the first process group will be the 0th dimension of the DeviceMesh.
The  tensor passed in must have the same number of dimensions as the number of process
groups passed in, and the order of the dimensions in the  tensor must match the order
in the process groups passed in.

Parameters

 (ProcessGroupProcessGroup) – the existing ProcessGroup
or a list of existing ProcessGroups.
device_type () – The device type of the mesh. Currently supports: “cpu”,
“cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”,
is not allowed.
 (torch.Tensor) – A multi-dimensional array or an
integer tensor describing the layout of devices, where the IDs are global IDs
of the default process group. Default is None.
mesh_dim_names () – A tuple of mesh dimension names to assign
to each dimension of the multi-dimensional array describing the layout of devices.
Its length must match the length of mesh_shape. Each string in mesh_dim_names
must be unique. Default is None.



================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 13)

A DeviceMesh object representing the device layout.

Return type
DeviceMesh

List:
(ProcessGroupProcessGroup) – the existing ProcessGroup
or a list of existing ProcessGroups.
device_type () – The device type of the mesh. Currently supports: “cpu”,
“cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”,
is not allowed.
 (torch.Tensor) – A multi-dimensional array or an
integer tensor describing the layout of devices, where the IDs are global IDs
of the default process group. Default is None.
mesh_dim_names () – A tuple of mesh dimension names to assign
to each dimension of the multi-dimensional array describing the layout of devices.
Its length must match the length of mesh_shape. Each string in mesh_dim_names
must be unique. Default is None.

(ProcessGroupProcessGroup) – the existing ProcessGroup
or a list of existing ProcessGroups.

device_type () – The device type of the mesh. Currently supports: “cpu”,
“cuda/cuda-like”. Passing in a device type with a GPU index, such as “cuda:0”,
is not allowed.

(torch.Tensor) – A multi-dimensional array or an
integer tensor describing the layout of devices, where the IDs are global IDs
of the default process group. Default is None.

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 14)

mesh_dim_names () – A tuple of mesh dimension names to assign
to each dimension of the multi-dimensional array describing the layout of devices.
Its length must match the length of mesh_shape. Each string in mesh_dim_names
must be unique. Default is None.

A DeviceMesh object representing the device layout.

get_all_groups
Returns a list of ProcessGroups for all mesh dimensions.


A list of ProcessGroup object.

Return type
[torch.distributed.distributed_c10d.ProcessGroup]

Returns a list of ProcessGroups for all mesh dimensions.

A list of ProcessGroup object.

Return type
[torch.distributed.distributed_c10d.ProcessGroup]

A list of ProcessGroup object.

[torch.distributed.distributed_c10d.ProcessGroup]

get_coordinate
Return the relative indices of this rank relative to all
dimensions of the mesh. If this rank is not part of the mesh, return None.

Return type
[[]]

Return the relative indices of this rank relative to all
dimensions of the mesh. If this rank is not part of the mesh, return None.

Returns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the
DeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh.

Parameters

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 15)

 (str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) – 



A ProcessGroup object.

Return type
ProcessGroup

Returns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the
DeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh.

Parameters

 (str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) – 



A ProcessGroup object.

Return type
ProcessGroup

List:
(str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) –

(str/python:int) – it can be the name of the mesh dimension or the index

(of the mesh dimension. Default is) –

A ProcessGroup object.

get_local_rank
Returns the local rank of the given mesh_dim of the DeviceMesh.

Parameters

 (str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) – 



An integer denotes the local rank.

Return type



================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 16)

The following program runs on each process/rank in an SPMD manner. In this example, we have 2
hosts with 4 GPUs each.
Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0.
Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.

 torch.distributed.device_mesh  DeviceMesh

# Initialize device mesh as (2, 4) to represent the topology
# of cross-host(dim 0), and within-host (dim 1).
  DeviceMeshdevice_type

Returns the local rank of the given mesh_dim of the DeviceMesh.

Parameters

 (str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) – 



An integer denotes the local rank.

Return type

List:
(str/python:int) – it can be the name of the mesh dimension or the index
 (of the mesh dimension. Default is) –

(str/python:int) – it can be the name of the mesh dimension or the index

(of the mesh dimension. Default is) –

================================================================================

# Distributed communication package - torch.distributed - DeviceMesh (Part 17)

An integer denotes the local rank.

The following program runs on each process/rank in an SPMD manner. In this example, we have 2
hosts with 4 GPUs each.
Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0.
Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2.
Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.

Code example:
torch.distributed.device_mesh  DeviceMesh

# Initialize device mesh as (2, 4) to represent the topology
# of cross-host(dim 0), and within-host (dim 1).
  DeviceMeshdevice_type

Returns the current global rank.

Return type

Returns the current global rank.

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 1)

torch.distributed., , , , 
Send a tensor synchronously.


 is not supported with the NCCL backend.


Parameters

 () – Tensor to send.
 () – Destination rank on global process group (regardless of  argument).
Destination rank should not be the same as the rank of the current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with remote recv
 () – Destination rank on .  Invalid to specify both  and .

Send a tensor synchronously.

is not supported with the NCCL backend.

Parameters

 () – Tensor to send.
 () – Destination rank on global process group (regardless of  argument).
Destination rank should not be the same as the rank of the current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with remote recv
 () – Destination rank on .  Invalid to specify both  and .

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 2)

List:
() – Tensor to send.
 () – Destination rank on global process group (regardless of  argument).
Destination rank should not be the same as the rank of the current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with remote recv
 () – Destination rank on .  Invalid to specify both  and .

() – Destination rank on global process group (regardless of  argument).
Destination rank should not be the same as the rank of the current process.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Tag to match send with remote recv

() – Destination rank on .  Invalid to specify both  and .

torch.distributed., , , , 
Receives a tensor synchronously.


 is not supported with the NCCL backend.


Parameters

 () – Tensor to fill with received data.
 () – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match recv with remote send
 () – Destination rank on .  Invalid to specify both  and .



================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 3)

Sender rank
-1, if not part of the group

Return type

Receives a tensor synchronously.

is not supported with the NCCL backend.

Parameters

 () – Tensor to fill with received data.
 () – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match recv with remote send
 () – Destination rank on .  Invalid to specify both  and .



Sender rank
-1, if not part of the group

Return type

List:
() – Tensor to fill with received data.
 () – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match recv with remote send
 () – Destination rank on .  Invalid to specify both  and .

() – Tensor to fill with received data.

() – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Tag to match recv with remote send

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 4)

() – Destination rank on .  Invalid to specify both  and .

Sender rank
-1, if not part of the group

and 
return distributed request objects when used. In general, the type of this object is unspecified
as they should never be created manually, but they are guaranteed to support two methods:

List:
is_completed() - returns True if the operation has finished
 - will block the process until the operation is finished.
is_completed() is guaranteed to return True once it returns.

is_completed() - returns True if the operation has finished

- will block the process until the operation is finished.
is_completed() is guaranteed to return True once it returns.

torch.distributed., , , , 
Send a tensor asynchronously.


Modifying  before the request completes causes undefined
behavior.



 is not supported with the NCCL backend.

Unlike send, which is blocking, isend allows src == dst rank, i.e. send to self.

Parameters

 () – Tensor to send.
 () – Destination rank on global process group (regardless of  argument)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with remote recv
 () – Destination rank on .  Invalid to specify both  and 



================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 5)

A distributed request object.
None, if not part of the group

Return type
[]

Send a tensor asynchronously.

Modifying  before the request completes causes undefined
behavior.

is not supported with the NCCL backend.

Unlike send, which is blocking, isend allows src == dst rank, i.e. send to self.

Parameters

 () – Tensor to send.
 () – Destination rank on global process group (regardless of  argument)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with remote recv
 () – Destination rank on .  Invalid to specify both  and 



A distributed request object.
None, if not part of the group

Return type
[]

List:
() – Tensor to send.
 () – Destination rank on global process group (regardless of  argument)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with remote recv
 () – Destination rank on .  Invalid to specify both  and

() – Destination rank on global process group (regardless of  argument)

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Tag to match send with remote recv

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 6)

() – Destination rank on .  Invalid to specify both  and

A distributed request object.
None, if not part of the group

torch.distributed., , , , 
Receives a tensor asynchronously.


 is not supported with the NCCL backend.

Unlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.

Parameters

 () – Tensor to fill with received data.
 () – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match recv with remote send
 () – Destination rank on .  Invalid to specify both  and .



A distributed request object.
None, if not part of the group

Return type
[]

Receives a tensor asynchronously.

is not supported with the NCCL backend.

Unlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 7)

 () – Tensor to fill with received data.
 () – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match recv with remote send
 () – Destination rank on .  Invalid to specify both  and .



A distributed request object.
None, if not part of the group

Return type
[]

List:
() – Tensor to fill with received data.
 () – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match recv with remote send
 () – Destination rank on .  Invalid to specify both  and .

() – Tensor to fill with received data.

() – Source rank on global process group (regardless of  argument).
Will receive from any process if unspecified.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Tag to match recv with remote send

() – Destination rank on .  Invalid to specify both  and .

A distributed request object.
None, if not part of the group

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 8)

torch.distributed.send_object_listobject_list, , , , 
Sends picklable objects in object_list synchronously.
Similar to , but Python objects can be passed in.
Note that all objects in object_list must be picklable in order to be
sent.

Parameters

object_list () – List of input objects to sent.
Each object must be picklable. Receiver must provide lists of equal sizes.
 () – Destination rank to send object_list to.
Destination rank is based on global process group (regardless of  argument)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before sending. Default is .
 () – Destination rank on .
Must specify one of  and  but not both



.




For NCCL-based process groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().



================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 9)

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.



send_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.



Calling send_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.


# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes backend is not NCCL
  
   
    # Assumes world_size of 2.
          # any picklable object
    send_object_list  

        
    recv_object_list  

['foo', 12, {1: 2}]

Sends picklable objects in object_list synchronously.

Similar to , but Python objects can be passed in.
Note that all objects in object_list must be picklable in order to be
sent.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 10)

object_list () – List of input objects to sent.
Each object must be picklable. Receiver must provide lists of equal sizes.
 () – Destination rank to send object_list to.
Destination rank is based on global process group (regardless of  argument)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before sending. Default is .
 () – Destination rank on .
Must specify one of  and  but not both



.

List:
object_list () – List of input objects to sent.
Each object must be picklable. Receiver must provide lists of equal sizes.
 () – Destination rank to send object_list to.
Destination rank is based on global process group (regardless of  argument)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before sending. Default is .
 () – Destination rank on .
Must specify one of  and  but not both

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 11)

object_list () – List of input objects to sent.
Each object must be picklable. Receiver must provide lists of equal sizes.

() – Destination rank to send object_list to.
Destination rank is based on global process group (regardless of  argument)

(ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .

(torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before sending. Default is .

() – Destination rank on .
Must specify one of  and  but not both

For NCCL-based process groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 12)

send_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.

Calling send_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes backend is not NCCL
  
   
    # Assumes world_size of 2.
          # any picklable object
    send_object_list  

        
    recv_object_list  

['foo', 12, {1: 2}]

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes backend is not NCCL
  
   
    # Assumes world_size of 2.
          # any picklable object
    send_object_list  

        
    recv_object_list  

['foo', 12, {1: 2}]

torch.distributed.recv_object_listobject_list, , , , 
Receives picklable objects in object_list synchronously.
Similar to , but can receive Python objects.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 13)

object_list () – List of objects to receive into.
Must provide a list of sizes equal to the size of the list being sent.
 () – Source rank from which to recv object_list.
Source rank is based on global process group (regardless of  argument)
Will receive from any rank if set to None. Default is .
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, receives on this device.
Default is .
 () – Destination rank on .  Invalid to specify both  and .



Sender rank. -1 if rank is not part of the group. If rank is part of the group,
object_list will contain the sent objects from  rank.




For NCCL-based process groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().



Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.



================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 14)

recv_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.



Calling recv_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.


# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes backend is not NCCL
  
   
    # Assumes world_size of 2.
          # any picklable object
    send_object_list  

        
    recv_object_list  

['foo', 12, {1: 2}]

Receives picklable objects in object_list synchronously.

Similar to , but can receive Python objects.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 15)

object_list () – List of objects to receive into.
Must provide a list of sizes equal to the size of the list being sent.
 () – Source rank from which to recv object_list.
Source rank is based on global process group (regardless of  argument)
Will receive from any rank if set to None. Default is .
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, receives on this device.
Default is .
 () – Destination rank on .  Invalid to specify both  and .



Sender rank. -1 if rank is not part of the group. If rank is part of the group,
object_list will contain the sent objects from  rank.

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 16)

List:
object_list () – List of objects to receive into.
Must provide a list of sizes equal to the size of the list being sent.
 () – Source rank from which to recv object_list.
Source rank is based on global process group (regardless of  argument)
Will receive from any rank if set to None. Default is .
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, receives on this device.
Default is .
 () – Destination rank on .  Invalid to specify both  and .

object_list () – List of objects to receive into.
Must provide a list of sizes equal to the size of the list being sent.

() – Source rank from which to recv object_list.
Source rank is based on global process group (regardless of  argument)
Will receive from any rank if set to None. Default is .

(ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .

(torch.device, optional) – If not None, receives on this device.
Default is .

() – Destination rank on .  Invalid to specify both  and .

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 17)

Sender rank. -1 if rank is not part of the group. If rank is part of the group,
object_list will contain the sent objects from  rank.

For NCCL-based process groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.

recv_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.

Calling recv_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes backend is not NCCL
  
   
    # Assumes world_size of 2.
          # any picklable object
    send_object_list  

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 18)

        
    recv_object_list  

['foo', 12, {1: 2}]

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes backend is not NCCL
  
   
    # Assumes world_size of 2.
          # any picklable object
    send_object_list  

        
    recv_object_list  

['foo', 12, {1: 2}]

torch.distributed.batch_isend_irecvp2p_op_list
Send or Receive a batch of tensors asynchronously and return a list of requests.
Process each of the operations in p2p_op_list and return the corresponding
requests. NCCL, Gloo, and UCC backend are currently supported.

Parameters
p2p_op_list (torch.distributed.distributed_c10d.P2POp) – A list of point-to-point operations(type of each operator is
torch.distributed.P2POp). The order of the isend/irecv in the list
matters and it needs to match with corresponding isend/irecv on the
remote end.


A list of distributed request objects returned by calling the corresponding
op in the op_list.

Return type
[torch.distributed.distributed_c10d.Work]



send_tensor       
recv_tensor   
   send_tensor     world_size
  
     recv_tensor     world_size  world_size

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 19)

  batch_isend_irecv 
   
    
recv_tensor
tensor([2, 3])     # Rank 0
tensor([0, 1])     # Rank 1




Note that when this API is used with the NCCL PG backend, users must set
the current GPU device with torch.cuda.set_device, otherwise it will
lead to unexpected hang issues.
In addition, if this API is the first collective call in the 
passed to dist.P2POp, all ranks of the  must participate in
this API call; otherwise, the behavior is undefined. If this API call is
not the first collective call in the , batched P2P operations
involving only a subset of ranks of the  are allowed.

Send or Receive a batch of tensors asynchronously and return a list of requests.

Process each of the operations in p2p_op_list and return the corresponding
requests. NCCL, Gloo, and UCC backend are currently supported.

Parameters
p2p_op_list (torch.distributed.distributed_c10d.P2POp) – A list of point-to-point operations(type of each operator is
torch.distributed.P2POp). The order of the isend/irecv in the list
matters and it needs to match with corresponding isend/irecv on the
remote end.


A list of distributed request objects returned by calling the corresponding
op in the op_list.

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 20)

Return type
[torch.distributed.distributed_c10d.Work]

p2p_op_list (torch.distributed.distributed_c10d.P2POp) – A list of point-to-point operations(type of each operator is
torch.distributed.P2POp). The order of the isend/irecv in the list
matters and it needs to match with corresponding isend/irecv on the
remote end.

A list of distributed request objects returned by calling the corresponding
op in the op_list.

[torch.distributed.distributed_c10d.Work]

Code example:
send_tensor       
recv_tensor   
   send_tensor     world_size
  
     recv_tensor     world_size  world_size

  batch_isend_irecv 
   
    
recv_tensor
tensor([2, 3])     # Rank 0
tensor([0, 1])     # Rank 1

Note that when this API is used with the NCCL PG backend, users must set
the current GPU device with torch.cuda.set_device, otherwise it will
lead to unexpected hang issues.

In addition, if this API is the first collective call in the 
passed to dist.P2POp, all ranks of the  must participate in
this API call; otherwise, the behavior is undefined. If this API call is
not the first collective call in the , batched P2P operations
involving only a subset of ranks of the  are allowed.

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 21)

torch.distributed., , , , , group_peer
A class to build point-to-point operations for batch_isend_irecv.
This class builds the type of P2P operation, communication buffer, peer rank,
Process Group, and tag. Instances of this class will be passed to
batch_isend_irecv for point-to-point communications.

Parameters

 () – A function to send data to or receive data from a peer process.
The type of  is either torch.distributed.isend or
torch.distributed.irecv.
 () – Tensor to send or receive.
 () – Destination or source rank.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with recv.
group_peer () – Destination or source rank.

A class to build point-to-point operations for batch_isend_irecv.

This class builds the type of P2P operation, communication buffer, peer rank,
Process Group, and tag. Instances of this class will be passed to
batch_isend_irecv for point-to-point communications.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Point-to-point communication (Part 22)

 () – A function to send data to or receive data from a peer process.
The type of  is either torch.distributed.isend or
torch.distributed.irecv.
 () – Tensor to send or receive.
 () – Destination or source rank.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with recv.
group_peer () – Destination or source rank.

List:
() – A function to send data to or receive data from a peer process.
The type of  is either torch.distributed.isend or
torch.distributed.irecv.
 () – Tensor to send or receive.
 () – Destination or source rank.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Tag to match send with recv.
group_peer () – Destination or source rank.

() – A function to send data to or receive data from a peer process.
The type of  is either torch.distributed.isend or
torch.distributed.irecv.

() – Tensor to send or receive.

() – Destination or source rank.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Tag to match send with recv.

group_peer () – Destination or source rank.

================================================================================

# Distributed communication package - torch.distributed - Synchronous and asynchronous collective operations (Part 1)

Every collective operation function supports the following two kinds of operations,
depending on the setting of the  flag passed into the collective:

Synchronous operation - the default mode, when  is set to .
When the function returns, it is guaranteed that
the collective operation is performed. In the case of CUDA operations, it is not guaranteed
that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any
further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives,
function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of
synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream
synchronization, see CUDA Semantics.
See the below script to see examples of differences in these semantics for CPU and CUDA operations.

Asynchronous operation - when  is set to True. The collective operation function
returns a distributed request object. In general, you don’t need to create it manually and it
is guaranteed to support two methods:

================================================================================

# Distributed communication package - torch.distributed - Synchronous and asynchronous collective operations (Part 2)

List:
is_completed() - in the case of CPU collectives, returns  if completed. In the case of CUDA operations,
returns  if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the
default stream without further synchronization.
 - in the case of CPU collectives, will block the process until the operation is completed. In the case
of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).
get_future() - returns torch._C.Future object. Supported for NCCL, also supported for most operations on GLOO
and MPI, except for peer to peer operations.
Note: as we continue adopting Futures and merging APIs, get_future() call might become redundant.

is_completed() - in the case of CPU collectives, returns  if completed. In the case of CUDA operations,
returns  if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the
default stream without further synchronization.

================================================================================

# Distributed communication package - torch.distributed - Synchronous and asynchronous collective operations (Part 3)

- in the case of CPU collectives, will block the process until the operation is completed. In the case
of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).

get_future() - returns torch._C.Future object. Supported for NCCL, also supported for most operations on GLOO
and MPI, except for peer to peer operations.
Note: as we continue adopting Futures and merging APIs, get_future() call might become redundant.

The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.
It shows the explicit need to synchronize when using collective outputs on different CUDA streams:

Code example:
# Code runs on each rank.
init_process_group  world_size
  
  
  all_reduce 
# Wait ensures the operation is enqueued, but not necessarily complete.

# Using result on non-default stream.
 
    wait_streamdefault_stream
    
   
    # if the explicit call to wait_stream was omitted, the output below will be
    # non-deterministically 1 or 101, depending on whether the allreduce overwrote
    # the value after the add completed.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 1)

torch.distributed., , , , 
Broadcasts the tensor to the whole group.
 must have the same number of elements in all processes
participating in the collective.

Parameters

 () – Data to be sent if  is the rank of current
process, and tensor to be used to save received data otherwise.
 () – Source rank on global process group (regardless of  argument).
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Source rank on .  Must specify one of 
and  but not both.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Broadcasts the tensor to the whole group.

must have the same number of elements in all processes
participating in the collective.

Parameters

 () – Data to be sent if  is the rank of current
process, and tensor to be used to save received data otherwise.
 () – Source rank on global process group (regardless of  argument).
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Source rank on .  Must specify one of 
and  but not both.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 2)

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
() – Data to be sent if  is the rank of current
process, and tensor to be used to save received data otherwise.
 () – Source rank on global process group (regardless of  argument).
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Source rank on .  Must specify one of 
and  but not both.

() – Data to be sent if  is the rank of current
process, and tensor to be used to save received data otherwise.

() – Source rank on global process group (regardless of  argument).

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

() – Source rank on .  Must specify one of 
and  but not both.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 3)

torch.distributed.broadcast_object_listobject_list, , , , 
Broadcasts picklable objects in object_list to the whole group.
Similar to broadcast(), but Python objects can be passed in.
Note that all objects in object_list must be picklable in order to be
broadcasted.

Parameters

object_list () – List of input objects to broadcast.
Each object must be picklable. Only objects on the  rank will
be broadcast, but each rank must provide lists of equal sizes.
 () – Source rank from which to broadcast object_list.
Source rank is based on global process group (regardless of  argument)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before broadcasting. Default is .
 () – Source rank on .  Must not specify one of 
and  but not both.



. If rank is part of the group, object_list will contain the
broadcasted objects from  rank.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 4)


For NCCL-based process groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().



Note that this API differs slightly from the broadcast()
collective since it does not provide an  handle and thus
will be a blocking call.



Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.



broadcast_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.



Calling broadcast_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using broadcast() instead.


# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    # Assumes world_size of 3.
          # any picklable object

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 5)

        
# Assumes backend is not NCCL
  
broadcast_object_list  

['foo', 12, {1: 2}]

Broadcasts picklable objects in object_list to the whole group.

Similar to broadcast(), but Python objects can be passed in.
Note that all objects in object_list must be picklable in order to be
broadcasted.

Parameters

object_list () – List of input objects to broadcast.
Each object must be picklable. Only objects on the  rank will
be broadcast, but each rank must provide lists of equal sizes.
 () – Source rank from which to broadcast object_list.
Source rank is based on global process group (regardless of  argument)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before broadcasting. Default is .
 () – Source rank on .  Must not specify one of 
and  but not both.



. If rank is part of the group, object_list will contain the
broadcasted objects from  rank.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 6)

List:
object_list () – List of input objects to broadcast.
Each object must be picklable. Only objects on the  rank will
be broadcast, but each rank must provide lists of equal sizes.
 () – Source rank from which to broadcast object_list.
Source rank is based on global process group (regardless of  argument)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 (torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before broadcasting. Default is .
 () – Source rank on .  Must not specify one of 
and  but not both.

object_list () – List of input objects to broadcast.
Each object must be picklable. Only objects on the  rank will
be broadcast, but each rank must provide lists of equal sizes.

() – Source rank from which to broadcast object_list.
Source rank is based on global process group (regardless of  argument)

(ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 7)

(torch.device, optional) – If not None, the objects are
serialized and converted to tensors which are moved to the
 before broadcasting. Default is .

() – Source rank on .  Must not specify one of 
and  but not both.

. If rank is part of the group, object_list will contain the
broadcasted objects from  rank.

For NCCL-based process groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().

Note that this API differs slightly from the broadcast()
collective since it does not provide an  handle and thus
will be a blocking call.

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.

broadcast_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 8)

Calling broadcast_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using broadcast() instead.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    # Assumes world_size of 3.
          # any picklable object

        
# Assumes backend is not NCCL
  
broadcast_object_list  

['foo', 12, {1: 2}]

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    # Assumes world_size of 3.
          # any picklable object

        
# Assumes backend is not NCCL
  
broadcast_object_list  

['foo', 12, {1: 2}]

torch.distributed.all_reduce, op=<RedOpType.SUM: , group=None, async_op=False
Reduces the tensor data across all machines in a way that all get the final result.
After the call  is going to be bitwise identical in all processes.
Complex tensors are supported.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 9)

 () – Input and output of the collective. The function
operates in-place.
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group



# All tensors below are of torch.int64 type.
# We have 2 process groups, 2 ranks.
  
          

tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
all_reduce 

tensor([4, 6], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1


# All tensors below are of torch.cfloat type.
# We have 2 process groups, 2 ranks.
  
           
        

tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0
tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1
all_reduce 

tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0
tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1

Reduces the tensor data across all machines in a way that all get the final result.

After the call  is going to be bitwise identical in all processes.

Complex tensors are supported.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 10)

Parameters

 () – Input and output of the collective. The function
operates in-place.
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
() – Input and output of the collective. The function
operates in-place.
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op

() – Input and output of the collective. The function
operates in-place.

() – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 11)

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Code example:
# All tensors below are of torch.int64 type.
# We have 2 process groups, 2 ranks.
  
          

tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
all_reduce 

tensor([4, 6], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1

Code example:
# All tensors below are of torch.cfloat type.
# We have 2 process groups, 2 ranks.
  
           
        

tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0
tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1
all_reduce 

tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0
tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1

torch.distributed., , op=<RedOpType.SUM: , group=None, async_op=False, group_dst=None
Reduces the tensor data across all machines.
Only the process with rank  is going to receive the final result.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 12)

 () – Input and output of the collective. The function
operates in-place.
 () – Destination rank on global process group (regardless of  argument)
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Destination rank on .  Must specify one of 
and  but not both.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Reduces the tensor data across all machines.

Only the process with rank  is going to receive the final result.

Parameters

 () – Input and output of the collective. The function
operates in-place.
 () – Destination rank on global process group (regardless of  argument)
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Destination rank on .  Must specify one of 
and  but not both.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 13)

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
() – Input and output of the collective. The function
operates in-place.
 () – Destination rank on global process group (regardless of  argument)
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Destination rank on .  Must specify one of 
and  but not both.

() – Input and output of the collective. The function
operates in-place.

() – Destination rank on global process group (regardless of  argument)

() – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

() – Destination rank on .  Must specify one of 
and  but not both.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 14)

torch.distributed.all_gathertensor_list, , , 
Gathers tensors from the whole group in a list.
Complex and uneven sized tensors are supported.

Parameters

tensor_list () – Output list. It should contain
correctly-sized tensors to be used for output of the collective.
Uneven sized tensors are supported.
 () – Tensor to be broadcast from current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group



# All tensors below are of torch.int64 dtype.
# We have 2 process groups, 2 ranks.
  
tensor_list  
          

tensor_list
[tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0
[tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1
          

tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
all_gathertensor_list 
tensor_list
[tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0
[tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 15)


# All tensors below are of torch.cfloat dtype.
# We have 2 process groups, 2 ranks.
tensor_list  
          

tensor_list
[tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0
[tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1
  
           
        

tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0
tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1
all_gathertensor_list 
tensor_list
[tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0
[tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1

Gathers tensors from the whole group in a list.

Complex and uneven sized tensors are supported.

Parameters

tensor_list () – Output list. It should contain
correctly-sized tensors to be used for output of the collective.
Uneven sized tensors are supported.
 () – Tensor to be broadcast from current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 16)

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
tensor_list () – Output list. It should contain
correctly-sized tensors to be used for output of the collective.
Uneven sized tensors are supported.
 () – Tensor to be broadcast from current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op

tensor_list () – Output list. It should contain
correctly-sized tensors to be used for output of the collective.
Uneven sized tensors are supported.

() – Tensor to be broadcast from current process.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Code example:
# All tensors below are of torch.int64 dtype.
# We have 2 process groups, 2 ranks.
  
tensor_list  
          

tensor_list
[tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0
[tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1
          

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 17)

tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
all_gathertensor_list 
tensor_list
[tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0
[tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1

Code example:
# All tensors below are of torch.cfloat dtype.
# We have 2 process groups, 2 ranks.
tensor_list  
          

tensor_list
[tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0
[tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1
  
           
        

tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0
tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1
all_gathertensor_list 
tensor_list
[tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0
[tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1

torch.distributed.all_gather_into_tensoroutput_tensor, input_tensor, , 
Gather tensors from all ranks and put them in a single output tensor.
This function requires all tensors to be the same size on each process.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 18)

output_tensor () – Output tensor to accommodate tensor elements
from all ranks. It must be correctly sized to have one of the
following forms:
(i) a concatenation of all the input tensors along the primary
dimension; for definition of “concatenation”, see torch.cat();
(ii) a stack of all the input tensors along the primary dimension;
for definition of “stack”, see torch.stack().
Examples below may better explain the supported output forms.
input_tensor () – Tensor to be gathered from current rank.
Different from the all_gather API, the input tensors in this
API must have the same size across all ranks.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group



# All tensors below are of torch.int64 dtype and on CUDA devices.
# We have two ranks.
  
          

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 19)

tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
# Output in concatenation form
tensor_out  world_size    
all_gather_into_tensortensor_out 
tensor_out
tensor([1, 2, 3, 4], device='cuda:0') # Rank 0
tensor([1, 2, 3, 4], device='cuda:1') # Rank 1
# Output in stack form
tensor_out2  world_size   
all_gather_into_tensortensor_out2 
tensor_out2
tensor([[1, 2],
        [3, 4]], device='cuda:0') # Rank 0
tensor([[1, 2],
        [3, 4]], device='cuda:1') # Rank 1

Gather tensors from all ranks and put them in a single output tensor.

This function requires all tensors to be the same size on each process.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 20)

output_tensor () – Output tensor to accommodate tensor elements
from all ranks. It must be correctly sized to have one of the
following forms:
(i) a concatenation of all the input tensors along the primary
dimension; for definition of “concatenation”, see torch.cat();
(ii) a stack of all the input tensors along the primary dimension;
for definition of “stack”, see torch.stack().
Examples below may better explain the supported output forms.
input_tensor () – Tensor to be gathered from current rank.
Different from the all_gather API, the input tensors in this
API must have the same size across all ranks.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 21)

List:
output_tensor () – Output tensor to accommodate tensor elements
from all ranks. It must be correctly sized to have one of the
following forms:
(i) a concatenation of all the input tensors along the primary
dimension; for definition of “concatenation”, see torch.cat();
(ii) a stack of all the input tensors along the primary dimension;
for definition of “stack”, see torch.stack().
Examples below may better explain the supported output forms.
input_tensor () – Tensor to be gathered from current rank.
Different from the all_gather API, the input tensors in this
API must have the same size across all ranks.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op

output_tensor () – Output tensor to accommodate tensor elements
from all ranks. It must be correctly sized to have one of the
following forms:
(i) a concatenation of all the input tensors along the primary
dimension; for definition of “concatenation”, see torch.cat();
(ii) a stack of all the input tensors along the primary dimension;
for definition of “stack”, see torch.stack().
Examples below may better explain the supported output forms.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 22)

input_tensor () – Tensor to be gathered from current rank.
Different from the all_gather API, the input tensors in this
API must have the same size across all ranks.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Code example:
# All tensors below are of torch.int64 dtype and on CUDA devices.
# We have two ranks.
  
          

tensor([1, 2], device='cuda:0') # Rank 0
tensor([3, 4], device='cuda:1') # Rank 1
# Output in concatenation form
tensor_out  world_size    
all_gather_into_tensortensor_out 
tensor_out
tensor([1, 2, 3, 4], device='cuda:0') # Rank 0
tensor([1, 2, 3, 4], device='cuda:1') # Rank 1
# Output in stack form
tensor_out2  world_size   
all_gather_into_tensortensor_out2 
tensor_out2
tensor([[1, 2],
        [3, 4]], device='cuda:0') # Rank 0
tensor([[1, 2],
        [3, 4]], device='cuda:1') # Rank 1

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 23)

torch.distributed.all_gather_objectobject_list, , 
Gathers picklable objects from the whole group into a list.
Similar to all_gather(), but Python objects can be passed in.
Note that the object must be picklable in order to be gathered.

Parameters

object_list () – Output list. It should be correctly sized as the
size of the group for this collective and will contain the output.
 () – Pickable Python object to be broadcast from current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used. Default is .



None. If the calling rank is part of this group, the output of the
collective will be populated into the input object_list. If the
calling rank is not part of the group, the passed in object_list will
be unmodified.




Note that this API differs slightly from the all_gather()
collective since it does not provide an  handle and thus
will be a blocking call.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 24)

For NCCL-based processed groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().



Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.



all_gather_object() uses  module implicitly, which is
known to be insecure. It is possible to construct malicious pickle data
which will execute arbitrary code during unpickling. Only call this
function with data you trust.



Calling all_gather_object() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using all_gather() instead.


# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes world_size of 3.
gather_objects      # any picklable object
      gather_objects
all_gather_object gather_objects

['foo', 12, {1: 2}]

Gathers picklable objects from the whole group into a list.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 25)

Similar to all_gather(), but Python objects can be passed in.
Note that the object must be picklable in order to be gathered.

Parameters

object_list () – Output list. It should be correctly sized as the
size of the group for this collective and will contain the output.
 () – Pickable Python object to be broadcast from current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used. Default is .



None. If the calling rank is part of this group, the output of the
collective will be populated into the input object_list. If the
calling rank is not part of the group, the passed in object_list will
be unmodified.

List:
object_list () – Output list. It should be correctly sized as the
size of the group for this collective and will contain the output.
 () – Pickable Python object to be broadcast from current process.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used. Default is .

object_list () – Output list. It should be correctly sized as the
size of the group for this collective and will contain the output.

() – Pickable Python object to be broadcast from current process.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 26)

(ProcessGroup) – The process group to work on. If None,
the default process group will be used. Default is .

None. If the calling rank is part of this group, the output of the
collective will be populated into the input object_list. If the
calling rank is not part of the group, the passed in object_list will
be unmodified.

Note that this API differs slightly from the all_gather()
collective since it does not provide an  handle and thus
will be a blocking call.

For NCCL-based processed groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.

all_gather_object() uses  module implicitly, which is
known to be insecure. It is possible to construct malicious pickle data
which will execute arbitrary code during unpickling. Only call this
function with data you trust.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 27)

Calling all_gather_object() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using all_gather() instead.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes world_size of 3.
gather_objects      # any picklable object
      gather_objects
all_gather_object gather_objects

['foo', 12, {1: 2}]

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes world_size of 3.
gather_objects      # any picklable object
      gather_objects
all_gather_object gather_objects

['foo', 12, {1: 2}]

torch.distributed., gather_list, , , , 
Gathers a list of tensors in a single process.
This function requires all tensors to be the same size on each process.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 28)

 () – Input tensor.
gather_list () – List of appropriately,
same-sized tensors to use for gathered data
(default is None, must be specified on the destination rank)
 () – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Destination rank on .  Invalid to specify both  and 



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group




Note that all Tensors in gather_list must have the same size.


# We have 2 process groups, 2 ranks.
tensor_size  
  
  tensor_size   
   
    gather_list  zeros_like     

    gather_list  
 gather_list 
# Rank 0 gets gathered data.
gather_list
[tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0
None                                                                   # Rank 1

Gathers a list of tensors in a single process.

This function requires all tensors to be the same size on each process.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 29)

 () – Input tensor.
gather_list () – List of appropriately,
same-sized tensors to use for gathered data
(default is None, must be specified on the destination rank)
 () – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Destination rank on .  Invalid to specify both  and 



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
() – Input tensor.
gather_list () – List of appropriately,
same-sized tensors to use for gathered data
(default is None, must be specified on the destination rank)
 () – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Destination rank on .  Invalid to specify both  and

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 30)

gather_list () – List of appropriately,
same-sized tensors to use for gathered data
(default is None, must be specified on the destination rank)

() – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

() – Destination rank on .  Invalid to specify both  and

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Note that all Tensors in gather_list must have the same size.

# We have 2 process groups, 2 ranks.
tensor_size  
  
  tensor_size   
   
    gather_list  zeros_like     

    gather_list  
 gather_list 
# Rank 0 gets gathered data.
gather_list
[tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0
None                                                                   # Rank 1

Code example:
# We have 2 process groups, 2 ranks.
tensor_size  
  
  tensor_size   
   
    gather_list  zeros_like     

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 31)

    gather_list  
 gather_list 
# Rank 0 gets gathered data.
gather_list
[tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0
None                                                                   # Rank 1

torch.distributed.gather_object, object_gather_list, , , 
Gathers picklable objects from the whole group in a single process.
Similar to , but Python objects can be passed in. Note that the
object must be picklable in order to be gathered.

Parameters

 () – Input object. Must be picklable.
object_gather_list () – Output list. On the  rank, it
should be correctly sized as the size of the group for this
collective and will contain the output. Must be  on non-dst
ranks. (default is )
 () – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 () – Destination rank on .  Invalid to specify both  and 



None. On the  rank, object_gather_list will contain the
output of the collective.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 32)


Note that this API differs slightly from the gather collective
since it does not provide an async_op handle and thus will be a blocking
call.



For NCCL-based processed groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().



Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.



gather_object() uses  module implicitly, which is
known to be insecure. It is possible to construct malicious pickle data
which will execute arbitrary code during unpickling. Only call this
function with data you trust.



Calling gather_object() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 33)


# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes world_size of 3.
gather_objects      # any picklable object
      gather_objects
gather_object
    gather_objects
          
    

# On rank 0

['foo', 12, {1: 2}]

Gathers picklable objects from the whole group in a single process.

Similar to , but Python objects can be passed in. Note that the
object must be picklable in order to be gathered.

Parameters

 () – Input object. Must be picklable.
object_gather_list () – Output list. On the  rank, it
should be correctly sized as the size of the group for this
collective and will contain the output. Must be  on non-dst
ranks. (default is )
 () – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 () – Destination rank on .  Invalid to specify both  and 



None. On the  rank, object_gather_list will contain the
output of the collective.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 34)

List:
() – Input object. Must be picklable.
object_gather_list () – Output list. On the  rank, it
should be correctly sized as the size of the group for this
collective and will contain the output. Must be  on non-dst
ranks. (default is )
 () – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 () – Destination rank on .  Invalid to specify both  and

() – Input object. Must be picklable.

object_gather_list () – Output list. On the  rank, it
should be correctly sized as the size of the group for this
collective and will contain the output. Must be  on non-dst
ranks. (default is )

() – Destination rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)

(ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .

() – Destination rank on .  Invalid to specify both  and

None. On the  rank, object_gather_list will contain the
output of the collective.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 35)

Note that this API differs slightly from the gather collective
since it does not provide an async_op handle and thus will be a blocking
call.

For NCCL-based processed groups, internal tensor representations
of objects must be moved to the GPU device before communication takes
place. In this case, the device used is given by
torch.cuda.current_device() and it is the user’s responsibility to
ensure that this is set so that each rank has an individual GPU, via
torch.cuda.set_device().

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.

gather_object() uses  module implicitly, which is
known to be insecure. It is possible to construct malicious pickle data
which will execute arbitrary code during unpickling. Only call this
function with data you trust.

Calling gather_object() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 36)

# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes world_size of 3.
gather_objects      # any picklable object
      gather_objects
gather_object
    gather_objects
          
    

# On rank 0

['foo', 12, {1: 2}]

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
# Assumes world_size of 3.
gather_objects      # any picklable object
      gather_objects
gather_object
    gather_objects
          
    

# On rank 0

['foo', 12, {1: 2}]

torch.distributed., scatter_list, , , , 
Scatters a list of tensors to all processes in a group.
Each process will receive exactly one tensor and store its data in the
 argument.
Complex tensors are supported.

Parameters

 () – Output tensor.
scatter_list () – List of tensors to scatter (default is
None, must be specified on the source rank)
 () – Source rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Source rank on .  Invalid to specify both  and 



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 37)

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group




Note that all Tensors in scatter_list must have the same size.


# Note: Process group initialization omitted on each rank.
 torch.distributed  
tensor_size  
  
output_tensor  tensor_size 
   
    # Assumes world_size of 2.
    # Only tensors, all of which must be the same size.
      tensor_size 
      tensor_size   
    scatter_list   

    scatter_list  
output_tensor scatter_list 
# Rank i gets scatter_list[i].
output_tensor
tensor([1., 1.], device='cuda:0') # Rank 0
tensor([5., 5.], device='cuda:1') # Rank 1

Scatters a list of tensors to all processes in a group.

Each process will receive exactly one tensor and store its data in the
 argument.

Complex tensors are supported.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 38)

 () – Output tensor.
scatter_list () – List of tensors to scatter (default is
None, must be specified on the source rank)
 () – Source rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Source rank on .  Invalid to specify both  and 



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
() – Output tensor.
scatter_list () – List of tensors to scatter (default is
None, must be specified on the source rank)
 () – Source rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
 () – Source rank on .  Invalid to specify both  and

scatter_list () – List of tensors to scatter (default is
None, must be specified on the source rank)

() – Source rank on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 39)

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

() – Source rank on .  Invalid to specify both  and

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

Note that all Tensors in scatter_list must have the same size.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
tensor_size  
  
output_tensor  tensor_size 
   
    # Assumes world_size of 2.
    # Only tensors, all of which must be the same size.
      tensor_size 
      tensor_size   
    scatter_list   

    scatter_list  
output_tensor scatter_list 
# Rank i gets scatter_list[i].
output_tensor
tensor([1., 1.], device='cuda:0') # Rank 0
tensor([5., 5.], device='cuda:1') # Rank 1

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
tensor_size  
  
output_tensor  tensor_size 
   
    # Assumes world_size of 2.
    # Only tensors, all of which must be the same size.
      tensor_size 
      tensor_size   
    scatter_list   

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 40)

    scatter_list  
output_tensor scatter_list 
# Rank i gets scatter_list[i].
output_tensor
tensor([1., 1.], device='cuda:0') # Rank 0
tensor([5., 5.], device='cuda:1') # Rank 1

torch.distributed.scatter_object_listscatter_object_output_list, scatter_object_input_list, , , 
Scatters picklable objects in scatter_object_input_list to the whole group.
Similar to , but Python objects can be passed in. On
each rank, the scattered object will be stored as the first element of
scatter_object_output_list. Note that all objects in
scatter_object_input_list must be picklable in order to be scattered.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 41)

scatter_object_output_list () – Non-empty list whose first
element will store the object scattered to this rank.
scatter_object_input_list () – List of input objects to scatter.
Each object must be picklable. Only objects on the  rank will
be scattered, and the argument can be  for non-src ranks.
 () – Source rank from which to scatter scatter_object_input_list.
Source rank is based on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 () – Source rank on .  Invalid to specify both  and 



. If rank is part of the group, scatter_object_output_list
will have its first element set to the scattered object for this rank.




Note that this API differs slightly from the scatter collective
since it does not provide an  handle and thus will be a
blocking call.



Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 42)

scatter_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.



Calling scatter_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.


# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    # Assumes world_size of 3.
          # any picklable object

    # Can be any list on non-src ranks, elements are not used.
        
output_list  
scatter_object_listoutput_list  
# Rank i gets objects[i]. For example, on rank 2:
output_list

Scatters picklable objects in scatter_object_input_list to the whole group.

Similar to , but Python objects can be passed in. On
each rank, the scattered object will be stored as the first element of
scatter_object_output_list. Note that all objects in
scatter_object_input_list must be picklable in order to be scattered.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 43)

scatter_object_output_list () – Non-empty list whose first
element will store the object scattered to this rank.
scatter_object_input_list () – List of input objects to scatter.
Each object must be picklable. Only objects on the  rank will
be scattered, and the argument can be  for non-src ranks.
 () – Source rank from which to scatter scatter_object_input_list.
Source rank is based on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 () – Source rank on .  Invalid to specify both  and 



. If rank is part of the group, scatter_object_output_list
will have its first element set to the scattered object for this rank.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 44)

List:
scatter_object_output_list () – Non-empty list whose first
element will store the object scattered to this rank.
scatter_object_input_list () – List of input objects to scatter.
Each object must be picklable. Only objects on the  rank will
be scattered, and the argument can be  for non-src ranks.
 () – Source rank from which to scatter scatter_object_input_list.
Source rank is based on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)
 (ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .
 () – Source rank on .  Invalid to specify both  and

scatter_object_output_list () – Non-empty list whose first
element will store the object scattered to this rank.

scatter_object_input_list () – List of input objects to scatter.
Each object must be picklable. Only objects on the  rank will
be scattered, and the argument can be  for non-src ranks.

() – Source rank from which to scatter scatter_object_input_list.
Source rank is based on global process group (regardless of  argument).
(If both  and  are None, default is global rank 0)

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 45)

(ProcessGroup) – (ProcessGroup, optional): The process group to work on. If None,
the default process group will be used. Default is .

() – Source rank on .  Invalid to specify both  and

. If rank is part of the group, scatter_object_output_list
will have its first element set to the scattered object for this rank.

Note that this API differs slightly from the scatter collective
since it does not provide an  handle and thus will be a
blocking call.

Object collectives have a number of serious performance and scalability
limitations.  See Object collectives for details.

scatter_object_list() uses  module implicitly, which
is known to be insecure. It is possible to construct malicious pickle
data which will execute arbitrary code during unpickling. Only call this
function with data you trust.

Calling scatter_object_list() with GPU tensors is not well supported
and inefficient as it incurs GPU -> CPU transfer since tensors would be
pickled. Please consider using  instead.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    # Assumes world_size of 3.
          # any picklable object

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 46)

    # Can be any list on non-src ranks, elements are not used.
        
output_list  
scatter_object_listoutput_list  
# Rank i gets objects[i]. For example, on rank 2:
output_list

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    # Assumes world_size of 3.
          # any picklable object

    # Can be any list on non-src ranks, elements are not used.
        
output_list  
scatter_object_listoutput_list  
# Rank i gets objects[i]. For example, on rank 2:
output_list

torch.distributed.reduce_scatter, input_list, op=<RedOpType.SUM: , group=None, async_op=False
Reduces, then scatters a list of tensors to all processes in a group.

Parameters

 () – Output tensor.
input_list () – List of tensors to reduce and scatter.
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

Reduces, then scatters a list of tensors to all processes in a group.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 47)

Parameters

 () – Output tensor.
input_list () – List of tensors to reduce and scatter.
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

List:
() – Output tensor.
input_list () – List of tensors to reduce and scatter.
 () – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.

input_list () – List of tensors to reduce and scatter.

() – One of the values from
torch.distributed.ReduceOp
enum.  Specifies an operation used for element-wise reductions.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 48)

torch.distributed.reduce_scatter_tensor, , op=<RedOpType.SUM: , group=None, async_op=False
Reduces, then scatters a tensor to all ranks in a group.

Parameters

 () – Output tensor. It should have the same size across all
ranks.
 () – Input tensor to be reduced and scattered. Its size
should be output tensor size times the world size. The input tensor
can have one of the following shapes:
(i) a concatenation of the output tensors along the primary
dimension, or
(ii) a stack of the output tensors along the primary dimension.
For definition of “concatenation”, see torch.cat().
For definition of “stack”, see torch.stack().
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.



# All tensors below are of torch.int64 dtype and on CUDA devices.
# We have two ranks.
  
tensor_out    
# Input in concatenation form
  world_size    

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 49)

tensor([0, 1, 2, 3], device='cuda:0') # Rank 0
tensor([0, 1, 2, 3], device='cuda:1') # Rank 1
reduce_scatter_tensortensor_out 
tensor_out
tensor([0, 2], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1
# Input in stack form
   world_size 

tensor([[0, 1],
        [2, 3]], device='cuda:0') # Rank 0
tensor([[0, 1],
        [2, 3]], device='cuda:1') # Rank 1
reduce_scatter_tensortensor_out 
tensor_out
tensor([0, 2], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1

Reduces, then scatters a tensor to all ranks in a group.

Parameters

 () – Output tensor. It should have the same size across all
ranks.
 () – Input tensor to be reduced and scattered. Its size
should be output tensor size times the world size. The input tensor
can have one of the following shapes:
(i) a concatenation of the output tensors along the primary
dimension, or
(ii) a stack of the output tensors along the primary dimension.
For definition of “concatenation”, see torch.cat().
For definition of “stack”, see torch.stack().
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 50)

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

List:
() – Output tensor. It should have the same size across all
ranks.
 () – Input tensor to be reduced and scattered. Its size
should be output tensor size times the world size. The input tensor
can have one of the following shapes:
(i) a concatenation of the output tensors along the primary
dimension, or
(ii) a stack of the output tensors along the primary dimension.
For definition of “concatenation”, see torch.cat().
For definition of “stack”, see torch.stack().
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.

() – Output tensor. It should have the same size across all
ranks.

() – Input tensor to be reduced and scattered. Its size
should be output tensor size times the world size. The input tensor
can have one of the following shapes:
(i) a concatenation of the output tensors along the primary
dimension, or
(ii) a stack of the output tensors along the primary dimension.
For definition of “concatenation”, see torch.cat().
For definition of “stack”, see torch.stack().

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 51)

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

Code example:
# All tensors below are of torch.int64 dtype and on CUDA devices.
# We have two ranks.
  
tensor_out    
# Input in concatenation form
  world_size    

tensor([0, 1, 2, 3], device='cuda:0') # Rank 0
tensor([0, 1, 2, 3], device='cuda:1') # Rank 1
reduce_scatter_tensortensor_out 
tensor_out
tensor([0, 2], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1
# Input in stack form
   world_size 

tensor([[0, 1],
        [2, 3]], device='cuda:0') # Rank 0
tensor([[0, 1],
        [2, 3]], device='cuda:1') # Rank 1
reduce_scatter_tensortensor_out 
tensor_out
tensor([0, 2], device='cuda:0') # Rank 0
tensor([4, 6], device='cuda:1') # Rank 1

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 52)

torch.distributed.all_to_all_single, , output_split_sizes, input_split_sizes, , 
Split input tensor and then scatter the split list to all processes in a group.
Later the received tensors are concatenated from all the processes in the group
and returned as a single output tensor.
Complex tensors are supported.

Parameters

 () – Gathered concatenated output tensor.
 () – Input tensor to scatter.
output_split_sizes – (list[Int], optional): Output split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.
input_split_sizes – (list[Int], optional): Input split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.




all_to_all_single is experimental and subject to change.


      

tensor([0, 1, 2, 3])     # Rank 0
tensor([4, 5, 6, 7])     # Rank 1
tensor([8, 9, 10, 11])   # Rank 2
tensor([12, 13, 14, 15]) # Rank 3
   
all_to_all_single 

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 53)

tensor([0, 4, 8, 12])    # Rank 0
tensor([1, 5, 9, 13])    # Rank 1
tensor([2, 6, 10, 14])   # Rank 2
tensor([3, 7, 11, 15])   # Rank 3


# Essentially, it is similar to following operation:
scatter_list  world_size
gather_list  world_size
   world_size
    gather_list scatter_list         


# Another example with uneven split

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 54)

tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
tensor([20, 21, 22, 23, 24])                                     # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
input_splits
[2, 2, 1, 1]                                                     # Rank 0
[3, 2, 2, 2]                                                     # Rank 1
[2, 1, 1, 1]                                                     # Rank 2
[2, 2, 2, 1]                                                     # Rank 3
output_splits
[2, 3, 2, 2]                                                     # Rank 0
[2, 2, 1, 2]                                                     # Rank 1
[1, 2, 1, 2]                                                     # Rank 2
[1, 2, 1, 1]                                                     # Rank 3
  
all_to_all_single  output_splits input_splits

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 55)

tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0
tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1
tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2
tensor([ 5, 17, 18, 24, 36])                                     # Rank 3


# Another example with tensors of torch.cfloat type.
  
                
        

tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0
tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1
tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2
tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3
   
all_to_all_single 

tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0
tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1
tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2
tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3

Split input tensor and then scatter the split list to all processes in a group.

Later the received tensors are concatenated from all the processes in the group
and returned as a single output tensor.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 56)

Complex tensors are supported.

Parameters

 () – Gathered concatenated output tensor.
 () – Input tensor to scatter.
output_split_sizes – (list[Int], optional): Output split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.
input_split_sizes – (list[Int], optional): Input split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

List:
() – Gathered concatenated output tensor.
 () – Input tensor to scatter.
output_split_sizes – (list[Int], optional): Output split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.
input_split_sizes – (list[Int], optional): Input split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 57)

() – Gathered concatenated output tensor.

() – Input tensor to scatter.

output_split_sizes – (list[Int], optional): Output split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.

input_split_sizes – (list[Int], optional): Input split sizes for dim 0
if specified None or empty, dim 0 of  tensor must divide
equally by world_size.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

all_to_all_single is experimental and subject to change.

Code example:
tensor([0, 1, 2, 3])     # Rank 0
tensor([4, 5, 6, 7])     # Rank 1
tensor([8, 9, 10, 11])   # Rank 2
tensor([12, 13, 14, 15]) # Rank 3
   
all_to_all_single 

tensor([0, 4, 8, 12])    # Rank 0
tensor([1, 5, 9, 13])    # Rank 1
tensor([2, 6, 10, 14])   # Rank 2
tensor([3, 7, 11, 15])   # Rank 3

Code example:
# Essentially, it is similar to following operation:
scatter_list  world_size
gather_list  world_size
   world_size
    gather_list scatter_list

Code example:
# Another example with uneven split

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 58)

tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
tensor([20, 21, 22, 23, 24])                                     # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
input_splits
[2, 2, 1, 1]                                                     # Rank 0
[3, 2, 2, 2]                                                     # Rank 1
[2, 1, 1, 1]                                                     # Rank 2
[2, 2, 2, 1]                                                     # Rank 3
output_splits
[2, 3, 2, 2]                                                     # Rank 0
[2, 2, 1, 2]                                                     # Rank 1
[1, 2, 1, 2]                                                     # Rank 2
[1, 2, 1, 1]                                                     # Rank 3
  
all_to_all_single  output_splits input_splits

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 59)

tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0
tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1
tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2
tensor([ 5, 17, 18, 24, 36])                                     # Rank 3

Code example:
# Another example with tensors of torch.cfloat type.
  
                
        

tensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0
tensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1
tensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2
tensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3
   
all_to_all_single 

tensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0
tensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1
tensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2
tensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 60)

torch.distributed.all_to_alloutput_tensor_list, input_tensor_list, , 
Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.
Complex tensors are supported.

Parameters

output_tensor_list () – List of tensors to be gathered one
per rank.
input_tensor_list () – List of tensors to scatter one per rank.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.




all_to_all is experimental and subject to change.


      
  

[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0
[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1
[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2
[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
   
all_to_all 

[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0
[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1
[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2
[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 61)


# Essentially, it is similar to following operation:
scatter_list  
gather_list  
   world_size
    gather_list scatter_list       



tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
tensor([20, 21, 22, 23, 24])                                     # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
input_splits
[2, 2, 1, 1]                                                     # Rank 0
[3, 2, 2, 2]                                                     # Rank 1
[2, 1, 1, 1]                                                     # Rank 2
[2, 2, 2, 1]                                                     # Rank 3
output_splits
[2, 3, 2, 2]                                                     # Rank 0
[2, 2, 1, 2]                                                     # Rank 1
[1, 2, 1, 2]                                                     # Rank 2
[1, 2, 1, 1]                                                     # Rank 3
  input_splits

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 62)

[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0
[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1
[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2
[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3
  
all_to_all 

[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0
[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1
[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2
[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3


# Another example with tensors of torch.cfloat type.
  
                
        
  

[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0
[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1
[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2
[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3
   
all_to_all 

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 63)

[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0
[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1
[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2
[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3

Scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.

Complex tensors are supported.

Parameters

output_tensor_list () – List of tensors to be gathered one
per rank.
input_tensor_list () – List of tensors to scatter one per rank.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

List:
output_tensor_list () – List of tensors to be gathered one
per rank.
input_tensor_list () – List of tensors to scatter one per rank.
 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 64)

output_tensor_list () – List of tensors to be gathered one
per rank.

input_tensor_list () – List of tensors to scatter one per rank.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group.

all_to_all is experimental and subject to change.

Code example:
[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0
[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1
[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2
[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
   
all_to_all 

[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0
[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1
[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2
[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3

Code example:
# Essentially, it is similar to following operation:
scatter_list  
gather_list  
   world_size
    gather_list scatter_list

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 65)

Code example:
tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
tensor([20, 21, 22, 23, 24])                                     # Rank 2
tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
input_splits
[2, 2, 1, 1]                                                     # Rank 0
[3, 2, 2, 2]                                                     # Rank 1
[2, 1, 1, 1]                                                     # Rank 2
[2, 2, 2, 1]                                                     # Rank 3
output_splits
[2, 3, 2, 2]                                                     # Rank 0
[2, 2, 1, 2]                                                     # Rank 1
[1, 2, 1, 2]                                                     # Rank 2
[1, 2, 1, 1]                                                     # Rank 3
  input_splits

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 66)

[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0
[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1
[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2
[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3
  
all_to_all 

[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0
[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1
[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2
[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3

Code example:
# Another example with tensors of torch.cfloat type.
  
                
        
  

[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0
[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1
[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2
[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3
   
all_to_all 

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 67)

[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0
[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1
[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2
[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3

torch.distributed., , device_ids
Synchronize all processes.
This collective blocks processes until the whole group enters this function,
if async_op is False, or if async work handle is called on wait().

Parameters

 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
device_ids () – List of device/GPU ids. Only one id is expected.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group




ProcessGroupNCCL now blocks the cpu thread till the completion of the barrier collective.

Synchronize all processes.

This collective blocks processes until the whole group enters this function,
if async_op is False, or if async work handle is called on wait().

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 68)

 (ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
device_ids () – List of device/GPU ids. Only one id is expected.



Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

List:
(ProcessGroup) – The process group to work on. If None,
the default process group will be used.
 () – Whether this op should be an async op
device_ids () – List of device/GPU ids. Only one id is expected.

(ProcessGroup) – The process group to work on. If None,
the default process group will be used.

() – Whether this op should be an async op

device_ids () – List of device/GPU ids. Only one id is expected.

Async work handle, if async_op is set to True.
None, if not async_op or if not part of the group

ProcessGroupNCCL now blocks the cpu thread till the completion of the barrier collective.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 69)

torch.distributed.monitored_barrier, , wait_all_ranks
Synchronize processes similar to torch.distributed.barrier, but consider a configurable timeout.
It is able to report ranks that did not pass this barrier within the provided timeout.
Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0.
Rank 0 will block until all send /recv from other ranks are processed, and will report
failures for ranks that failed to respond in time. Note that if one rank does not reach the
monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.
This collective will block all processes/ranks in the group, until the
whole group exits the function successfully, making it useful for debugging
and synchronizing. However, it can have a performance impact and should only
be used for debugging or scenarios that require full synchronization points
on the host-side. For debugging purposes, this barrier can be inserted
before the application’s collective calls to check if any ranks are
desynchronized.


Note that this collective is only supported with the GLOO backend.


Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 70)

 (ProcessGroup) – The process group to work on. If
, the default process group will be used.
 (datetime.timedelta) – Timeout for monitored_barrier.
If , the default process group timeout will be used.
wait_all_ranks () – Whether to collect all failed ranks or
not. By default, this is  and monitored_barrier on rank 0
will throw on the first failed rank it encounters in order to fail
fast. By setting wait_all_ranks=True monitored_barrier will
collect all failed ranks and throw an error containing information
about all failed ranks.



.



# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    monitored_barrier # Raises exception indicating that
# rank 1 did not call into monitored_barrier.
# Example with wait_all_ranks=True
   
    monitored_barrierwait_all_ranks # Raises exception
# indicating that ranks 1, 2, ... world_size - 1 did not call into
# monitored_barrier.

Synchronize processes similar to torch.distributed.barrier, but consider a configurable timeout.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 71)

It is able to report ranks that did not pass this barrier within the provided timeout.
Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0.
Rank 0 will block until all send /recv from other ranks are processed, and will report
failures for ranks that failed to respond in time. Note that if one rank does not reach the
monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.

This collective will block all processes/ranks in the group, until the
whole group exits the function successfully, making it useful for debugging
and synchronizing. However, it can have a performance impact and should only
be used for debugging or scenarios that require full synchronization points
on the host-side. For debugging purposes, this barrier can be inserted
before the application’s collective calls to check if any ranks are
desynchronized.

Note that this collective is only supported with the GLOO backend.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 72)

 (ProcessGroup) – The process group to work on. If
, the default process group will be used.
 (datetime.timedelta) – Timeout for monitored_barrier.
If , the default process group timeout will be used.
wait_all_ranks () – Whether to collect all failed ranks or
not. By default, this is  and monitored_barrier on rank 0
will throw on the first failed rank it encounters in order to fail
fast. By setting wait_all_ranks=True monitored_barrier will
collect all failed ranks and throw an error containing information
about all failed ranks.



.

List:
(ProcessGroup) – The process group to work on. If
, the default process group will be used.
 (datetime.timedelta) – Timeout for monitored_barrier.
If , the default process group timeout will be used.
wait_all_ranks () – Whether to collect all failed ranks or
not. By default, this is  and monitored_barrier on rank 0
will throw on the first failed rank it encounters in order to fail
fast. By setting wait_all_ranks=True monitored_barrier will
collect all failed ranks and throw an error containing information
about all failed ranks.

(ProcessGroup) – The process group to work on. If
, the default process group will be used.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 73)

(datetime.timedelta) – Timeout for monitored_barrier.
If , the default process group timeout will be used.

wait_all_ranks () – Whether to collect all failed ranks or
not. By default, this is  and monitored_barrier on rank 0
will throw on the first failed rank it encounters in order to fail
fast. By setting wait_all_ranks=True monitored_barrier will
collect all failed ranks and throw an error containing information
about all failed ranks.

# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    monitored_barrier # Raises exception indicating that
# rank 1 did not call into monitored_barrier.
# Example with wait_all_ranks=True
   
    monitored_barrierwait_all_ranks # Raises exception
# indicating that ranks 1, 2, ... world_size - 1 did not call into
# monitored_barrier.

Code example:
# Note: Process group initialization omitted on each rank.
 torch.distributed  
   
    monitored_barrier # Raises exception indicating that
# rank 1 did not call into monitored_barrier.
# Example with wait_all_ranks=True
   
    monitored_barrierwait_all_ranks # Raises exception
# indicating that ranks 1, 2, ... world_size - 1 did not call into
# monitored_barrier.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 74)

torch.distributed.
A  object represents the handle to a pending asynchronous operation in
PyTorch’s distributed package. It is returned by non-blocking collective operations,
such as dist.all_reduce(tensor, async_op=True).


torch._C._distributed_c10d.Work  



torch._C._distributed_c10d.Work  std::__exception_ptr::exception_ptr



get_futuretorch._C._distributed_c10d.Work  torch.Future


A torch.futures.Future object which is associated with the completion of
the . As an example, a future object can be retrieved
by   process_group.allreduce(tensors).get_future().



Below is an example of a simple allreduce DDP communication hook that uses
get_future API to retrieve a Future associated with the completion of
.
 process_group ProcessGroup  GradBucket  
    group_to_use  process_group  process_group     distributed
      group_to_use
     distributedall_reduce group_to_use get_future
register_comm_hook 





================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 75)


get_future API supports NCCL, and partially GLOO and MPI backends
(no support for peer-to-peer operations like send/recv) and will return a torch.futures.Future.
In the example above,  work will be done on GPU using NCCL backend,
fut.wait() will return after synchronizing the appropriate NCCL streams
with PyTorch’s current device streams to ensure we can have asynchronous CUDA
execution and it does not wait for the entire operation to complete on GPU. Note that
CUDAFuture  does not support TORCH_NCCL_BLOCKING_WAIT flag or NCCL’s .
In addition, if a callback function was added by fut.then(), it will wait until
’s NCCL streams synchronize with ProcessGroupNCCL’s dedicated callback
stream and invoke the callback inline after running the callback on the callback stream.
fut.then() will return another CUDAFuture that holds the return value of the
callback and a  that recorded the callback stream.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 76)


For CPU work, fut.done() returns true when work has been completed and value()
tensors are ready.
For GPU work, fut.done() returns true only whether the operation has been enqueued.
For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), fut.done() returns
true when tensors have arrived on respective nodes, but not yet necessarily synched on
respective GPUs (similarly to GPU work).






get_future_resulttorch._C._distributed_c10d.Work  torch.Future


A torch.futures.Future object of int type which maps to the enum type of WorkResult
As an example, a future object can be retrieved
by   process_group.allreduce(tensor).get_future_result().



users can use fut.wait() to blocking wait for the completion of the work and
get the WorkResult by fut.value().
Also, users can use fut.then(call_back_func) to register a callback function to be called
when the work is completed, without blocking the current thread.




get_future_result API supports NCCL




is_completedtorch._C._distributed_c10d.Work  



is_successtorch._C._distributed_c10d.Work  



torch._C._distributed_c10d.Work  torch.Tensor



source_ranktorch._C._distributed_c10d.Work  



synchronizetorch._C._distributed_c10d.Work  



  torch._C._distributed_c10d.Work



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 77)

torch._C._distributed_c10d.Work, datetime.timedeltadatetime.timedelta(0)  


true/false.




work.wait(timeout)

# some handling






In normal cases, users do not need to set the timeout.
calling wait() is the same as calling synchronize():
Letting the current stream block on the completion of the NCCL work.
However, if timeout is set, it will block the CPU thread until the NCCL work is completed
or timed out. If timeout, exception will be thrown.

A  object represents the handle to a pending asynchronous operation in
PyTorch’s distributed package. It is returned by non-blocking collective operations,
such as dist.all_reduce(tensor, async_op=True).

torch._C._distributed_c10d.Work

torch._C._distributed_c10d.Work  std::__exception_ptr::exception_ptr

get_futuretorch._C._distributed_c10d.Work  torch.Future


A torch.futures.Future object which is associated with the completion of
the . As an example, a future object can be retrieved
by   process_group.allreduce(tensors).get_future().



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 78)

Below is an example of a simple allreduce DDP communication hook that uses
get_future API to retrieve a Future associated with the completion of
.
 process_group ProcessGroup  GradBucket  
    group_to_use  process_group  process_group     distributed
      group_to_use
     distributedall_reduce group_to_use get_future
register_comm_hook 





================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 79)


get_future API supports NCCL, and partially GLOO and MPI backends
(no support for peer-to-peer operations like send/recv) and will return a torch.futures.Future.
In the example above,  work will be done on GPU using NCCL backend,
fut.wait() will return after synchronizing the appropriate NCCL streams
with PyTorch’s current device streams to ensure we can have asynchronous CUDA
execution and it does not wait for the entire operation to complete on GPU. Note that
CUDAFuture  does not support TORCH_NCCL_BLOCKING_WAIT flag or NCCL’s .
In addition, if a callback function was added by fut.then(), it will wait until
’s NCCL streams synchronize with ProcessGroupNCCL’s dedicated callback
stream and invoke the callback inline after running the callback on the callback stream.
fut.then() will return another CUDAFuture that holds the return value of the
callback and a  that recorded the callback stream.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 80)


For CPU work, fut.done() returns true when work has been completed and value()
tensors are ready.
For GPU work, fut.done() returns true only whether the operation has been enqueued.
For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), fut.done() returns
true when tensors have arrived on respective nodes, but not yet necessarily synched on
respective GPUs (similarly to GPU work).

A torch.futures.Future object which is associated with the completion of
the . As an example, a future object can be retrieved
by   process_group.allreduce(tensors).get_future().

A torch.futures.Future object which is associated with the completion of
the . As an example, a future object can be retrieved
by   process_group.allreduce(tensors).get_future().

Below is an example of a simple allreduce DDP communication hook that uses
get_future API to retrieve a Future associated with the completion of
.
 process_group ProcessGroup  GradBucket  
    group_to_use  process_group  process_group     distributed
      group_to_use
     distributedall_reduce group_to_use get_future
register_comm_hook

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 81)

Below is an example of a simple allreduce DDP communication hook that uses
get_future API to retrieve a Future associated with the completion of
.

Code example:
process_group ProcessGroup  GradBucket  
    group_to_use  process_group  process_group     distributed
      group_to_use
     distributedall_reduce group_to_use get_future
register_comm_hook

get_future API supports NCCL, and partially GLOO and MPI backends
(no support for peer-to-peer operations like send/recv) and will return a torch.futures.Future.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 82)

In the example above,  work will be done on GPU using NCCL backend,
fut.wait() will return after synchronizing the appropriate NCCL streams
with PyTorch’s current device streams to ensure we can have asynchronous CUDA
execution and it does not wait for the entire operation to complete on GPU. Note that
CUDAFuture  does not support TORCH_NCCL_BLOCKING_WAIT flag or NCCL’s .
In addition, if a callback function was added by fut.then(), it will wait until
’s NCCL streams synchronize with ProcessGroupNCCL’s dedicated callback
stream and invoke the callback inline after running the callback on the callback stream.
fut.then() will return another CUDAFuture that holds the return value of the
callback and a  that recorded the callback stream.

Quote:
For CPU work, fut.done() returns true when work has been completed and value()
tensors are ready.
For GPU work, fut.done() returns true only whether the operation has been enqueued.
For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), fut.done() returns
true when tensors have arrived on respective nodes, but not yet necessarily synched on
respective GPUs (similarly to GPU work).

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 83)

List:
For CPU work, fut.done() returns true when work has been completed and value()
tensors are ready.
For GPU work, fut.done() returns true only whether the operation has been enqueued.
For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), fut.done() returns
true when tensors have arrived on respective nodes, but not yet necessarily synched on
respective GPUs (similarly to GPU work).

For CPU work, fut.done() returns true when work has been completed and value()
tensors are ready.

For GPU work, fut.done() returns true only whether the operation has been enqueued.

For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), fut.done() returns
true when tensors have arrived on respective nodes, but not yet necessarily synched on
respective GPUs (similarly to GPU work).

get_future_resulttorch._C._distributed_c10d.Work  torch.Future


A torch.futures.Future object of int type which maps to the enum type of WorkResult
As an example, a future object can be retrieved
by   process_group.allreduce(tensor).get_future_result().



================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 84)

users can use fut.wait() to blocking wait for the completion of the work and
get the WorkResult by fut.value().
Also, users can use fut.then(call_back_func) to register a callback function to be called
when the work is completed, without blocking the current thread.




get_future_result API supports NCCL

A torch.futures.Future object of int type which maps to the enum type of WorkResult
As an example, a future object can be retrieved
by   process_group.allreduce(tensor).get_future_result().

A torch.futures.Future object of int type which maps to the enum type of WorkResult
As an example, a future object can be retrieved
by   process_group.allreduce(tensor).get_future_result().

users can use fut.wait() to blocking wait for the completion of the work and
get the WorkResult by fut.value().
Also, users can use fut.then(call_back_func) to register a callback function to be called
when the work is completed, without blocking the current thread.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 85)

users can use fut.wait() to blocking wait for the completion of the work and
get the WorkResult by fut.value().
Also, users can use fut.then(call_back_func) to register a callback function to be called
when the work is completed, without blocking the current thread.

get_future_result API supports NCCL

is_completedtorch._C._distributed_c10d.Work

is_successtorch._C._distributed_c10d.Work

torch._C._distributed_c10d.Work  torch.Tensor

source_ranktorch._C._distributed_c10d.Work

synchronizetorch._C._distributed_c10d.Work

torch._C._distributed_c10d.Work

torch._C._distributed_c10d.Work, datetime.timedeltadatetime.timedelta(0)  


true/false.




work.wait(timeout)

# some handling






In normal cases, users do not need to set the timeout.
calling wait() is the same as calling synchronize():
Letting the current stream block on the completion of the NCCL work.
However, if timeout is set, it will block the CPU thread until the NCCL work is completed
or timed out. If timeout, exception will be thrown.

work.wait(timeout)

# some handling

work.wait(timeout)

# some handling

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 86)

In normal cases, users do not need to set the timeout.
calling wait() is the same as calling synchronize():
Letting the current stream block on the completion of the NCCL work.
However, if timeout is set, it will block the CPU thread until the NCCL work is completed
or timed out. If timeout, exception will be thrown.

torch.distributed.
An enum-like class for available reduction operations: , ,
, , , , , and PREMUL_SUM.
, , and  reductions are not available when
using the  backend.
 divides values by the world size before summing across ranks.
 is only available with the  backend,
and only for NCCL versions 2.10 or later.
PREMUL_SUM multiplies inputs by a given scalar locally before reduction.
PREMUL_SUM is only available with the  backend,
and only available for NCCL versions 2.11 or later. Users are supposed to
use torch.distributed._make_nccl_premul_sum.
Additionally, ,  and  are not supported for complex tensors.
The values of this class can be accessed as attributes, e.g., ReduceOp.SUM.
They are used in specifying strategies for reduction collectives, e.g.,
.
This class does not support __members__ property.

================================================================================

# Distributed communication package - torch.distributed - Collective functions (Part 87)

An enum-like class for available reduction operations: , ,
, , , , , and PREMUL_SUM.

, , and  reductions are not available when
using the  backend.

divides values by the world size before summing across ranks.
 is only available with the  backend,
and only for NCCL versions 2.10 or later.

PREMUL_SUM multiplies inputs by a given scalar locally before reduction.
PREMUL_SUM is only available with the  backend,
and only available for NCCL versions 2.11 or later. Users are supposed to
use torch.distributed._make_nccl_premul_sum.

Additionally, ,  and  are not supported for complex tensors.

The values of this class can be accessed as attributes, e.g., ReduceOp.SUM.
They are used in specifying strategies for reduction collectives, e.g.,
.

This class does not support __members__ property.

torch.distributed.
Deprecated enum-like class for reduction operations: , ,
, and .
 is recommended to use instead.

Deprecated enum-like class for reduction operations: , ,
, and .

is recommended to use instead.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 1)

The distributed package comes with a distributed key-value store, which can be
used to share information between processes in the group as well as to
initialize the distributed package in
torch.distributed.init_process_group() (by explicitly creating the store
as an alternative to specifying init_method.) There are 3 choices for
Key-Value Stores: ,
, and .

torch.distributed.
Base class for all store implementations, such as the 3 provided by PyTorch
distributed: (, ,
and ).


torch._C._distributed_c10d.Store  



torch._C._distributed_c10d.Store, ,   
The first call to add for a given  creates a counter associated
with  in the store, initialized to . Subsequent calls to add
with the same  increment the counter by the specified .
Calling  with a key that has already
been set in the store by  will result
in an exception.

Parameters

 () – The key in the store whose counter will be incremented.
 () – The quantity by which the counter will be incremented.




 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return 7
"first_key"







================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 2)

torch._C._distributed_c10d.Store, ,   
Append the key-value pair into the store based on the supplied  and
. If  does not exists in the store, it will be created.

Parameters

 () – The key to be appended to the store.
 () – The value associated with  to be added to the store.




 torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return "potato"
"first_key"







torch._C._distributed_c10d.Store,   
The call to check whether a given list of  have value stored in
the store. This call immediately returns in normal cases but still suffers
from some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.
Calling  with a list of keys that
one wants to check whether stored in the store or not.

Parameters
 () – The keys to query whether stored in the store.



 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
# Should return 7
"first_key"







================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 3)

torch._C._distributed_c10d.Store  torch._C._distributed_c10d.Store
Clones the store and returns a new object that points to the same underlying
store. The returned store can be used concurrently with the original object.
This is intended to provide a safe way to use a store from multiple threads by
cloning one store per thread.



compare_settorch._C._distributed_c10d.Store, , ,   
Inserts the key-value pair into the store based on the supplied  and
performs comparison between expected_value and desired_value before inserting. desired_value
will only be set if expected_value for the  already exists in the store or if expected_value
is an empty string.

Parameters

 () – The key to be checked in the store.
expected_value () – The value associated with  to be checked before insertion.
desired_value () – The value associated with  to be added to the store.




 torch.distributed  
   
  "127.0.0.1"    
 "first_value"
compare_set "first_value" "second_value"
# Should return "second_value"








delete_keytorch._C._distributed_c10d.Store,   
Deletes the key-value pair associated with  from the store. Returns
 if the key was successfully deleted, and  if it was not.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 4)


The delete_key API is only supported by the  and . Using this API
with the  will result in an exception.


Parameters
 () – The key to be deleted from the store


 if  was deleted, otherwise .



 torch.distributed  
   
# Using TCPStore as an example, HashStore can also be used
  "127.0.0.1"    
"first_key"
# This should return true
delete_key"first_key"
# This should return false
delete_key







torch._C._distributed_c10d.Store,   
Retrieves the value associated with the given  in the store. If  is not
present in the store, the function will wait for , which is defined
when initializing the store, before throwing an exception.

Parameters
 () – The function will return the value associated with this key.


Value associated with  if  is in the store.



 torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"







has_extended_apitorch._C._distributed_c10d.Store  
Returns true if the store supports extended operations.



torch._C._distributed_c10d.Store,   
Retrieve all values in . If any key in  is not
present in the store, the function will wait for 

Parameters
 () – The keys to be retrieved from the store.



================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 5)

 torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"second_key" 
# Should return [b"po", b"tato"]
"first_key" "second_key"







torch._C._distributed_c10d.Store, ,   
Inserts a list key-value pair into the store based on the supplied  and 

Parameters

 () – The keys to insert.
 () – The values to insert.




 torch.distributed  
   
  "127.0.0.1"    
"first_key" "second_key"  
# Should return b"po"
"first_key"







torch._C._distributed_c10d.Store  
Returns the number of keys set in the store. Note that this number will typically
be one greater than the number of keys added by 
and  since one key is used to coordinate all
the workers using the store.


When used with the ,  returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.



The number of keys present in the store.



 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" "first_value"
# This should return 2








torch._C._distributed_c10d.Store,   
Returns the length of the specified queue.
If the queue doesn’t exist it returns 0.
See queue_push for more details.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 6)

Parameters
 () – The key of the queue to get the length.





torch._C._distributed_c10d.Store, ,   
Pops a value from the specified queue or waits until timeout if the queue is empty.
See queue_push for more details.
If block is False, a dist.QueueEmptyError will be raised if the queue is empty.

Parameters

 () – The key of the queue to pop from.
 () – Whether to block waiting for the key or immediately return.






queue_pushtorch._C._distributed_c10d.Store, ,   
Pushes a value into the specified queue.
Using the same key for queues and set/get operations may result in unexpected
behavior.
wait/check operations are supported for queues.
wait with queues will only wake one waiting worker rather than all.

Parameters

 () – The key of the queue to push to.
 () – The value to push into the queue.






torch._C._distributed_c10d.Store, ,   
Inserts the key-value pair into the store based on the supplied  and
. If  already exists in the store, it will overwrite the old
value with the new supplied .

Parameters

 () – The key to be added to the store.
 () – The value associated with  to be added to the store.



================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 7)


 torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"







set_timeouttorch._C._distributed_c10d.Store, datetime.timedelta  
Sets the store’s default timeout. This timeout is used during initialization and in
 and .

Parameters
 () – timeout to be set in the store.



 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
set_timeout
# This will throw an exception after 10 seconds









Gets the timeout of the store.



, 
Overloaded function.

wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None

Waits for each key in  to be added to the store. If not all keys are
set before the  (set during store initialization), then 
will throw an exception.

Parameters
 () – List of keys on which to wait until they are set in the store.



 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 30 seconds






wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 8)

Waits for each key in  to be added to the store, and throws an exception
if the keys have not been set by the supplied .

Parameters

 () – List of keys on which to wait until they are set in the store.
 () – Time to wait for the keys to be added before throwing an exception.




 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 10 seconds

Base class for all store implementations, such as the 3 provided by PyTorch
distributed: (, ,
and ).

torch._C._distributed_c10d.Store

torch._C._distributed_c10d.Store, ,   
The first call to add for a given  creates a counter associated
with  in the store, initialized to . Subsequent calls to add
with the same  increment the counter by the specified .
Calling  with a key that has already
been set in the store by  will result
in an exception.

Parameters

 () – The key in the store whose counter will be incremented.
 () – The quantity by which the counter will be incremented.




 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return 7
"first_key"

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 9)

The first call to add for a given  creates a counter associated
with  in the store, initialized to . Subsequent calls to add
with the same  increment the counter by the specified .
Calling  with a key that has already
been set in the store by  will result
in an exception.

Parameters

 () – The key in the store whose counter will be incremented.
 () – The quantity by which the counter will be incremented.

List:
() – The key in the store whose counter will be incremented.
 () – The quantity by which the counter will be incremented.

() – The key in the store whose counter will be incremented.

() – The quantity by which the counter will be incremented.

torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return 7
"first_key"

Code example:
torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return 7
"first_key"

torch._C._distributed_c10d.Store, ,   
Append the key-value pair into the store based on the supplied  and
. If  does not exists in the store, it will be created.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 10)

 () – The key to be appended to the store.
 () – The value associated with  to be added to the store.




 torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return "potato"
"first_key"

Append the key-value pair into the store based on the supplied  and
. If  does not exists in the store, it will be created.

Parameters

 () – The key to be appended to the store.
 () – The value associated with  to be added to the store.

List:
() – The key to be appended to the store.
 () – The value associated with  to be added to the store.

() – The key to be appended to the store.

() – The value associated with  to be added to the store.

torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return "potato"
"first_key"

Code example:
torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"first_key" 
# Should return "potato"
"first_key"

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 11)

torch._C._distributed_c10d.Store,   
The call to check whether a given list of  have value stored in
the store. This call immediately returns in normal cases but still suffers
from some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.
Calling  with a list of keys that
one wants to check whether stored in the store or not.

Parameters
 () – The keys to query whether stored in the store.



 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
# Should return 7
"first_key"

The call to check whether a given list of  have value stored in
the store. This call immediately returns in normal cases but still suffers
from some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.
Calling  with a list of keys that
one wants to check whether stored in the store or not.

Parameters
 () – The keys to query whether stored in the store.

() – The keys to query whether stored in the store.

torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
# Should return 7
"first_key"

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 12)

Code example:
torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" 
# Should return 7
"first_key"

torch._C._distributed_c10d.Store  torch._C._distributed_c10d.Store
Clones the store and returns a new object that points to the same underlying
store. The returned store can be used concurrently with the original object.
This is intended to provide a safe way to use a store from multiple threads by
cloning one store per thread.

Clones the store and returns a new object that points to the same underlying
store. The returned store can be used concurrently with the original object.
This is intended to provide a safe way to use a store from multiple threads by
cloning one store per thread.

compare_settorch._C._distributed_c10d.Store, , ,   
Inserts the key-value pair into the store based on the supplied  and
performs comparison between expected_value and desired_value before inserting. desired_value
will only be set if expected_value for the  already exists in the store or if expected_value
is an empty string.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 13)

 () – The key to be checked in the store.
expected_value () – The value associated with  to be checked before insertion.
desired_value () – The value associated with  to be added to the store.




 torch.distributed  
   
  "127.0.0.1"    
 "first_value"
compare_set "first_value" "second_value"
# Should return "second_value"

Inserts the key-value pair into the store based on the supplied  and
performs comparison between expected_value and desired_value before inserting. desired_value
will only be set if expected_value for the  already exists in the store or if expected_value
is an empty string.

Parameters

 () – The key to be checked in the store.
expected_value () – The value associated with  to be checked before insertion.
desired_value () – The value associated with  to be added to the store.

List:
() – The key to be checked in the store.
expected_value () – The value associated with  to be checked before insertion.
desired_value () – The value associated with  to be added to the store.

() – The key to be checked in the store.

expected_value () – The value associated with  to be checked before insertion.

desired_value () – The value associated with  to be added to the store.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 14)

torch.distributed  
   
  "127.0.0.1"    
 "first_value"
compare_set "first_value" "second_value"
# Should return "second_value"

Code example:
torch.distributed  
   
  "127.0.0.1"    
 "first_value"
compare_set "first_value" "second_value"
# Should return "second_value"

delete_keytorch._C._distributed_c10d.Store,   
Deletes the key-value pair associated with  from the store. Returns
 if the key was successfully deleted, and  if it was not.


The delete_key API is only supported by the  and . Using this API
with the  will result in an exception.


Parameters
 () – The key to be deleted from the store


 if  was deleted, otherwise .



 torch.distributed  
   
# Using TCPStore as an example, HashStore can also be used
  "127.0.0.1"    
"first_key"
# This should return true
delete_key"first_key"
# This should return false
delete_key

Deletes the key-value pair associated with  from the store. Returns
 if the key was successfully deleted, and  if it was not.

The delete_key API is only supported by the  and . Using this API
with the  will result in an exception.

Parameters
 () – The key to be deleted from the store


 if  was deleted, otherwise .

() – The key to be deleted from the store

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 15)

if  was deleted, otherwise .

torch.distributed  
   
# Using TCPStore as an example, HashStore can also be used
  "127.0.0.1"    
"first_key"
# This should return true
delete_key"first_key"
# This should return false
delete_key

Code example:
torch.distributed  
   
# Using TCPStore as an example, HashStore can also be used
  "127.0.0.1"    
"first_key"
# This should return true
delete_key"first_key"
# This should return false
delete_key

torch._C._distributed_c10d.Store,   
Retrieves the value associated with the given  in the store. If  is not
present in the store, the function will wait for , which is defined
when initializing the store, before throwing an exception.

Parameters
 () – The function will return the value associated with this key.


Value associated with  if  is in the store.



 torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"

Retrieves the value associated with the given  in the store. If  is not
present in the store, the function will wait for , which is defined
when initializing the store, before throwing an exception.

Parameters
 () – The function will return the value associated with this key.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 16)


Value associated with  if  is in the store.

() – The function will return the value associated with this key.

Value associated with  if  is in the store.

torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"

Code example:
torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"

has_extended_apitorch._C._distributed_c10d.Store  
Returns true if the store supports extended operations.

Returns true if the store supports extended operations.

torch._C._distributed_c10d.Store,   
Retrieve all values in . If any key in  is not
present in the store, the function will wait for 

Parameters
 () – The keys to be retrieved from the store.



 torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"second_key" 
# Should return [b"po", b"tato"]
"first_key" "second_key"

Retrieve all values in . If any key in  is not
present in the store, the function will wait for

Parameters
 () – The keys to be retrieved from the store.

() – The keys to be retrieved from the store.

torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"second_key" 
# Should return [b"po", b"tato"]
"first_key" "second_key"

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 17)

Code example:
torch.distributed  
   
  "127.0.0.1"    
"first_key" 
"second_key" 
# Should return [b"po", b"tato"]
"first_key" "second_key"

torch._C._distributed_c10d.Store, ,   
Inserts a list key-value pair into the store based on the supplied  and 

Parameters

 () – The keys to insert.
 () – The values to insert.




 torch.distributed  
   
  "127.0.0.1"    
"first_key" "second_key"  
# Should return b"po"
"first_key"

Inserts a list key-value pair into the store based on the supplied  and

Parameters

 () – The keys to insert.
 () – The values to insert.

List:
() – The keys to insert.
 () – The values to insert.

() – The keys to insert.

() – The values to insert.

torch.distributed  
   
  "127.0.0.1"    
"first_key" "second_key"  
# Should return b"po"
"first_key"

Code example:
torch.distributed  
   
  "127.0.0.1"    
"first_key" "second_key"  
# Should return b"po"
"first_key"

torch._C._distributed_c10d.Store  
Returns the number of keys set in the store. Note that this number will typically
be one greater than the number of keys added by 
and  since one key is used to coordinate all
the workers using the store.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 18)


When used with the ,  returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.



The number of keys present in the store.



 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" "first_value"
# This should return 2

Returns the number of keys set in the store. Note that this number will typically
be one greater than the number of keys added by 
and  since one key is used to coordinate all
the workers using the store.

When used with the ,  returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.

The number of keys present in the store.

The number of keys present in the store.

torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" "first_value"
# This should return 2

Code example:
torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
"first_key" "first_value"
# This should return 2

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 19)

torch._C._distributed_c10d.Store,   
Returns the length of the specified queue.
If the queue doesn’t exist it returns 0.
See queue_push for more details.

Parameters
 () – The key of the queue to get the length.

Returns the length of the specified queue.

If the queue doesn’t exist it returns 0.

See queue_push for more details.

Parameters
 () – The key of the queue to get the length.

() – The key of the queue to get the length.

torch._C._distributed_c10d.Store, ,   
Pops a value from the specified queue or waits until timeout if the queue is empty.
See queue_push for more details.
If block is False, a dist.QueueEmptyError will be raised if the queue is empty.

Parameters

 () – The key of the queue to pop from.
 () – Whether to block waiting for the key or immediately return.

Pops a value from the specified queue or waits until timeout if the queue is empty.

See queue_push for more details.

If block is False, a dist.QueueEmptyError will be raised if the queue is empty.

Parameters

 () – The key of the queue to pop from.
 () – Whether to block waiting for the key or immediately return.

List:
() – The key of the queue to pop from.
 () – Whether to block waiting for the key or immediately return.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 20)

() – The key of the queue to pop from.

() – Whether to block waiting for the key or immediately return.

queue_pushtorch._C._distributed_c10d.Store, ,   
Pushes a value into the specified queue.
Using the same key for queues and set/get operations may result in unexpected
behavior.
wait/check operations are supported for queues.
wait with queues will only wake one waiting worker rather than all.

Parameters

 () – The key of the queue to push to.
 () – The value to push into the queue.

Pushes a value into the specified queue.

Using the same key for queues and set/get operations may result in unexpected
behavior.

wait/check operations are supported for queues.

wait with queues will only wake one waiting worker rather than all.

Parameters

 () – The key of the queue to push to.
 () – The value to push into the queue.

List:
() – The key of the queue to push to.
 () – The value to push into the queue.

() – The key of the queue to push to.

() – The value to push into the queue.

torch._C._distributed_c10d.Store, ,   
Inserts the key-value pair into the store based on the supplied  and
. If  already exists in the store, it will overwrite the old
value with the new supplied .

Parameters

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 21)

 () – The key to be added to the store.
 () – The value associated with  to be added to the store.




 torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"

Inserts the key-value pair into the store based on the supplied  and
. If  already exists in the store, it will overwrite the old
value with the new supplied .

Parameters

 () – The key to be added to the store.
 () – The value associated with  to be added to the store.

List:
() – The key to be added to the store.
 () – The value associated with  to be added to the store.

() – The key to be added to the store.

() – The value associated with  to be added to the store.

torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"

Code example:
torch.distributed  
   
  "127.0.0.1"    
"first_key" "first_value"
# Should return "first_value"
"first_key"

set_timeouttorch._C._distributed_c10d.Store, datetime.timedelta  
Sets the store’s default timeout. This timeout is used during initialization and in
 and .

Parameters
 () – timeout to be set in the store.



================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 22)

 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
set_timeout
# This will throw an exception after 10 seconds

Sets the store’s default timeout. This timeout is used during initialization and in
 and .

Parameters
 () – timeout to be set in the store.

() – timeout to be set in the store.

torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
set_timeout
# This will throw an exception after 10 seconds

Code example:
torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
set_timeout
# This will throw an exception after 10 seconds

Gets the timeout of the store.

Gets the timeout of the store.

, 
Overloaded function.

wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None

Waits for each key in  to be added to the store. If not all keys are
set before the  (set during store initialization), then 
will throw an exception.

Parameters
 () – List of keys on which to wait until they are set in the store.



================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 23)

 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 30 seconds






wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None

Waits for each key in  to be added to the store, and throws an exception
if the keys have not been set by the supplied .

Parameters

 () – List of keys on which to wait until they are set in the store.
 () – Time to wait for the keys to be added before throwing an exception.




 torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 10 seconds

List:
wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None

wait(self: torch._C._distributed_c10d.Store, arg0: list[str]) -> None

Waits for each key in  to be added to the store. If not all keys are
set before the  (set during store initialization), then 
will throw an exception.

Parameters
 () – List of keys on which to wait until they are set in the store.

() – List of keys on which to wait until they are set in the store.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 24)

torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 30 seconds

Code example:
torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 30 seconds

List:
wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None

wait(self: torch._C._distributed_c10d.Store, arg0: list[str], arg1: datetime.timedelta) -> None

Waits for each key in  to be added to the store, and throws an exception
if the keys have not been set by the supplied .

Parameters

 () – List of keys on which to wait until they are set in the store.
 () – Time to wait for the keys to be added before throwing an exception.

List:
() – List of keys on which to wait until they are set in the store.
 () – Time to wait for the keys to be added before throwing an exception.

() – List of keys on which to wait until they are set in the store.

() – Time to wait for the keys to be added before throwing an exception.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 25)

torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 10 seconds

Code example:
torch.distributed  
   
# Using TCPStore as an example, other store types can also be used
  "127.0.0.1"    
# This will throw an exception after 10 seconds

torch.distributed.
A TCP-based distributed key-value store implementation. The server store holds
the data, while the client stores can connect to the server store over TCP and
perform actions such as  to insert a key-value
pair,  to retrieve a key-value pair, etc. There
should always be one server store initialized because the client store(s) will wait for
the server to establish a connection.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 26)

 () – The hostname or IP Address the server store should run on.
 () – The port on which the server store should listen for incoming requests.
world_size () – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).
 () – True when initializing the server store and False for client stores. Default is False.
 () – Timeout used by the store during initialization and for methods such as  and . Default is timedelta(seconds=300)
wait_for_workers () – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.
multi_tenant () – If True, all  instances in the current process with the same host/port will use the same underlying . Default is False.
master_listen_fd () – If specified, the underlying  will listen on this file descriptor, which must be a socket already bound to . To bind an ephemeral port we recommend setting the port to 0 and reading . Default is None (meaning the server creates a new socket and attempts to bind it to ).
 () – If True, use libuv for  backend. Default is True.



================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 27)


 torch.distributed  
   
# Run on process 1 (server)
server_store  "127.0.0.1"    
# Run on process 2 (client)
client_store  "127.0.0.1"   
# Use any of the store methods from either the client or server after initialization
server_store"first_key" "first_value"
client_store"first_key"






torch._C._distributed_c10d.TCPStore, , , world_size, , datetime.timedeltadatetime.timedelta(seconds=300), wait_for_workers, multi_tenant, master_listen_fd,   
Creates a new TCPStore.




Gets the hostname on which the store listens for requests.



libuvBackend
Returns True if it’s using the libuv backend.




Gets the port number on which the store listens for requests.

A TCP-based distributed key-value store implementation. The server store holds
the data, while the client stores can connect to the server store over TCP and
perform actions such as  to insert a key-value
pair,  to retrieve a key-value pair, etc. There
should always be one server store initialized because the client store(s) will wait for
the server to establish a connection.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 28)

 () – The hostname or IP Address the server store should run on.
 () – The port on which the server store should listen for incoming requests.
world_size () – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).
 () – True when initializing the server store and False for client stores. Default is False.
 () – Timeout used by the store during initialization and for methods such as  and . Default is timedelta(seconds=300)
wait_for_workers () – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.
multi_tenant () – If True, all  instances in the current process with the same host/port will use the same underlying . Default is False.
master_listen_fd () – If specified, the underlying  will listen on this file descriptor, which must be a socket already bound to . To bind an ephemeral port we recommend setting the port to 0 and reading . Default is None (meaning the server creates a new socket and attempts to bind it to ).
 () – If True, use libuv for  backend. Default is True.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 29)

List:
() – The hostname or IP Address the server store should run on.
 () – The port on which the server store should listen for incoming requests.
world_size () – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).
 () – True when initializing the server store and False for client stores. Default is False.
 () – Timeout used by the store during initialization and for methods such as  and . Default is timedelta(seconds=300)
wait_for_workers () – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.
multi_tenant () – If True, all  instances in the current process with the same host/port will use the same underlying . Default is False.
master_listen_fd () – If specified, the underlying  will listen on this file descriptor, which must be a socket already bound to . To bind an ephemeral port we recommend setting the port to 0 and reading . Default is None (meaning the server creates a new socket and attempts to bind it to ).
 () – If True, use libuv for  backend. Default is True.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 30)

() – The hostname or IP Address the server store should run on.

() – The port on which the server store should listen for incoming requests.

world_size () – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).

() – True when initializing the server store and False for client stores. Default is False.

() – Timeout used by the store during initialization and for methods such as  and . Default is timedelta(seconds=300)

wait_for_workers () – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.

multi_tenant () – If True, all  instances in the current process with the same host/port will use the same underlying . Default is False.

master_listen_fd () – If specified, the underlying  will listen on this file descriptor, which must be a socket already bound to . To bind an ephemeral port we recommend setting the port to 0 and reading . Default is None (meaning the server creates a new socket and attempts to bind it to ).

() – If True, use libuv for  backend. Default is True.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 31)

torch.distributed  
   
# Run on process 1 (server)
server_store  "127.0.0.1"    
# Run on process 2 (client)
client_store  "127.0.0.1"   
# Use any of the store methods from either the client or server after initialization
server_store"first_key" "first_value"
client_store"first_key"

Code example:
torch.distributed  
   
# Run on process 1 (server)
server_store  "127.0.0.1"    
# Run on process 2 (client)
client_store  "127.0.0.1"   
# Use any of the store methods from either the client or server after initialization
server_store"first_key" "first_value"
client_store"first_key"

torch._C._distributed_c10d.TCPStore, , , world_size, , datetime.timedeltadatetime.timedelta(seconds=300), wait_for_workers, multi_tenant, master_listen_fd,   
Creates a new TCPStore.

Creates a new TCPStore.

Gets the hostname on which the store listens for requests.

Gets the hostname on which the store listens for requests.

libuvBackend
Returns True if it’s using the libuv backend.

Returns True if it’s using the libuv backend.

Gets the port number on which the store listens for requests.

Gets the port number on which the store listens for requests.

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 32)

torch.distributed.
A thread-safe store implementation based on an underlying hashmap. This store can be used
within the same process (for example, by other threads), but cannot be used across processes.

 torch.distributed  
  
# store can be used from other threads
# Use any of the store methods after initialization
"first_key" "first_value"






torch._C._distributed_c10d.HashStore  
Creates a new HashStore.

A thread-safe store implementation based on an underlying hashmap. This store can be used
within the same process (for example, by other threads), but cannot be used across processes.

torch.distributed  
  
# store can be used from other threads
# Use any of the store methods after initialization
"first_key" "first_value"

Code example:
torch.distributed  
  
# store can be used from other threads
# Use any of the store methods after initialization
"first_key" "first_value"

torch._C._distributed_c10d.HashStore  
Creates a new HashStore.

Creates a new HashStore.

torch.distributed.
A store implementation that uses a file to store the underlying key-value pairs.

Parameters

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 33)

 () – path of the file in which to store the key-value pairs
world_size () – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).




 torch.distributed  
  "/tmp/filestore" 
  "/tmp/filestore" 
# Use any of the store methods from either the client or server after initialization
"first_key" "first_value"
"first_key"






torch._C._distributed_c10d.FileStore, , world_size  
Creates a new FileStore.




Gets the path of the file used by FileStore to store key-value pairs.

A store implementation that uses a file to store the underlying key-value pairs.

Parameters

 () – path of the file in which to store the key-value pairs
world_size () – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).

List:
() – path of the file in which to store the key-value pairs
world_size () – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).

() – path of the file in which to store the key-value pairs

================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 34)

world_size () – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).

torch.distributed  
  "/tmp/filestore" 
  "/tmp/filestore" 
# Use any of the store methods from either the client or server after initialization
"first_key" "first_value"
"first_key"

Code example:
torch.distributed  
  "/tmp/filestore" 
  "/tmp/filestore" 
# Use any of the store methods from either the client or server after initialization
"first_key" "first_value"
"first_key"

torch._C._distributed_c10d.FileStore, , world_size  
Creates a new FileStore.

Creates a new FileStore.

Gets the path of the file used by FileStore to store key-value pairs.

Gets the path of the file used by FileStore to store key-value pairs.

torch.distributed.PrefixStore
A wrapper around any of the 3 key-value stores (,
, and )
that adds a prefix to each key inserted to the store.

Parameters

 () – The prefix string that is prepended to each key before being inserted into the store.
 (torch.distributed.store) – A store object that forms the underlying key-value store.





torch._C._distributed_c10d.PrefixStore, , torch._C._distributed_c10d.Store  
Creates a new PrefixStore.



================================================================================

# Distributed communication package - torch.distributed - Distributed Key-Value Store (Part 35)

underlying_store
Gets the underlying store object that PrefixStore wraps around.

A wrapper around any of the 3 key-value stores (,
, and )
that adds a prefix to each key inserted to the store.

Parameters

 () – The prefix string that is prepended to each key before being inserted into the store.
 (torch.distributed.store) – A store object that forms the underlying key-value store.

List:
() – The prefix string that is prepended to each key before being inserted into the store.
 (torch.distributed.store) – A store object that forms the underlying key-value store.

() – The prefix string that is prepended to each key before being inserted into the store.

(torch.distributed.store) – A store object that forms the underlying key-value store.

torch._C._distributed_c10d.PrefixStore, , torch._C._distributed_c10d.Store  
Creates a new PrefixStore.

Creates a new PrefixStore.

underlying_store
Gets the underlying store object that PrefixStore wraps around.

Gets the underlying store object that PrefixStore wraps around.

================================================================================

# Distributed communication package - torch.distributed - Profiling Collective Communication

Note that you can use torch.profiler (recommended, only available after 1.8.1) or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (,
, ) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:

Code example:
torch.distributed  
 
       
    all_reduce

Please refer to the profiler documentation for a full overview of profiler features.

================================================================================

# Distributed communication package - torch.distributed - Multi-GPU collective functions

The multi-GPU functions (which stand for multiple GPUs per CPU thread) are
deprecated. As of today, PyTorch Distributed’s preferred programming model
is one device per thread, as exemplified by the APIs in this document. If
you are a backend developer and want to support multiple devices per thread,
please contact PyTorch Distributed’s maintainers.

================================================================================

# Distributed communication package - torch.distributed - Object collectives (Part 1)

Object collectives have a number of serious limitations. Read further to determine
if they are safe to use for your use case.

Object collectives are a set of collective-like operations that work on arbitrary
Python objects, as long as they can be pickled. There are various collective patterns
implemented (e.g. broadcast, all_gather, …) but they each roughly follow this pattern:

List:
convert the input object into a pickle (raw bytes), then shove it into a byte tensor
communicate the size of this byte tensor to peers (first collective operation)
allocate appropriately sized tensor to perform the real collective
communicate the object data (second collective operation)
convert raw data back into Python (unpickle)

convert the input object into a pickle (raw bytes), then shove it into a byte tensor

communicate the size of this byte tensor to peers (first collective operation)

allocate appropriately sized tensor to perform the real collective

communicate the object data (second collective operation)

convert raw data back into Python (unpickle)

================================================================================

# Distributed communication package - torch.distributed - Object collectives (Part 2)

Object collectives sometimes have surprising performance or memory characteristics that lead to
long runtimes or OOMs, and thus they should be used with caution. Here are some common issues.

Asymmetric pickle/unpickle time - Pickling objects can be slow, depending on the number, type and size of the objects.
When the collective has a fan-in (e.g. gather_object), the receiving rank(s) must unpickle N times more objects than
the sending rank(s) had to pickle, which can cause other ranks to time out on their next collective.

Inefficient tensor communication - Tensors should be sent via regular collective APIs, not object collective APIs.
It is possible to send Tensors via object collective APIs, but they will be serialized and deserialized (including a
CPU-sync and device-to-host copy in the case of non-CPU tensors), and in almost every case other than debugging or
troubleshooting code, it would be worth the trouble to refactor the code to use non-object collectives instead.

================================================================================

# Distributed communication package - torch.distributed - Object collectives (Part 3)

Unexpected tensor devices - If you still want to send tensors via object collectives, there is another aspect
specific to cuda (and possibly other accelerators) tensors. If you pickle a tensor that is currently on , and
then unpickle it, you will get another tensor on  regardless of which process you are on, or which CUDA device
is the ‘default’ device for that process. With regular tensor collective APIs, ‘output tensors’ will always be on the
same, local device, which is generally what you’d expect.

Unpickling a tensor will implicitly activate a CUDA context if it is the first
time a GPU is used by the process, which can waste significant amounts of GPU memory. This issue can be avoided by
moving tensors to CPU before passing them as inputs to an object collective.

================================================================================

# Distributed communication package - torch.distributed - Third-party backends

Besides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports
third-party backends through a run-time register mechanism.
For references on how to develop a third-party backend through C++ Extension,
please refer to Tutorials - Custom C++ and CUDA Extensions and
test/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party
backends are decided by their own implementations.

The new backend derives from c10d::ProcessGroup and registers the backend
name and the instantiating interface through torch.distributed.Backend.register_backend()
when imported.

When manually importing this backend and invoking torch.distributed.init_process_group()
with the corresponding backend name, the torch.distributed package runs on
the new backend.

The support of third-party backend is experimental and subject to change.

================================================================================

# Distributed communication package - torch.distributed - Launch utility (Part 1)

The torch.distributed package also provides a launch utility in
torch.distributed.launch. This helper utility can be used to launch
multiple processes per node for distributed training.

Module torch.distributed.launch.

torch.distributed.launch is a module that spawns up multiple distributed
training processes on each of the training nodes.

This module is going to be deprecated in favor of .

The utility can be used for single-node distributed training, in which one or
more processes per node will be spawned. The utility can be used for either
CPU training or GPU training. If the utility is used for GPU training,
each distributed process will be operating on a single GPU. This can achieve
well-improved single-node training performance. It can also be used in
multi-node distributed training, by spawning up multiple processes on each node
for well-improved multi-node distributed training performance as well.
This will especially be beneficial for systems with multiple Infiniband
interfaces that have direct-GPU support, since all of them can be utilized for
aggregated communication bandwidth.

================================================================================

# Distributed communication package - torch.distributed - Launch utility (Part 2)

In both cases of single-node distributed training or multi-node distributed
training, this utility will launch the given number of processes per node
(--nproc-per-node). If used for GPU training, this number needs to be less
or equal to the number of GPUs on the current system (nproc_per_node),
and each process will be operating on a single GPU from GPU 0 to
GPU (nproc_per_node - 1).

How to use this module:

List:
Single-Node multi-process distributed training

Single-Node multi-process distributed training

Code example:
distributed NUM_GPUS_YOU_HAVE
           YOUR_TRAINING_SCRIPT

List:
Multi-Node multi-process distributed training: (e.g. two nodes)

Multi-Node multi-process distributed training: (e.g. two nodes)

Node 1: (IP: 192.168.1.1, and has a free port: 1234)

Code example:
distributed NUM_GPUS_YOU_HAVE
             "192.168.1.1"
            YOUR_TRAINING_SCRIPT

Code example:
distributed NUM_GPUS_YOU_HAVE
             "192.168.1.1"
            YOUR_TRAINING_SCRIPT

List:
To look up what optional arguments this module offers:

To look up what optional arguments this module offers:

================================================================================

# Distributed communication package - torch.distributed - Launch utility (Part 3)

1. This utility and multi-process distributed (single-node or
multi-node) GPU training currently only achieves the best performance using
the NCCL distributed backend. Thus NCCL backend is the recommended backend to
use for GPU training.

2. In your training program, you must parse the command-line argument:
--local-rank=LOCAL_PROCESS_RANK, which will be provided by this module.
If your training program uses GPUs, you should ensure that your code only
runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:

Parsing the local_rank argument

Code example:
ArgumentParser
add_argument"--local-rank" "--local_rank" 
  parse_args

Set your device to local rank using either

Code example:
set_devicelocal_rank  # before your code runs

Code example:
local_rank
   # your code to run

Changed in version 2.0.0: The launcher will passes the --local-rank=<rank> argument to your script.
From PyTorch 2.0.0 onwards, the dashed --local-rank is preferred over the
previously used underscored --local_rank.

================================================================================

# Distributed communication package - torch.distributed - Launch utility (Part 4)

For backward compatibility, it may be necessary for users to handle both
cases in their argument parsing code. This means including both "--local-rank"
and "--local_rank" in the argument parser. If only "--local_rank" is
provided, the launcher will trigger an error: “error: unrecognized arguments:
–local-rank=<rank>”. For training code that only supports PyTorch 2.0.0+,
including "--local-rank" should be sufficient.

3. In your training program, you are supposed to call the following function
at the beginning to start the distributed backend. It is strongly recommended
that init_method=env://. Other init methods (e.g. ) may work,
but  is the one that is officially supported by this module.

Code example:
distributedinit_process_group'YOUR BACKEND'
                                     init_method

4. In your training program, you can either use regular distributed functions
or use torch.nn.parallel.DistributedDataParallel() module. If your
training program uses GPUs for training and you would like to use
torch.nn.parallel.DistributedDataParallel() module,
here is how to configure it.

================================================================================

# Distributed communication package - torch.distributed - Launch utility (Part 5)

Code example:
DistributedDataParallel
                                                  device_idslocal_rank
                                                  output_devicelocal_rank

Please ensure that device_ids argument is set to be the only GPU device id
that your code will be operating on. This is generally the local rank of the
process. In other words, the device_ids needs to be [args.local_rank],
and output_device needs to be args.local_rank in order to use this
utility

5. Another way to pass local_rank to the subprocesses via environment variable
LOCAL_RANK. This behavior is enabled when you launch the script with
--use-env=True. You must adjust the subprocess example above to replace
args.local_rank with os.environ['LOCAL_RANK']; the launcher
will not pass --local-rank when you specify this flag.

local_rank is NOT globally unique: it is only unique per process
on a machine.  Thus, don’t use it to decide if you should, e.g.,
write to a networked filesystem.  See
pytorch/pytorch#12042 for an example of
how things can go wrong if you don’t do this correctly.

================================================================================

# Distributed communication package - torch.distributed - Spawn utility

The Multiprocessing package - torch.multiprocessing package also provides a 
function in torch.multiprocessing.spawn(). This helper function
can be used to spawn multiple processes. It works by passing in the
function that you want to run and spawns N processes to run it. This
can be used for multiprocess distributed training as well.

For references on how to use it, please refer to PyTorch example - ImageNet
implementation

Note that this function requires Python 3.4 or higher.

================================================================================

# Distributed communication package - torch.distributed - Debugging torch.distributed applications

Debugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. torch.distributed provides
a suite of tools to help debug training applications in a self-serve fashion:

================================================================================

# Distributed communication package - torch.distributed - Python Breakpoint

It is extremely convenient to use python’s debugger in a distributed environment, but because it does not work out of the box many people do not use it at all.
PyTorch offers a customized wrapper around pdb that streamlines the process.

torch.distributed.breakpoint makes this process easy. Internally, it customizes ’s breakpoint behavior in two ways but otherwise behaves as normal .

List:
Attaches the debugger only on one rank (specified by the user).
Ensures all other ranks stop, by using a torch.distributed.barrier() that will release once the debugged rank issues a 
Reroutes stdin from the child process such that it connects to your terminal.

Attaches the debugger only on one rank (specified by the user).

Ensures all other ranks stop, by using a torch.distributed.barrier() that will release once the debugged rank issues a

Reroutes stdin from the child process such that it connects to your terminal.

To use it, simply issue torch.distributed.breakpoint(rank) on all ranks, using the same value for  in each case.

================================================================================

# Distributed communication package - torch.distributed - Monitored Barrier (Part 1)

As of v1.10, torch.distributed.monitored_barrier() exists as an alternative to torch.distributed.barrier() which fails with helpful information about which rank may be faulty
when crashing, i.e. not all ranks calling into torch.distributed.monitored_barrier() within the provided timeout. torch.distributed.monitored_barrier() implements a host-side
barrier using / communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge
the barrier in time. As an example, consider the following function where rank 1 fails to call into torch.distributed.monitored_barrier() (in practice this could be due
to an application bug or hang in a previous collective):

Code example:
torch.distributed  
 torch.multiprocessing  


 
    init_process_group  world_size
    # monitored barrier requires gloo process group to perform host-side sync.
    group_gloo  
        
        monitored_barriergroup_gloo 


   "__main__"
    "MASTER_ADDR"  "localhost"
    "MASTER_PORT"

The following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:

================================================================================

# Distributed communication package - torch.distributed - Monitored Barrier (Part 2)

Code example:
RuntimeError      monitoredBarrier   
  
 Connection

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 1)

With TORCH_CPP_LOG_LEVEL=INFO, the environment variable TORCH_DISTRIBUTED_DEBUG can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks
are synchronized appropriately. TORCH_DISTRIBUTED_DEBUG can be set to either  (default), , or  depending on the debugging level
required. Please note that the most verbose option,  may impact the application performance and thus should only be used when debugging issues.

Setting TORCH_DISTRIBUTED_DEBUG=INFO will result in additional debug logging when models trained with torch.nn.parallel.DistributedDataParallel() are initialized, and
TORCH_DISTRIBUTED_DEBUG=DETAIL will additionally log runtime performance statistics a select number of iterations. These runtime statistics
include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:

Code example:
torch.distributed  
 torch.multiprocessing  


 TwoLinLayerNet
     
        
            
            

      
          
          
          


 
    init_process_group  world_size
    set_device
    "init model"
      TwoLinLayerNet
    "init ddp"
      DistributedDataParallel device_ids

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 2)

       
    

       
          
            
        


   "__main__"
    "MASTER_ADDR"  "localhost"
    "MASTER_PORT"  
    "TORCH_CPP_LOG_LEVEL"
    
        "TORCH_DISTRIBUTED_DEBUG"
        # set to DETAIL for runtime logging.

The following logs are rendered at initialization time:

Code example:
Initialized 
broadcast_buffers 
bucket_cap_bytes 
find_unused_parameters 
gradient_as_bucket_view 
is_multi_device_module 
 
num_parameter_tensors 
output_device 
 
total_parameter_size_bytes 
world_size 
backend_name 
bucket_sizes 
cuda_visible_devices 
device_ids 
 
master_addr 
master_port 
module_name TwoLinLayerNet
nccl_async_error_handling 
nccl_blocking_wait 
nccl_debug 
nccl_ib_timeout 
nccl_nthreads 
nccl_socket_ifname 
torch_distributed_debug

The following logs are rendered during runtime (when TORCH_DISTRIBUTED_DEBUG=DETAIL is set):

Code example:
TwoLinLayerNet unused_parameter_size
     
     
    
      
         TwoLinLayerNet unused_parameter_size

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 3)

In addition, TORCH_DISTRIBUTED_DEBUG=INFO enhances crash logging in torch.nn.parallel.DistributedDataParallel() due to unused parameters in the model. Currently, find_unused_parameters=True
must be passed into torch.nn.parallel.DistributedDataParallel() initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required
to be used in loss computation as torch.nn.parallel.DistributedDataParallel() does not support unused parameters in the backwards pass. These constraints are challenging especially for larger
models, thus when crashing with an error, torch.nn.parallel.DistributedDataParallel() will log the fully qualified name of all parameters that went unused. For example, in the above application,
if we modify  to be instead computed as   , then TwoLinLayerNet.a does not receive a gradient in the backwards pass, and
thus results in  failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 4)

Code example:
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing
 the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by
making sure all `forward` function outputs participate in calculating loss.
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va
lue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameters which did not receive grad for rank 0: a.weight
Parameter indices which did not receive grad for rank 0: 0

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 5)

Setting TORCH_DISTRIBUTED_DEBUG=DETAIL will trigger additional consistency and synchronization checks on every collective call issued by the user
either directly or indirectly (such as DDP ). This is done by creating a wrapper process group that wraps all process groups returned by
torch.distributed.init_process_group() and torch.distributed.new_group() APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process
group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a torch.distributed.monitored_barrier(),
which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by
ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the
application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into
torch.distributed.all_reduce():

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 6)

Code example:
torch.distributed  
 torch.multiprocessing  


 
    init_process_group  world_size
    set_device
            
    all_reduce
    synchronize


   "__main__"
    "MASTER_ADDR"  "localhost"
    "MASTER_PORT"  
    "TORCH_CPP_LOG_LEVEL"
    "TORCH_DISTRIBUTED_DEBUG"

With the  backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables
TORCH_DISTRIBUTED_DEBUG=DETAIL and reruns the application, the following error message reveals the root cause:

Code example:
default_pg 
RuntimeError       collective             collective  mismatched      
 LongTensor

For fine-grained control of the debug level during runtime the functions torch.distributed.set_debug_level(), torch.distributed.set_debug_level_from_env(), and
torch.distributed.get_debug_level() can also be used.

================================================================================

# Distributed communication package - torch.distributed - TORCH_DISTRIBUTED_DEBUG (Part 7)

In addition, TORCH_DISTRIBUTED_DEBUG=DETAIL can be used in conjunction with TORCH_SHOW_CPP_STACKTRACES=1 to log the entire callstack when a collective desynchronization is detected. These
collective desynchronization checks will work for all applications that use  collective calls backed by process groups created with the
torch.distributed.init_process_group() and torch.distributed.new_group() APIs.

================================================================================

# Distributed communication package - torch.distributed - Logging (Part 1)

In addition to explicit debugging support via torch.distributed.monitored_barrier() and TORCH_DISTRIBUTED_DEBUG, the underlying C++ library of torch.distributed also outputs log
messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The
following matrix shows how the log level can be adjusted via the combination of TORCH_CPP_LOG_LEVEL and TORCH_DISTRIBUTED_DEBUG environment variables.

Table:
TORCH_CPP_LOG_LEVEL | TORCH_DISTRIBUTED_DEBUG | Effective Log Level
Trace (a.k.a. All)

TORCH_DISTRIBUTED_DEBUG

Distributed components raise custom Exception types derived from RuntimeError:

================================================================================

# Distributed communication package - torch.distributed - Logging (Part 2)

List:
torch.distributed.DistError: This is the base type of all distributed exceptions.
torch.distributed.DistBackendError: This exception is thrown when a backend-specific error occurs. For example, if
the  backend is used and the user attempts to use a GPU that is not available to the  library.
torch.distributed.DistNetworkError: This exception is thrown when networking
libraries encounter errors (ex: Connection reset by peer)
torch.distributed.DistStoreError: This exception is thrown when the Store encounters
an error (ex: TCPStore timeout)

torch.distributed.DistError: This is the base type of all distributed exceptions.

torch.distributed.DistBackendError: This exception is thrown when a backend-specific error occurs. For example, if
the  backend is used and the user attempts to use a GPU that is not available to the  library.

torch.distributed.DistNetworkError: This exception is thrown when networking
libraries encounter errors (ex: Connection reset by peer)

torch.distributed.DistStoreError: This exception is thrown when the Store encounters
an error (ex: TCPStore timeout)

torch.distributed.
Exception raised when an error occurs in the distributed library

================================================================================

# Distributed communication package - torch.distributed - Logging (Part 3)

Exception raised when an error occurs in the distributed library

torch.distributed.DistBackendError
Exception raised when a backend error occurs in distributed

Exception raised when a backend error occurs in distributed

torch.distributed.DistNetworkError
Exception raised when a network error occurs in distributed

Exception raised when a network error occurs in distributed

torch.distributed.DistStoreError
Exception raised when an error occurs in the distributed store

Exception raised when an error occurs in the distributed store

If you are running single node training, it may be convenient to interactively breakpoint your script. We offer a way to conveniently breakpoint a single rank:

torch.distributed.breakpoint, 
Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be
done with the breakpoint before continuing.

Parameters

 () – Which rank to break on.  Default: 
 () – Skip the first  calls to this breakpoint. Default: .

Set a breakpoint, but only on a single rank.  All other ranks will wait for you to be
done with the breakpoint before continuing.

Parameters

 () – Which rank to break on.  Default: 
 () – Skip the first  calls to this breakpoint. Default: .

================================================================================

# Distributed communication package - torch.distributed - Logging (Part 4)

List:
() – Which rank to break on.  Default: 
 () – Skip the first  calls to this breakpoint. Default: .

() – Which rank to break on.  Default:

() – Skip the first  calls to this breakpoint. Default: .

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.distributed.tensor

Created On: Jun 13, 2025 | Last Updated On: Jun 18, 2025

torch.distributed.tensor is currently in alpha state and under
development, we are committing backward compatibility for the most APIs listed
in the doc, but there might be API changes if necessary.

================================================================================

# torch.distributed.tensor - PyTorch DTensor (Distributed Tensor) (Part 1)

PyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed
logic, including sharded storage, operator computation and collective communications across devices/hosts.
 could be used to build different parallelism solutions and support sharded state_dict representation
when working with multi-dimensional sharding.

Please see examples from the PyTorch native parallelism solutions that are built on top of :

follows the SPMD (single program, multiple data) programming model to empower users to
write distributed program as if it’s a single-device program with the same convergence property. It
provides a uniform tensor sharding layout (DTensor Layout) through specifying the DeviceMesh
and :

List:
DeviceMesh represents the device topology and the communicators of the cluster using
an n-dimensional array.
 describes the sharding layout of the logical tensor on the DeviceMesh.
DTensor supports three types of placements: ,  and .

DeviceMesh represents the device topology and the communicators of the cluster using
an n-dimensional array.

================================================================================

# torch.distributed.tensor - PyTorch DTensor (Distributed Tensor) (Part 2)

describes the sharding layout of the logical tensor on the DeviceMesh.
DTensor supports three types of placements: ,  and .

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 1)

is a torch.Tensor subclass. This means once a  is created, it could be
used in very similar way to torch.Tensor, including running different types of PyTorch operators as if
running them in a single device, allowing proper distributed computation for PyTorch operators.

In addition to existing torch.Tensor methods, it also offers a set of additional methods to interact with
torch.Tensor, redistribute the DTensor Layout to a new DTensor, get the full tensor content
on all devices, etc.

torch.distributed.tensor.local_tensor, , , requires_grad
 (Distributed Tensor) is a subclass of torch.Tensor that provides single-device like
abstraction to program with multi-device torch.Tensor. It describes the distributed tensor sharding
layout (DTensor Layout) through the DeviceMesh and following types of :

: Tensor sharded on the tensor dimension  on the devices of the DeviceMesh dimension
: Tensor replicated on the devices of the DeviceMesh dimension
: Tensor is pending reduction on the devices of the DeviceMesh dimension

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 2)

When calling PyTorch operators,  overrides the PyTorch operators to perform sharded computation and issue
communications whenever necessary. Along with the operator computation,  will transform or propagate the
placements (DTensor Layout) properly (based on the operator semantic itself) and generate new  outputs.
To ensure numerical correctness of the  sharded computation when calling PyTorch operators, 
requires every Tensor argument of the operator be DTensor.


Directly using the Tensor subclass constructor here is not the recommended way to create a 
(i.e. it does not handle autograd correctly hence is not the public API). Please refer to the create_dtensor
section to see how to create a .


Return type





__create_chunk_list__
Return a list of ChunkStorageMetadata, which is a dataclass that describes the size/offset of the local shard/replica
on current rank. For DTensor, each rank will have a single local shard/replica, so the returned list usually only
has one element.
This dunder method is primariy used for distributed checkpoint purpose.


A List[ChunkStorageMetadata] object that represents the shard size/offset on the current rank.





================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 3)

from_locallocal_tensor, device_mesh, placements, , , , 
Create a  from a local torch.Tensor on each rank
according to the device_mesh and placements specified.

Parameters

local_tensor (torch.Tensor) – local torch.Tensor on each rank.
device_mesh (DeviceMesh, optional) – DeviceMesh to place the
tensor, if not specified, must be called under a DeviceMesh
context manager, default: None
placements (List[], optional) – the placements that
describes how to place the local torch.Tensor on DeviceMesh, must
have the same number of elements as device_mesh.ndim.


Keyword Arguments

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 4)

 () – at a cost of extra communications, perform
sanity check across ranks to check each local tensor’s meta information
to ensure correctness. If have  in placements, the
data on first rank of the device mesh dimension will be broadcasted
to other ranks. default: False
 (torch.Size) – A List of int which specifies the size of
DTensor which build on top of local_tensor. Note this needs to be
provided if the shape of local_tensor are different across the ranks.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None
 () – A List of int which specifies the stride of DTensor.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None



A  object

Return type





When run_check=False, it is the user’s responsibility to ensure the
local tensor passed in is correct across ranks (i.e. the tensor is sharded for
the Shard(dim) placement or replicated for the Replicate() placement).
If not, the behavior of the created DTensor is undefined.



from_local is differentiable, the requires_grad of the created
 object will depend on if local_tensor requires_grad or not.



================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 5)


full_tensor, grad_placements
Return the full tensor of this DTensor. It will perform necessary collectives
to gather the local tensors from other ranks in its DeviceMesh and concatenate
them together. It’s a syntatic sugar of the following code:
dtensor.redistribute(placements=[Replicate()]  mesh.ndim).to_local()

Keyword Arguments
grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the full Tensor returned from this
function.
full_tensor converts DTensor to a full torch.Tensor and the returned torch.tensor
might not be used as the original replicated DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original replicated DTensor layout.
If not specified, we will assume the gradient layout of the full tensor be replicated.


A torch.Tensor object that represents the full tensor of this DTensor.

Return type





full_tensor is differentiable.



================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 6)


redistributedevice_mesh, placements, , , forward_dtype, backward_dtype
redistribute performs necessary collective operations that redistribute the current
DTensor from its current placements to a new placements, or from its current DeviceMesh
to a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by
specifying a Replicate placement for each dimension of the DeviceMesh.
When redistributing from current to the new placements on one device mesh dimension, we
will perform the following operations including communication collective or local operation:

Shard(dim) -> Replicate(): all_gather
Shard(src_dim) -> Shard(dst_dim): all_to_all
Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)
 -> Replicate(): all_reduce
 -> Shard(dim): reduce_scatter

redistribute would correctly figure out the necessary redistribute steps for DTensors
that are created either on 1-D or N-D DeviceMesh.

Parameters

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 7)

device_mesh (DeviceMesh, optional) – DeviceMesh to place the
DTensor. If not specified, it would use the current DTensor’s DeviceMesh.
default: None
placements (List[], optional) – the new placements that
describes how to place the DTensor into the DeviceMesh, must
have the same number of elements as device_mesh.ndim.
default: replicate on all mesh dimensions


Keyword Arguments

 () – whether to perform the DTensor redistribute operation
asynchronously or not. Default: False
forward_dtype (torch.dtype) – the local tensor datatype can be converted to
forward_dtype before redistributing the local tensor in its forward.
The result DTensor will be in forward_dtype Default: None.
backward_dtype (torch.dtype) – the local tensor datatype can be converted to
backward_dtype before redistributing the local tensor in its backward.
The result DTensor gradient would be converted back to the current DTensor dtype. Default: None



A  object

Return type





redistribute is differentiable, which means user do not need to worry about
the backward formula of the redistribute operation.



================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 8)

redistribute currently only supports redistributing DTensor on the same DeviceMesh,
Please file an issue if you need to redistribute DTensor to different DeviceMesh.




, grad_placements
Get the local tensor of this DTensor on its current rank. For sharding it returns
a local shard of the logical tensor view, for replication it returns the replica on
its current rank.

Keyword Arguments
grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the Tensor returned from this
function.
 converts DTensor to local tensor and the returned local tensor
might not be used as the original DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original DTensor layout.
If not specified, we will assume the gradient layout remains the same
as the original DTensor and use that for gradient computation.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 9)


A torch.Tensor or AsyncCollectiveTensor object. it represents the
local tensor on its current rank. When an AsyncCollectiveTensor object is returned,
it means the local tensor is not ready yet (i.e. communication is not finished). In this
case, user needs to call  to wait the local tensor to be ready.

Return type





 is differentiable, the requires_grad of the local tensor returned
will depend on if the  requires_grad or not.




device_meshDeviceMesh
The DeviceMesh attribute that associates with this DTensor object.


device_mesh is a read-only property, it can not be set.




placementstorch.distributed.tensor.placement_types.Placement
The placements attribute of this DTensor that describes the layout of this
DTensor on the its DeviceMesh.


placements is a read-only property, it can not be set.

(Distributed Tensor) is a subclass of torch.Tensor that provides single-device like
abstraction to program with multi-device torch.Tensor. It describes the distributed tensor sharding
layout (DTensor Layout) through the DeviceMesh and following types of :

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 10)

List:
: Tensor sharded on the tensor dimension  on the devices of the DeviceMesh dimension
: Tensor replicated on the devices of the DeviceMesh dimension
: Tensor is pending reduction on the devices of the DeviceMesh dimension

: Tensor sharded on the tensor dimension  on the devices of the DeviceMesh dimension

: Tensor replicated on the devices of the DeviceMesh dimension

: Tensor is pending reduction on the devices of the DeviceMesh dimension

When calling PyTorch operators,  overrides the PyTorch operators to perform sharded computation and issue
communications whenever necessary. Along with the operator computation,  will transform or propagate the
placements (DTensor Layout) properly (based on the operator semantic itself) and generate new  outputs.

To ensure numerical correctness of the  sharded computation when calling PyTorch operators, 
requires every Tensor argument of the operator be DTensor.

Directly using the Tensor subclass constructor here is not the recommended way to create a 
(i.e. it does not handle autograd correctly hence is not the public API). Please refer to the create_dtensor
section to see how to create a .

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 11)

__create_chunk_list__
Return a list of ChunkStorageMetadata, which is a dataclass that describes the size/offset of the local shard/replica
on current rank. For DTensor, each rank will have a single local shard/replica, so the returned list usually only
has one element.
This dunder method is primariy used for distributed checkpoint purpose.


A List[ChunkStorageMetadata] object that represents the shard size/offset on the current rank.

Return a list of ChunkStorageMetadata, which is a dataclass that describes the size/offset of the local shard/replica
on current rank. For DTensor, each rank will have a single local shard/replica, so the returned list usually only
has one element.

This dunder method is primariy used for distributed checkpoint purpose.

A List[ChunkStorageMetadata] object that represents the shard size/offset on the current rank.

A List[ChunkStorageMetadata] object that represents the shard size/offset on the current rank.

from_locallocal_tensor, device_mesh, placements, , , , 
Create a  from a local torch.Tensor on each rank
according to the device_mesh and placements specified.

Parameters

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 12)

local_tensor (torch.Tensor) – local torch.Tensor on each rank.
device_mesh (DeviceMesh, optional) – DeviceMesh to place the
tensor, if not specified, must be called under a DeviceMesh
context manager, default: None
placements (List[], optional) – the placements that
describes how to place the local torch.Tensor on DeviceMesh, must
have the same number of elements as device_mesh.ndim.


Keyword Arguments

 () – at a cost of extra communications, perform
sanity check across ranks to check each local tensor’s meta information
to ensure correctness. If have  in placements, the
data on first rank of the device mesh dimension will be broadcasted
to other ranks. default: False
 (torch.Size) – A List of int which specifies the size of
DTensor which build on top of local_tensor. Note this needs to be
provided if the shape of local_tensor are different across the ranks.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None
 () – A List of int which specifies the stride of DTensor.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None



A  object

Return type





================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 13)

When run_check=False, it is the user’s responsibility to ensure the
local tensor passed in is correct across ranks (i.e. the tensor is sharded for
the Shard(dim) placement or replicated for the Replicate() placement).
If not, the behavior of the created DTensor is undefined.



from_local is differentiable, the requires_grad of the created
 object will depend on if local_tensor requires_grad or not.

Create a  from a local torch.Tensor on each rank
according to the device_mesh and placements specified.

Parameters

local_tensor (torch.Tensor) – local torch.Tensor on each rank.
device_mesh (DeviceMesh, optional) – DeviceMesh to place the
tensor, if not specified, must be called under a DeviceMesh
context manager, default: None
placements (List[], optional) – the placements that
describes how to place the local torch.Tensor on DeviceMesh, must
have the same number of elements as device_mesh.ndim.


Keyword Arguments

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 14)

 () – at a cost of extra communications, perform
sanity check across ranks to check each local tensor’s meta information
to ensure correctness. If have  in placements, the
data on first rank of the device mesh dimension will be broadcasted
to other ranks. default: False
 (torch.Size) – A List of int which specifies the size of
DTensor which build on top of local_tensor. Note this needs to be
provided if the shape of local_tensor are different across the ranks.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None
 () – A List of int which specifies the stride of DTensor.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None



A  object

Return type

List:
local_tensor (torch.Tensor) – local torch.Tensor on each rank.
device_mesh (DeviceMesh, optional) – DeviceMesh to place the
tensor, if not specified, must be called under a DeviceMesh
context manager, default: None
placements (List[], optional) – the placements that
describes how to place the local torch.Tensor on DeviceMesh, must
have the same number of elements as device_mesh.ndim.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 15)

local_tensor (torch.Tensor) – local torch.Tensor on each rank.

device_mesh (DeviceMesh, optional) – DeviceMesh to place the
tensor, if not specified, must be called under a DeviceMesh
context manager, default: None

placements (List[], optional) – the placements that
describes how to place the local torch.Tensor on DeviceMesh, must
have the same number of elements as device_mesh.ndim.

List:
() – at a cost of extra communications, perform
sanity check across ranks to check each local tensor’s meta information
to ensure correctness. If have  in placements, the
data on first rank of the device mesh dimension will be broadcasted
to other ranks. default: False
 (torch.Size) – A List of int which specifies the size of
DTensor which build on top of local_tensor. Note this needs to be
provided if the shape of local_tensor are different across the ranks.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None
 () – A List of int which specifies the stride of DTensor.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 16)

() – at a cost of extra communications, perform
sanity check across ranks to check each local tensor’s meta information
to ensure correctness. If have  in placements, the
data on first rank of the device mesh dimension will be broadcasted
to other ranks. default: False

(torch.Size) – A List of int which specifies the size of
DTensor which build on top of local_tensor. Note this needs to be
provided if the shape of local_tensor are different across the ranks.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None

() – A List of int which specifies the stride of DTensor.
If not provided,  will be computed assuming the given distributed
tensor is evenly sharded across ranks. default: None

When run_check=False, it is the user’s responsibility to ensure the
local tensor passed in is correct across ranks (i.e. the tensor is sharded for
the Shard(dim) placement or replicated for the Replicate() placement).
If not, the behavior of the created DTensor is undefined.

from_local is differentiable, the requires_grad of the created
 object will depend on if local_tensor requires_grad or not.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 17)

full_tensor, grad_placements
Return the full tensor of this DTensor. It will perform necessary collectives
to gather the local tensors from other ranks in its DeviceMesh and concatenate
them together. It’s a syntatic sugar of the following code:
dtensor.redistribute(placements=[Replicate()]  mesh.ndim).to_local()

Keyword Arguments
grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the full Tensor returned from this
function.
full_tensor converts DTensor to a full torch.Tensor and the returned torch.tensor
might not be used as the original replicated DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original replicated DTensor layout.
If not specified, we will assume the gradient layout of the full tensor be replicated.


A torch.Tensor object that represents the full tensor of this DTensor.

Return type





full_tensor is differentiable.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 18)

Return the full tensor of this DTensor. It will perform necessary collectives
to gather the local tensors from other ranks in its DeviceMesh and concatenate
them together. It’s a syntatic sugar of the following code:

dtensor.redistribute(placements=[Replicate()]  mesh.ndim).to_local()

Keyword Arguments
grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the full Tensor returned from this
function.
full_tensor converts DTensor to a full torch.Tensor and the returned torch.tensor
might not be used as the original replicated DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original replicated DTensor layout.
If not specified, we will assume the gradient layout of the full tensor be replicated.


A torch.Tensor object that represents the full tensor of this DTensor.

Return type

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 19)

grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the full Tensor returned from this
function.
full_tensor converts DTensor to a full torch.Tensor and the returned torch.tensor
might not be used as the original replicated DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original replicated DTensor layout.
If not specified, we will assume the gradient layout of the full tensor be replicated.

A torch.Tensor object that represents the full tensor of this DTensor.

full_tensor is differentiable.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 20)

redistributedevice_mesh, placements, , , forward_dtype, backward_dtype
redistribute performs necessary collective operations that redistribute the current
DTensor from its current placements to a new placements, or from its current DeviceMesh
to a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by
specifying a Replicate placement for each dimension of the DeviceMesh.
When redistributing from current to the new placements on one device mesh dimension, we
will perform the following operations including communication collective or local operation:

Shard(dim) -> Replicate(): all_gather
Shard(src_dim) -> Shard(dst_dim): all_to_all
Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)
 -> Replicate(): all_reduce
 -> Shard(dim): reduce_scatter

redistribute would correctly figure out the necessary redistribute steps for DTensors
that are created either on 1-D or N-D DeviceMesh.

Parameters

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 21)

device_mesh (DeviceMesh, optional) – DeviceMesh to place the
DTensor. If not specified, it would use the current DTensor’s DeviceMesh.
default: None
placements (List[], optional) – the new placements that
describes how to place the DTensor into the DeviceMesh, must
have the same number of elements as device_mesh.ndim.
default: replicate on all mesh dimensions


Keyword Arguments

 () – whether to perform the DTensor redistribute operation
asynchronously or not. Default: False
forward_dtype (torch.dtype) – the local tensor datatype can be converted to
forward_dtype before redistributing the local tensor in its forward.
The result DTensor will be in forward_dtype Default: None.
backward_dtype (torch.dtype) – the local tensor datatype can be converted to
backward_dtype before redistributing the local tensor in its backward.
The result DTensor gradient would be converted back to the current DTensor dtype. Default: None



A  object

Return type





redistribute is differentiable, which means user do not need to worry about
the backward formula of the redistribute operation.



================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 22)

redistribute currently only supports redistributing DTensor on the same DeviceMesh,
Please file an issue if you need to redistribute DTensor to different DeviceMesh.

redistribute performs necessary collective operations that redistribute the current
DTensor from its current placements to a new placements, or from its current DeviceMesh
to a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by
specifying a Replicate placement for each dimension of the DeviceMesh.

When redistributing from current to the new placements on one device mesh dimension, we
will perform the following operations including communication collective or local operation:

List:
Shard(dim) -> Replicate(): all_gather
Shard(src_dim) -> Shard(dst_dim): all_to_all
Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)
 -> Replicate(): all_reduce
 -> Shard(dim): reduce_scatter

Shard(dim) -> Replicate(): all_gather

Shard(src_dim) -> Shard(dst_dim): all_to_all

Replicate() -> Shard(dim): local chunking (i.e. torch.chunk)

-> Replicate(): all_reduce

-> Shard(dim): reduce_scatter

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 23)

redistribute would correctly figure out the necessary redistribute steps for DTensors
that are created either on 1-D or N-D DeviceMesh.

Parameters

device_mesh (DeviceMesh, optional) – DeviceMesh to place the
DTensor. If not specified, it would use the current DTensor’s DeviceMesh.
default: None
placements (List[], optional) – the new placements that
describes how to place the DTensor into the DeviceMesh, must
have the same number of elements as device_mesh.ndim.
default: replicate on all mesh dimensions


Keyword Arguments

 () – whether to perform the DTensor redistribute operation
asynchronously or not. Default: False
forward_dtype (torch.dtype) – the local tensor datatype can be converted to
forward_dtype before redistributing the local tensor in its forward.
The result DTensor will be in forward_dtype Default: None.
backward_dtype (torch.dtype) – the local tensor datatype can be converted to
backward_dtype before redistributing the local tensor in its backward.
The result DTensor gradient would be converted back to the current DTensor dtype. Default: None



A  object

Return type

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 24)

List:
device_mesh (DeviceMesh, optional) – DeviceMesh to place the
DTensor. If not specified, it would use the current DTensor’s DeviceMesh.
default: None
placements (List[], optional) – the new placements that
describes how to place the DTensor into the DeviceMesh, must
have the same number of elements as device_mesh.ndim.
default: replicate on all mesh dimensions

device_mesh (DeviceMesh, optional) – DeviceMesh to place the
DTensor. If not specified, it would use the current DTensor’s DeviceMesh.
default: None

placements (List[], optional) – the new placements that
describes how to place the DTensor into the DeviceMesh, must
have the same number of elements as device_mesh.ndim.
default: replicate on all mesh dimensions

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 25)

List:
() – whether to perform the DTensor redistribute operation
asynchronously or not. Default: False
forward_dtype (torch.dtype) – the local tensor datatype can be converted to
forward_dtype before redistributing the local tensor in its forward.
The result DTensor will be in forward_dtype Default: None.
backward_dtype (torch.dtype) – the local tensor datatype can be converted to
backward_dtype before redistributing the local tensor in its backward.
The result DTensor gradient would be converted back to the current DTensor dtype. Default: None

() – whether to perform the DTensor redistribute operation
asynchronously or not. Default: False

forward_dtype (torch.dtype) – the local tensor datatype can be converted to
forward_dtype before redistributing the local tensor in its forward.
The result DTensor will be in forward_dtype Default: None.

backward_dtype (torch.dtype) – the local tensor datatype can be converted to
backward_dtype before redistributing the local tensor in its backward.
The result DTensor gradient would be converted back to the current DTensor dtype. Default: None

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 26)

redistribute is differentiable, which means user do not need to worry about
the backward formula of the redistribute operation.

redistribute currently only supports redistributing DTensor on the same DeviceMesh,
Please file an issue if you need to redistribute DTensor to different DeviceMesh.

, grad_placements
Get the local tensor of this DTensor on its current rank. For sharding it returns
a local shard of the logical tensor view, for replication it returns the replica on
its current rank.

Keyword Arguments
grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the Tensor returned from this
function.
 converts DTensor to local tensor and the returned local tensor
might not be used as the original DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original DTensor layout.
If not specified, we will assume the gradient layout remains the same
as the original DTensor and use that for gradient computation.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 27)


A torch.Tensor or AsyncCollectiveTensor object. it represents the
local tensor on its current rank. When an AsyncCollectiveTensor object is returned,
it means the local tensor is not ready yet (i.e. communication is not finished). In this
case, user needs to call  to wait the local tensor to be ready.

Return type





 is differentiable, the requires_grad of the local tensor returned
will depend on if the  requires_grad or not.

Get the local tensor of this DTensor on its current rank. For sharding it returns
a local shard of the logical tensor view, for replication it returns the replica on
its current rank.

Keyword Arguments
grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the Tensor returned from this
function.
 converts DTensor to local tensor and the returned local tensor
might not be used as the original DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original DTensor layout.
If not specified, we will assume the gradient layout remains the same
as the original DTensor and use that for gradient computation.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 28)


A torch.Tensor or AsyncCollectiveTensor object. it represents the
local tensor on its current rank. When an AsyncCollectiveTensor object is returned,
it means the local tensor is not ready yet (i.e. communication is not finished). In this
case, user needs to call  to wait the local tensor to be ready.

Return type

grad_placements (List[], optional) – the placements describes
the future layout of any gradient layout of the Tensor returned from this
function.
 converts DTensor to local tensor and the returned local tensor
might not be used as the original DTensor layout later in the code. This
argument is the hint that user can give to autograd in case the gradient
layout of the returned tensor does not match the original DTensor layout.
If not specified, we will assume the gradient layout remains the same
as the original DTensor and use that for gradient computation.

A torch.Tensor or AsyncCollectiveTensor object. it represents the
local tensor on its current rank. When an AsyncCollectiveTensor object is returned,
it means the local tensor is not ready yet (i.e. communication is not finished). In this
case, user needs to call  to wait the local tensor to be ready.

================================================================================

# torch.distributed.tensor - DTensor Class APIs (Part 29)

is differentiable, the requires_grad of the local tensor returned
will depend on if the  requires_grad or not.

device_meshDeviceMesh
The DeviceMesh attribute that associates with this DTensor object.


device_mesh is a read-only property, it can not be set.

The DeviceMesh attribute that associates with this DTensor object.

device_mesh is a read-only property, it can not be set.

placementstorch.distributed.tensor.placement_types.Placement
The placements attribute of this DTensor that describes the layout of this
DTensor on the its DeviceMesh.


placements is a read-only property, it can not be set.

The placements attribute of this DTensor that describes the layout of this
DTensor on the its DeviceMesh.

placements is a read-only property, it can not be set.

================================================================================

# torch.distributed.tensor - DeviceMesh as the distributed communicator

DeviceMesh was built from DTensor as the abstraction to describe cluster’s device topology and represent
multi-dimensional communicators (on top of ProcessGroup). To see the details of how to create/use a DeviceMesh,
please refer to the DeviceMesh recipe.

================================================================================

# torch.distributed.tensor - DTensor Placement Types (Part 1)

DTensor supports the following types of  on each DeviceMesh dimension:

torch.distributed.tensor.placement_types.
The Shard(dim) placement describes the DTensor sharding on tensor dimension
 over a corresponding DeviceMesh dimension, where each rank on the
DeviceMesh dimension only holds a shard/piece of the global Tensor. The
Shard(dim) placement follows the torch.chunk(dim) semantic, where the
last few shards on the DeviceMesh dimension might be empty when the tensor dimension
is not evenly divisible on the DeviceMesh dimension. The  placement can be
used by all DTensor APIs (i.e. distribute_tensor, from_local, etc.)

Parameters
 () – The tensor dimension that describes the DTensor is sharded over its
corresponding DeviceMesh dimension.




sharding on a tensor dimension where the tensor dimension size is not
evenly divisible on a DeviceMesh dimension is currently experimental and subject to change.

================================================================================

# torch.distributed.tensor - DTensor Placement Types (Part 2)

The Shard(dim) placement describes the DTensor sharding on tensor dimension
 over a corresponding DeviceMesh dimension, where each rank on the
DeviceMesh dimension only holds a shard/piece of the global Tensor. The
Shard(dim) placement follows the torch.chunk(dim) semantic, where the
last few shards on the DeviceMesh dimension might be empty when the tensor dimension
is not evenly divisible on the DeviceMesh dimension. The  placement can be
used by all DTensor APIs (i.e. distribute_tensor, from_local, etc.)

Parameters
 () – The tensor dimension that describes the DTensor is sharded over its
corresponding DeviceMesh dimension.

() – The tensor dimension that describes the DTensor is sharded over its
corresponding DeviceMesh dimension.

sharding on a tensor dimension where the tensor dimension size is not
evenly divisible on a DeviceMesh dimension is currently experimental and subject to change.

================================================================================

# torch.distributed.tensor - DTensor Placement Types (Part 3)

torch.distributed.tensor.placement_types.
The Replicate() placement describes the DTensor replicating on a corresponding
DeviceMesh dimension, where each rank on the DeviceMesh dimension holds a
replica of the global Tensor. The  placement can be used by all
DTensor APIs (i.e. distribute_tensor, DTensor.from_local, etc.)

The Replicate() placement describes the DTensor replicating on a corresponding
DeviceMesh dimension, where each rank on the DeviceMesh dimension holds a
replica of the global Tensor. The  placement can be used by all
DTensor APIs (i.e. distribute_tensor, DTensor.from_local, etc.)

torch.distributed.tensor.placement_types.
The Partial(reduce_op) placement describes the DTensor that is pending
reduction on a specified DeviceMesh dimension, where each rank on the
DeviceMesh dimension holds the partial value of the global Tensor. User can
redistribute the  DTensor to a  or Shard(dim)
placement on the specified DeviceMesh dimension using redistribute,
which would trigger necessary communication operations under the hood (i.e.
, reduce_scatter).

================================================================================

# torch.distributed.tensor - DTensor Placement Types (Part 4)

Parameters
 () – The reduction op to be used for the partial DTensor
to produce Replicated/Sharded DTensor. Only element-wise reduction operations
are supported, including: “sum”, “avg”, “product”, “max”, “min”, default: “sum”.




The  placement can be generated as a result of the DTensor operators,
and can only be used by the DTensor.from_local API.

The Partial(reduce_op) placement describes the DTensor that is pending
reduction on a specified DeviceMesh dimension, where each rank on the
DeviceMesh dimension holds the partial value of the global Tensor. User can
redistribute the  DTensor to a  or Shard(dim)
placement on the specified DeviceMesh dimension using redistribute,
which would trigger necessary communication operations under the hood (i.e.
, reduce_scatter).

Parameters
 () – The reduction op to be used for the partial DTensor
to produce Replicated/Sharded DTensor. Only element-wise reduction operations
are supported, including: “sum”, “avg”, “product”, “max”, “min”, default: “sum”.

================================================================================

# torch.distributed.tensor - DTensor Placement Types (Part 5)

() – The reduction op to be used for the partial DTensor
to produce Replicated/Sharded DTensor. Only element-wise reduction operations
are supported, including: “sum”, “avg”, “product”, “max”, “min”, default: “sum”.

The  placement can be generated as a result of the DTensor operators,
and can only be used by the DTensor.from_local API.

torch.distributed.tensor.placement_types.
The base class for the Placement type, where it describes how a DTensor is placed onto the
DeviceMesh.  and DeviceMesh together could describe the DTensor Layout.
It is the base class of the three main DTensor Placement types: , ,
and .
This class is not meant to be used directly, mainly served as a typing stub.


is_partial

Return type






is_replicate

Return type








Return type

The base class for the Placement type, where it describes how a DTensor is placed onto the
DeviceMesh.  and DeviceMesh together could describe the DTensor Layout.
It is the base class of the three main DTensor Placement types: , ,
and .

This class is not meant to be used directly, mainly served as a typing stub.

is_partial

Return type

is_replicate

Return type

================================================================================

# torch.distributed.tensor - Different ways to create a DTensor (Part 1)

There’re three ways to construct a :
distribute_tensor() creates a  from a logical or “global” torch.Tensor on
each rank. This could be used to shard the leaf torch.Tensor s (i.e. model parameters/buffers
and inputs).
DTensor.from_local() creates a  from a local torch.Tensor on each rank, which can
be used to create  from a non-leaf torch.Tensor s (i.e. intermediate activation
tensors during forward/backward).
DTensor provides dedicated tensor factory functions (e.g. , , , etc.)
to allow different  creations by directly specifying the DeviceMesh and
. Compare to distribute_tensor(), this could directly materializing the sharded memory
on device, instead of performing sharding after initializing the logical Tensor memory.

================================================================================

# torch.distributed.tensor - Different ways to create a DTensor (Part 2)

List:
distribute_tensor() creates a  from a logical or “global” torch.Tensor on
each rank. This could be used to shard the leaf torch.Tensor s (i.e. model parameters/buffers
and inputs).
DTensor.from_local() creates a  from a local torch.Tensor on each rank, which can
be used to create  from a non-leaf torch.Tensor s (i.e. intermediate activation
tensors during forward/backward).
DTensor provides dedicated tensor factory functions (e.g. , , , etc.)
to allow different  creations by directly specifying the DeviceMesh and
. Compare to distribute_tensor(), this could directly materializing the sharded memory
on device, instead of performing sharding after initializing the logical Tensor memory.

distribute_tensor() creates a  from a logical or “global” torch.Tensor on
each rank. This could be used to shard the leaf torch.Tensor s (i.e. model parameters/buffers
and inputs).

DTensor.from_local() creates a  from a local torch.Tensor on each rank, which can
be used to create  from a non-leaf torch.Tensor s (i.e. intermediate activation
tensors during forward/backward).

================================================================================

# torch.distributed.tensor - Different ways to create a DTensor (Part 3)

DTensor provides dedicated tensor factory functions (e.g. , , , etc.)
to allow different  creations by directly specifying the DeviceMesh and
. Compare to distribute_tensor(), this could directly materializing the sharded memory
on device, instead of performing sharding after initializing the logical Tensor memory.

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 1)

The SPMD (single program, multiple data) programming model in torch.distributed launches multiple processes
(i.e. via ) to execute the same program, this means that the model inside the program would be
initialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly
on GPU if enough memory).

offers a distribute_tensor() API that could shard the model weights or Tensors to  s,
where it would create a DTensor from the “logical” Tensor on each process. This would empower the created
 s to comply with the single device semantic, which is critical for numerical correctness.

torch.distributed.tensor.distribute_tensor, device_mesh, placements, , src_data_rank
Distribute a leaf torch.Tensor (i.e. nn.Parameter/buffers) to the device_mesh according
to the placements specified. The rank of device_mesh and placements must be the
same. The  to distribute is the logical or “global” tensor, and the API would use
the  from first rank of the DeviceMesh dimension as the source of truth to preserve
the single-device semantic. If you want to construct a DTensor in the middle of the Autograd
computation, please use DTensor.from_local() instead.

Parameters

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 2)

 (torch.Tensor) – torch.Tensor to be distributed. Note that if you
want to shard a tensor on a dimension that is not evenly divisible by
the number of devices in that mesh dimension, we use torch.chunk
semantic to shard the tensor and scatter the shards. The uneven sharding
behavior is experimental and subject to change.
device_mesh (DeviceMesh, optional) – DeviceMesh to distribute the
tensor, if not specified, must be called under a DeviceMesh context
manager, default: None
placements (List[], optional) – the placements that
describes how to place the tensor on DeviceMesh, must have the same
number of elements as device_mesh.ndim. If not specified, we will
by default replicate the tensor across the device_mesh from the
first rank of each dimension of the device_mesh.

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 3)


Keyword Arguments
src_data_rank () – the rank of the source data for the logical/global tensor, it is
used by distribute_tensor() to scatter/broadcast the shards/replicas to other ranks.
By default, we use group_rank=0 on each DeviceMesh dimension as the source data to preserve
the single-device semantic. If passing  explicitly, distribute_tensor() simply uses
its local data instead of trying to preserve the single-device semantic via scatter/broadcast.
Default: 0


A  or XLAShardedTensor object.

Return type





When initialize the DeviceMesh with the  device_type, distribute_tensor
return XLAShardedTensor instead. see this issue
for more details. The XLA integration is experimental and subject to change.

Distribute a leaf torch.Tensor (i.e. nn.Parameter/buffers) to the device_mesh according
to the placements specified. The rank of device_mesh and placements must be the
same. The  to distribute is the logical or “global” tensor, and the API would use
the  from first rank of the DeviceMesh dimension as the source of truth to preserve
the single-device semantic. If you want to construct a DTensor in the middle of the Autograd
computation, please use DTensor.from_local() instead.

Parameters

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 4)

 (torch.Tensor) – torch.Tensor to be distributed. Note that if you
want to shard a tensor on a dimension that is not evenly divisible by
the number of devices in that mesh dimension, we use torch.chunk
semantic to shard the tensor and scatter the shards. The uneven sharding
behavior is experimental and subject to change.
device_mesh (DeviceMesh, optional) – DeviceMesh to distribute the
tensor, if not specified, must be called under a DeviceMesh context
manager, default: None
placements (List[], optional) – the placements that
describes how to place the tensor on DeviceMesh, must have the same
number of elements as device_mesh.ndim. If not specified, we will
by default replicate the tensor across the device_mesh from the
first rank of each dimension of the device_mesh.

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 5)


Keyword Arguments
src_data_rank () – the rank of the source data for the logical/global tensor, it is
used by distribute_tensor() to scatter/broadcast the shards/replicas to other ranks.
By default, we use group_rank=0 on each DeviceMesh dimension as the source data to preserve
the single-device semantic. If passing  explicitly, distribute_tensor() simply uses
its local data instead of trying to preserve the single-device semantic via scatter/broadcast.
Default: 0


A  or XLAShardedTensor object.

Return type

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 6)

List:
(torch.Tensor) – torch.Tensor to be distributed. Note that if you
want to shard a tensor on a dimension that is not evenly divisible by
the number of devices in that mesh dimension, we use torch.chunk
semantic to shard the tensor and scatter the shards. The uneven sharding
behavior is experimental and subject to change.
device_mesh (DeviceMesh, optional) – DeviceMesh to distribute the
tensor, if not specified, must be called under a DeviceMesh context
manager, default: None
placements (List[], optional) – the placements that
describes how to place the tensor on DeviceMesh, must have the same
number of elements as device_mesh.ndim. If not specified, we will
by default replicate the tensor across the device_mesh from the
first rank of each dimension of the device_mesh.

(torch.Tensor) – torch.Tensor to be distributed. Note that if you
want to shard a tensor on a dimension that is not evenly divisible by
the number of devices in that mesh dimension, we use torch.chunk
semantic to shard the tensor and scatter the shards. The uneven sharding
behavior is experimental and subject to change.

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 7)

device_mesh (DeviceMesh, optional) – DeviceMesh to distribute the
tensor, if not specified, must be called under a DeviceMesh context
manager, default: None

placements (List[], optional) – the placements that
describes how to place the tensor on DeviceMesh, must have the same
number of elements as device_mesh.ndim. If not specified, we will
by default replicate the tensor across the device_mesh from the
first rank of each dimension of the device_mesh.

src_data_rank () – the rank of the source data for the logical/global tensor, it is
used by distribute_tensor() to scatter/broadcast the shards/replicas to other ranks.
By default, we use group_rank=0 on each DeviceMesh dimension as the source data to preserve
the single-device semantic. If passing  explicitly, distribute_tensor() simply uses
its local data instead of trying to preserve the single-device semantic via scatter/broadcast.
Default: 0

A  or XLAShardedTensor object.

When initialize the DeviceMesh with the  device_type, distribute_tensor
return XLAShardedTensor instead. see this issue
for more details. The XLA integration is experimental and subject to change.

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 8)

Along with distribute_tensor(), DTensor also offers a distribute_module() API to allow easier
sharding on the  level

torch.distributed.tensor.distribute_module, device_mesh, partition_fn, , 
This function expose three functions to control the parameters/inputs/outputs of the module:
1. To perform sharding on the module before runtime execution by specifying the
partition_fn (i.e. allow user to convert Module parameters to 
parameters according to the partition_fn specified).
2. To control the inputs or outputs of the module during runtime execution by
specifying the  and . (i.e. convert the input to
, convert the output back to torch.Tensor)

Parameters

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 9)

 () – user module to be partitioned.
device_mesh (DeviceMesh) – the device mesh to place the module.
partition_fn () – the function to partition parameters (i.e. shard certain
parameters across the device_mesh). If partition_fn is not specified,
by default we replicate all module parameters of  across the mesh.
 () – specify the input distribution, i.e. could control how the
input of the module is sharded.  will be installed as a module
forward_pre_hook (pre forward hook).
 () – specify the output distribution, i.e. could control how the
output is sharded, or convert it back to torch.Tensor.  will be
installed as a module forward_hook (post forward hook).



A module that contains parameters/buffers that are all  s.

Return type





When initialize the DeviceMesh with the  device_type, distribute_module
return nn.Module with PyTorch/XLA SPMD annotated parameters. See
this issue
for more details. The XLA integration is experimental and subject to change.

This function expose three functions to control the parameters/inputs/outputs of the module:

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 10)

1. To perform sharding on the module before runtime execution by specifying the
partition_fn (i.e. allow user to convert Module parameters to 
parameters according to the partition_fn specified).
2. To control the inputs or outputs of the module during runtime execution by
specifying the  and . (i.e. convert the input to
, convert the output back to torch.Tensor)

Parameters

 () – user module to be partitioned.
device_mesh (DeviceMesh) – the device mesh to place the module.
partition_fn () – the function to partition parameters (i.e. shard certain
parameters across the device_mesh). If partition_fn is not specified,
by default we replicate all module parameters of  across the mesh.
 () – specify the input distribution, i.e. could control how the
input of the module is sharded.  will be installed as a module
forward_pre_hook (pre forward hook).
 () – specify the output distribution, i.e. could control how the
output is sharded, or convert it back to torch.Tensor.  will be
installed as a module forward_hook (post forward hook).



A module that contains parameters/buffers that are all  s.

Return type

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 11)

List:
() – user module to be partitioned.
device_mesh (DeviceMesh) – the device mesh to place the module.
partition_fn () – the function to partition parameters (i.e. shard certain
parameters across the device_mesh). If partition_fn is not specified,
by default we replicate all module parameters of  across the mesh.
 () – specify the input distribution, i.e. could control how the
input of the module is sharded.  will be installed as a module
forward_pre_hook (pre forward hook).
 () – specify the output distribution, i.e. could control how the
output is sharded, or convert it back to torch.Tensor.  will be
installed as a module forward_hook (post forward hook).

() – user module to be partitioned.

device_mesh (DeviceMesh) – the device mesh to place the module.

partition_fn () – the function to partition parameters (i.e. shard certain
parameters across the device_mesh). If partition_fn is not specified,
by default we replicate all module parameters of  across the mesh.

() – specify the input distribution, i.e. could control how the
input of the module is sharded.  will be installed as a module
forward_pre_hook (pre forward hook).

================================================================================

# torch.distributed.tensor - Create DTensor from a logical torch.Tensor (Part 12)

() – specify the output distribution, i.e. could control how the
output is sharded, or convert it back to torch.Tensor.  will be
installed as a module forward_hook (post forward hook).

A module that contains parameters/buffers that are all  s.

When initialize the DeviceMesh with the  device_type, distribute_module
return nn.Module with PyTorch/XLA SPMD annotated parameters. See
this issue
for more details. The XLA integration is experimental and subject to change.

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 1)

DTensor also provides dedicated tensor factory functions to allow creating  directly
using torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally
specifying the DeviceMesh and  for the  created:

torch.distributed.tensor., requires_grad, , torch.strided, device_mesh, placements
Returns a  filled with the scalar value 0.

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..))

Keyword Arguments

requires_grad () – If autograd should record operations on the
returned . Default: .
 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned .
Default: torch.strided.
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: , 



A  object on each rank

Return type

Returns a  filled with the scalar value 0.

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 2)

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..))

Keyword Arguments

requires_grad () – If autograd should record operations on the
returned . Default: .
 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned .
Default: torch.strided.
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: , 



A  object on each rank

Return type

() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..))

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 3)

List:
requires_grad () – If autograd should record operations on the
returned . Default: .
 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned .
Default: torch.strided.
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: ,

requires_grad () – If autograd should record operations on the
returned . Default: .

(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).

(torch.layout, optional) – the desired layout of returned .
Default: torch.strided.

device_mesh – DeviceMesh type, contains the mesh info of ranks

placements – a sequence of  type: ,

A  object on each rank

torch.distributed.tensor., , torch.strided, requires_grad, device_mesh, placements
Returns a  filled with the scalar value 1, with the shape defined
by the variable argument .

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 4)

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: , 



A  object on each rank

Return type

Returns a  filled with the scalar value 1, with the shape defined
by the variable argument .

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

Keyword Arguments

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 5)

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: , 



A  object on each rank

Return type

() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

List:
(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: ,

(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 6)

(torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.

requires_grad () – If autograd should record operations on the
returned . Default: .

device_mesh – DeviceMesh type, contains the mesh info of ranks

placements – a sequence of  type: ,

A  object on each rank

torch.distributed.tensor., , torch.strided, requires_grad, device_mesh, placements
Returns a  filled with uninitialized data. The shape of the 
is defined by the variable argument .

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..))

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).        layout (torch.layout, optional): the desired layout of returned .
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: , 



A  object on each rank

Return type

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 7)

Returns a  filled with uninitialized data. The shape of the 
is defined by the variable argument .

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..))

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).        layout (torch.layout, optional): the desired layout of returned .
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: , 



A  object on each rank

Return type

() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..))

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 8)

List:
(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).        layout (torch.layout, optional): the desired layout of returned .
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks
placements – a sequence of  type: ,

(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).        layout (torch.layout, optional): the desired layout of returned .
Default: torch.strided.

requires_grad () – If autograd should record operations on the
returned . Default: .

device_mesh – DeviceMesh type, contains the mesh info of ranks

placements – a sequence of  type: ,

A  object on each rank

torch.distributed.tensor., fill_value, , , torch.strided, requires_grad, device_mesh, placements
Returns a  filled with fill_value according to device_mesh and
placements, with the shape defined by the argument .

Parameters

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 9)

 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))
fill_value () – the value to fill the output tensor with.


Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: , 



A  object on each rank

Return type

Returns a  filled with fill_value according to device_mesh and
placements, with the shape defined by the argument .

Parameters

 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))
fill_value () – the value to fill the output tensor with.


Keyword Arguments

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 10)

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: , 



A  object on each rank

Return type

List:
() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))
fill_value () – the value to fill the output tensor with.

() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

fill_value () – the value to fill the output tensor with.

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 11)

List:
(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: ,

(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).

(torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.

requires_grad () – If autograd should record operations on the
returned . Default: .

device_mesh – DeviceMesh type, contains the mesh info of ranks.

placements – a sequence of  type: ,

A  object on each rank

torch.distributed.tensor., requires_grad, , torch.strided, device_mesh, placements
Returns a  filled with random numbers from a uniform distribution
on the interval  . The shape of the tensor is defined by the variable
argument .

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 12)

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: , 



A  object on each rank

Return type

Returns a  filled with random numbers from a uniform distribution
on the interval  . The shape of the tensor is defined by the variable
argument .

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

Keyword Arguments

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 13)

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: , 



A  object on each rank

Return type

() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

List:
(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: ,

(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 14)

(torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.

requires_grad () – If autograd should record operations on the
returned . Default: .

device_mesh – DeviceMesh type, contains the mesh info of ranks.

placements – a sequence of  type: ,

A  object on each rank

torch.distributed.tensor., requires_grad, , torch.strided, device_mesh, placements
Returns a  filled with random numbers from a normal distribution
with mean 0 and variance 1. The shape of the tensor is defined by the variable
argument .

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: , 



A  object on each rank

Return type

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 15)

Returns a  filled with random numbers from a normal distribution
with mean 0 and variance 1. The shape of the tensor is defined by the variable
argument .

Parameters
 () – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: , 



A  object on each rank

Return type

() – a sequence of integers defining the shape of the output .
Can be a variable number of arguments or a collection like a list or tuple.
E.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))

================================================================================

# torch.distributed.tensor - DTensor Factory Functions (Part 16)

List:
(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).
 (torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.
requires_grad () – If autograd should record operations on the
returned . Default: .
device_mesh – DeviceMesh type, contains the mesh info of ranks.
placements – a sequence of  type: ,

(torch.dtype, optional) – the desired data type of returned .
Default: if , uses a global default (see torch.set_default_dtype()).

(torch.layout, optional) – the desired layout of returned DTensor.
Default: torch.strided.

requires_grad () – If autograd should record operations on the
returned . Default: .

device_mesh – DeviceMesh type, contains the mesh info of ranks.

placements – a sequence of  type: ,

A  object on each rank

================================================================================

# torch.distributed.tensor - Logging

When launching the program, you can turn on additional logging using the TORCH_LOGS environment variable from
torch._logging :

List:
TORCH_LOGS=+dtensor will display logging.DEBUG messages and all levels above it.
TORCH_LOGS=dtensor will display logging.INFO messages and above.
TORCH_LOGS=-dtensor will display logging.WARNING messages and above.

TORCH_LOGS=+dtensor will display logging.DEBUG messages and all levels above it.

TORCH_LOGS=dtensor will display logging.INFO messages and above.

TORCH_LOGS=-dtensor will display logging.WARNING messages and above.

================================================================================

# torch.distributed.tensor - Debugging Tools (Part 1)

To debug the program that applied DTensor, and understand more details about what collectives happened under the
hood, DTensor provides a CommDebugMode:

torch.distributed.tensor.debug.CommDebugMode
CommDebugMode is a context manager that counts the number of
functional collectives within its context. It does this using a
TorchDispatchMode.


Not all collectives are supported yet.

Example usage
  
  CommDebugMode
 
    
get_comm_counts




generate_comm_debug_tracing_tablenoise_level
Generates detailed table displaying operations and collective tracing information
on a module level. Amount of information is dependent on noise_level

prints module-level collective counts
prints dTensor operations not included in trivial operations, module information
prints operations not included in trivial operations
prints all operations




generate_json_dump'comm_mode_log.json', noise_level
Creates json file used to build browser visual
0. prints module-level collective counts
1. prints dTensor operations not included in trivial operations
2. prints operations not included in trivial operations
3. prints all operations



get_comm_counts
Returns the communication counts as a dictionary.

================================================================================

# torch.distributed.tensor - Debugging Tools (Part 2)


The communication counts as a dictionary.

Return type
Dict[Any, ]





get_parameter_info

Return type
[, [, ]]





get_sharding_info

Return type
[, [, ]]





get_total_counts

Return type






log_comm_debug_tracing_table_to_file'comm_mode_log.txt', noise_level
Alternative to console CommDebugMode output, writes to file specified by the user

CommDebugMode is a context manager that counts the number of
functional collectives within its context. It does this using a
TorchDispatchMode.

Not all collectives are supported yet.

Code example:
CommDebugMode
 
    
get_comm_counts

generate_comm_debug_tracing_tablenoise_level
Generates detailed table displaying operations and collective tracing information
on a module level. Amount of information is dependent on noise_level

prints module-level collective counts
prints dTensor operations not included in trivial operations, module information
prints operations not included in trivial operations
prints all operations

Generates detailed table displaying operations and collective tracing information
on a module level. Amount of information is dependent on noise_level

================================================================================

# torch.distributed.tensor - Debugging Tools (Part 3)

List:
prints module-level collective counts
prints dTensor operations not included in trivial operations, module information
prints operations not included in trivial operations
prints all operations

prints module-level collective counts

prints dTensor operations not included in trivial operations, module information

prints operations not included in trivial operations

prints all operations

generate_json_dump'comm_mode_log.json', noise_level
Creates json file used to build browser visual
0. prints module-level collective counts
1. prints dTensor operations not included in trivial operations
2. prints operations not included in trivial operations
3. prints all operations

Creates json file used to build browser visual
0. prints module-level collective counts
1. prints dTensor operations not included in trivial operations
2. prints operations not included in trivial operations
3. prints all operations

get_comm_counts
Returns the communication counts as a dictionary.


The communication counts as a dictionary.

Return type
Dict[Any, ]

Returns the communication counts as a dictionary.

The communication counts as a dictionary.

Return type
Dict[Any, ]

The communication counts as a dictionary.

================================================================================

# torch.distributed.tensor - Debugging Tools (Part 4)

get_parameter_info

Return type
[, [, ]]

get_sharding_info

Return type
[, [, ]]

get_total_counts

Return type

log_comm_debug_tracing_table_to_file'comm_mode_log.txt', noise_level
Alternative to console CommDebugMode output, writes to file specified by the user

Alternative to console CommDebugMode output, writes to file specified by the user

To visualize the sharding of a DTensor that have less than 3 dimensions, DTensor provides visualize_sharding():

torch.distributed.tensor.debug.visualize_sharding, , 
Visualizes sharding in the terminal for  that are 1D or 2D.


This requires the  package, or  and matplotlib.
No sharding info will be printed for empty tensors

Visualizes sharding in the terminal for  that are 1D or 2D.

This requires the  package, or  and matplotlib.
No sharding info will be printed for empty tensors

================================================================================

# torch.distributed.tensor - Experimental Features (Part 1)

also provides a set of experimental features. These features are either in prototyping stage, or the basic
functionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to
these features.

torch.distributed.tensor.experimental.context_parallel, , , buffer_seq_dims, no_restore_buffers
context_parallel is an experimental API to enable context
parallelism (CP). This API performs two actions: 1) patch the SDPA
(torch.nn.functional.scaled_dot_product_attention) with the CP-enabled
one, 2) shard  along the sequence dimension and each rank will
preserve the corresponding shard according .

Parameters

================================================================================

# torch.distributed.tensor - Experimental Features (Part 2)

 (DeviceMesh) – the device mesh for the context parallelism.
 (torch.Tensor) – buffers that the usage depend
on the sequence dimension. Examples are input batch, labels and
positional embedding buffers. These buffers must be sharded along
the sequence dimension to ensure the accuracy. The sharding will
happen in-place, the buffer’s shape will change within the context.
The buffers will be restored after the context finishes.
no_restore_buffers can be used to specify which buffers don’t
need to be restored. Note that  should not contain any
nn.Parameter.
buffer_seq_dims () – the sequence dimensions of .
no_restore_buffers (torch.Tensor) – buffers in these set
won’t be restored after the context exits. This set must be a subset
of . If the buffers won’t be used after the context exits,
these buffers can be put in this list to avoid extra restore time.


Return type
[None, None, None]




torch.distributed.tensor.experimental.context_parallel is a
prototype feature in PyTorch. The API is subject to change.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 3)

context_parallel is an experimental API to enable context
parallelism (CP). This API performs two actions: 1) patch the SDPA
(torch.nn.functional.scaled_dot_product_attention) with the CP-enabled
one, 2) shard  along the sequence dimension and each rank will
preserve the corresponding shard according .

Parameters

 (DeviceMesh) – the device mesh for the context parallelism.
 (torch.Tensor) – buffers that the usage depend
on the sequence dimension. Examples are input batch, labels and
positional embedding buffers. These buffers must be sharded along
the sequence dimension to ensure the accuracy. The sharding will
happen in-place, the buffer’s shape will change within the context.
The buffers will be restored after the context finishes.
no_restore_buffers can be used to specify which buffers don’t
need to be restored. Note that  should not contain any
nn.Parameter.
buffer_seq_dims () – the sequence dimensions of .
no_restore_buffers (torch.Tensor) – buffers in these set
won’t be restored after the context exits. This set must be a subset
of . If the buffers won’t be used after the context exits,
these buffers can be put in this list to avoid extra restore time.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 4)


Return type
[None, None, None]

List:
(DeviceMesh) – the device mesh for the context parallelism.
 (torch.Tensor) – buffers that the usage depend
on the sequence dimension. Examples are input batch, labels and
positional embedding buffers. These buffers must be sharded along
the sequence dimension to ensure the accuracy. The sharding will
happen in-place, the buffer’s shape will change within the context.
The buffers will be restored after the context finishes.
no_restore_buffers can be used to specify which buffers don’t
need to be restored. Note that  should not contain any
nn.Parameter.
buffer_seq_dims () – the sequence dimensions of .
no_restore_buffers (torch.Tensor) – buffers in these set
won’t be restored after the context exits. This set must be a subset
of . If the buffers won’t be used after the context exits,
these buffers can be put in this list to avoid extra restore time.

(DeviceMesh) – the device mesh for the context parallelism.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 5)

(torch.Tensor) – buffers that the usage depend
on the sequence dimension. Examples are input batch, labels and
positional embedding buffers. These buffers must be sharded along
the sequence dimension to ensure the accuracy. The sharding will
happen in-place, the buffer’s shape will change within the context.
The buffers will be restored after the context finishes.
no_restore_buffers can be used to specify which buffers don’t
need to be restored. Note that  should not contain any
nn.Parameter.

buffer_seq_dims () – the sequence dimensions of .

no_restore_buffers (torch.Tensor) – buffers in these set
won’t be restored after the context exits. This set must be a subset
of . If the buffers won’t be used after the context exits,
these buffers can be put in this list to avoid extra restore time.

torch.distributed.tensor.experimental.context_parallel is a
prototype feature in PyTorch. The API is subject to change.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 6)

torch.distributed.tensor.experimental., out_placements, in_placements, in_grad_placements, device_mesh, , redistribute_inputs
local_map() is an experimental API that allows users to pass  s
to a function that is written to be applied on torch.Tensor s. It is done by extracting
the local components of , call the function, and wrap the outputs to
 according to the out_placements.

Parameters

================================================================================

# torch.distributed.tensor - Experimental Features (Part 7)

 () – the function to be applied on each local shard of
 s.
out_placements (Union[PlacementType, Tuple[PlacementType, …]]) – the desired placements of the  s in ’s flattened output.
If the flattened  is a single value, the out_placements should be
of type PlacementType. Otherwise if the flattened  has multiple
values, the out_placements should be a tuple of PlacementType values 1:1
mapping to the flattened .
Besides, for  output, we use PlacementType as its
placements (a Tuple[Placement] value). For non-Tensor output, the PlacementType
should be .
Note that the only exception is when no  argument is passed
in. In this case, even if out_placements is not , the result function
should ignore the desired placements because the function is not running with
 s.
in_placements (Tuple[PlacementType, …], optional) – the required placements of the  s in the flattened inputs of .
If in_placements is specified, local_map() would examine whether the
placements of each  argument is the same as the required
placements or not. If the placements are not the same and
redistribute_inputs is , an exception will be raised. Otherwise if
redistribute_inputs is , the argument will be first redistributed to
the required sharding placements before passing its local tensor to .
The only exception is when required placements are not  and the
argument is a torch.Tensor. In this case, the placements examination
will be skipped and the argument will be directly passed to .
If in_placements is , no placements examination will be performed.
Default: None
in_grad_placements (Tuple[PlacementType, …], optional) – the placements hint of the  s gradient corresponds
to the flattened input DTensor. This argument is the hint that user
can give to to_local() in case the gradient layout of the
local tensor input does not match its  input layout.
If not specified, we will assume the gradient layout of the local
tensor input remains the same as the original  input
and use that for gradient computation. Default: None.
device_mesh (DeviceMesh, optional) – the device mesh that all the  s are placed on. If not
specified, this will be inferred from the input  s’ device
mesh.  requires every  s to be placed on the same
device mesh. Default: None.
redistribute_inputs () – the bool value indicating whether to reshard the input  s when
their placements are different from the required input placements. If this
value is  and some  input has a different placement,
an exception will be raised. Default: False.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 8)



A  that applies  to each local shard of the input 
and returns a  constructed from the return value of .



AssertionError – If the input  is not placed on the same device
    mesh, or if they are placed on a different device mesh than the device_mesh
    argument passed in.
AssertionError – For any non-DTensor output, we require its corresponding
    output placement in out_placements be None. An AssertionError will be raised
    if this is not the case.
ValueError – If redistribute_inputs=False but the input  needs
    a redistribution according to in_placements.




 mm_allreduce_forwarddevice_mesh  
    partial_sum_tensor   
    reduced_tensor  all_reducepartial_sum_tensor  device_mesh
     reduced_tensor

    requires_grad
    requires_grad
   
    # row-wise sharding placements on 1-d mesh
    # col-wise sharding placements on 1-d mesh

# local_mm_allreduce_forward is the function wrapped with DTensor/Tensor convertion
local_mm_allreduce_forward  
    mm_allreduce_forward
    out_placements
    in_placements 
    device_meshdevice_mesh

================================================================================

# torch.distributed.tensor - Experimental Features (Part 9)


  distribute_tensor
     device_mesh 
  # col-wisely sharded W tensor
  distribute_tensor
     device_mesh 
  # row-wisely sharded X tensor
  local_mm_allreduce_forward
    device_mesh  
  # apply local_mm_allreduce_forward to DTensors




This API is currently experimental and subject to change

local_map() is an experimental API that allows users to pass  s
to a function that is written to be applied on torch.Tensor s. It is done by extracting
the local components of , call the function, and wrap the outputs to
 according to the out_placements.

Parameters

================================================================================

# torch.distributed.tensor - Experimental Features (Part 10)

 () – the function to be applied on each local shard of
 s.
out_placements (Union[PlacementType, Tuple[PlacementType, …]]) – the desired placements of the  s in ’s flattened output.
If the flattened  is a single value, the out_placements should be
of type PlacementType. Otherwise if the flattened  has multiple
values, the out_placements should be a tuple of PlacementType values 1:1
mapping to the flattened .
Besides, for  output, we use PlacementType as its
placements (a Tuple[Placement] value). For non-Tensor output, the PlacementType
should be .
Note that the only exception is when no  argument is passed
in. In this case, even if out_placements is not , the result function
should ignore the desired placements because the function is not running with
 s.
in_placements (Tuple[PlacementType, …], optional) – the required placements of the  s in the flattened inputs of .
If in_placements is specified, local_map() would examine whether the
placements of each  argument is the same as the required
placements or not. If the placements are not the same and
redistribute_inputs is , an exception will be raised. Otherwise if
redistribute_inputs is , the argument will be first redistributed to
the required sharding placements before passing its local tensor to .
The only exception is when required placements are not  and the
argument is a torch.Tensor. In this case, the placements examination
will be skipped and the argument will be directly passed to .
If in_placements is , no placements examination will be performed.
Default: None
in_grad_placements (Tuple[PlacementType, …], optional) – the placements hint of the  s gradient corresponds
to the flattened input DTensor. This argument is the hint that user
can give to to_local() in case the gradient layout of the
local tensor input does not match its  input layout.
If not specified, we will assume the gradient layout of the local
tensor input remains the same as the original  input
and use that for gradient computation. Default: None.
device_mesh (DeviceMesh, optional) – the device mesh that all the  s are placed on. If not
specified, this will be inferred from the input  s’ device
mesh.  requires every  s to be placed on the same
device mesh. Default: None.
redistribute_inputs () – the bool value indicating whether to reshard the input  s when
their placements are different from the required input placements. If this
value is  and some  input has a different placement,
an exception will be raised. Default: False.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 11)



A  that applies  to each local shard of the input 
and returns a  constructed from the return value of .



AssertionError – If the input  is not placed on the same device
    mesh, or if they are placed on a different device mesh than the device_mesh
    argument passed in.
AssertionError – For any non-DTensor output, we require its corresponding
    output placement in out_placements be None. An AssertionError will be raised
    if this is not the case.
ValueError – If redistribute_inputs=False but the input  needs
    a redistribution according to in_placements.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 12)

List:
() – the function to be applied on each local shard of
 s.
out_placements (Union[PlacementType, Tuple[PlacementType, …]]) – the desired placements of the  s in ’s flattened output.
If the flattened  is a single value, the out_placements should be
of type PlacementType. Otherwise if the flattened  has multiple
values, the out_placements should be a tuple of PlacementType values 1:1
mapping to the flattened .
Besides, for  output, we use PlacementType as its
placements (a Tuple[Placement] value). For non-Tensor output, the PlacementType
should be .
Note that the only exception is when no  argument is passed
in. In this case, even if out_placements is not , the result function
should ignore the desired placements because the function is not running with
 s.
in_placements (Tuple[PlacementType, …], optional) – the required placements of the  s in the flattened inputs of .
If in_placements is specified, local_map() would examine whether the
placements of each  argument is the same as the required
placements or not. If the placements are not the same and
redistribute_inputs is , an exception will be raised. Otherwise if
redistribute_inputs is , the argument will be first redistributed to
the required sharding placements before passing its local tensor to .
The only exception is when required placements are not  and the
argument is a torch.Tensor. In this case, the placements examination
will be skipped and the argument will be directly passed to .
If in_placements is , no placements examination will be performed.
Default: None
in_grad_placements (Tuple[PlacementType, …], optional) – the placements hint of the  s gradient corresponds
to the flattened input DTensor. This argument is the hint that user
can give to to_local() in case the gradient layout of the
local tensor input does not match its  input layout.
If not specified, we will assume the gradient layout of the local
tensor input remains the same as the original  input
and use that for gradient computation. Default: None.
device_mesh (DeviceMesh, optional) – the device mesh that all the  s are placed on. If not
specified, this will be inferred from the input  s’ device
mesh.  requires every  s to be placed on the same
device mesh. Default: None.
redistribute_inputs () – the bool value indicating whether to reshard the input  s when
their placements are different from the required input placements. If this
value is  and some  input has a different placement,
an exception will be raised. Default: False.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 13)

() – the function to be applied on each local shard of
 s.

out_placements (Union[PlacementType, Tuple[PlacementType, …]]) – the desired placements of the  s in ’s flattened output.
If the flattened  is a single value, the out_placements should be
of type PlacementType. Otherwise if the flattened  has multiple
values, the out_placements should be a tuple of PlacementType values 1:1
mapping to the flattened .
Besides, for  output, we use PlacementType as its
placements (a Tuple[Placement] value). For non-Tensor output, the PlacementType
should be .
Note that the only exception is when no  argument is passed
in. In this case, even if out_placements is not , the result function
should ignore the desired placements because the function is not running with
 s.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 14)

in_placements (Tuple[PlacementType, …], optional) – the required placements of the  s in the flattened inputs of .
If in_placements is specified, local_map() would examine whether the
placements of each  argument is the same as the required
placements or not. If the placements are not the same and
redistribute_inputs is , an exception will be raised. Otherwise if
redistribute_inputs is , the argument will be first redistributed to
the required sharding placements before passing its local tensor to .
The only exception is when required placements are not  and the
argument is a torch.Tensor. In this case, the placements examination
will be skipped and the argument will be directly passed to .
If in_placements is , no placements examination will be performed.
Default: None

================================================================================

# torch.distributed.tensor - Experimental Features (Part 15)

in_grad_placements (Tuple[PlacementType, …], optional) – the placements hint of the  s gradient corresponds
to the flattened input DTensor. This argument is the hint that user
can give to to_local() in case the gradient layout of the
local tensor input does not match its  input layout.
If not specified, we will assume the gradient layout of the local
tensor input remains the same as the original  input
and use that for gradient computation. Default: None.

device_mesh (DeviceMesh, optional) – the device mesh that all the  s are placed on. If not
specified, this will be inferred from the input  s’ device
mesh.  requires every  s to be placed on the same
device mesh. Default: None.

redistribute_inputs () – the bool value indicating whether to reshard the input  s when
their placements are different from the required input placements. If this
value is  and some  input has a different placement,
an exception will be raised. Default: False.

A  that applies  to each local shard of the input 
and returns a  constructed from the return value of .

================================================================================

# torch.distributed.tensor - Experimental Features (Part 16)

List:
AssertionError – If the input  is not placed on the same device
    mesh, or if they are placed on a different device mesh than the device_mesh
    argument passed in.
AssertionError – For any non-DTensor output, we require its corresponding
    output placement in out_placements be None. An AssertionError will be raised
    if this is not the case.
ValueError – If redistribute_inputs=False but the input  needs
    a redistribution according to in_placements.

AssertionError – If the input  is not placed on the same device
    mesh, or if they are placed on a different device mesh than the device_mesh
    argument passed in.

AssertionError – For any non-DTensor output, we require its corresponding
    output placement in out_placements be None. An AssertionError will be raised
    if this is not the case.

ValueError – If redistribute_inputs=False but the input  needs
    a redistribution according to in_placements.

Code example:
mm_allreduce_forwarddevice_mesh  
    partial_sum_tensor   
    reduced_tensor  all_reducepartial_sum_tensor  device_mesh
     reduced_tensor

================================================================================

# torch.distributed.tensor - Experimental Features (Part 17)

    requires_grad
    requires_grad
   
    # row-wise sharding placements on 1-d mesh
    # col-wise sharding placements on 1-d mesh

# local_mm_allreduce_forward is the function wrapped with DTensor/Tensor convertion
local_mm_allreduce_forward  
    mm_allreduce_forward
    out_placements
    in_placements 
    device_meshdevice_mesh


  distribute_tensor
     device_mesh 
  # col-wisely sharded W tensor
  distribute_tensor
     device_mesh 
  # row-wisely sharded X tensor
  local_mm_allreduce_forward
    device_mesh  
  # apply local_mm_allreduce_forward to DTensors

This API is currently experimental and subject to change

torch.distributed.tensor.experimental.register_sharding
register_sharding() is an experimental API that allows users to register sharding
strategies for an operator when the tensor inputs and outputs are DTensor.
It can be useful when: (1) there doesn’t exist a default sharding strategy for ,
e.g. when  is a custom operator that is not supported by ; (2)
when users would like to overwrite default sharding strategies of existing operators.

Parameters
 (OpOverloadOpOverload) – An op or a list of ops to register the customized sharding function.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 18)


A function decorator which can be used to wrap a function that defines the sharding
strategy for the operator specified in . The defined sharding strategy will be
registered to DTensor and will override the default sharding strategy if DTensor has
already implemented the operator. The customized sharding function takes the same inputs
as the original op (except that if an arg is a torch.Tensor, it will be
replaced by a tensor-like object that DTensor uses internally). The function should
return a sequence of 2-tuples, each specifying acceptable output placements and its
corresponding intput placements.



@register_sharding
 custom_softmax_sharding  half_to_float
    softmax_dim          
    acceptable_shardings  

    all_replicate     
    acceptable_shardingsall_replicate

     sharding_dim  
         sharding_dim  softmax_dim
            all_sharded  
                sharding_dim
                sharding_dim  
            
            acceptable_shardingsall_sharded

     acceptable_shardings




This API is currently experimental and subject to change

================================================================================

# torch.distributed.tensor - Experimental Features (Part 19)

register_sharding() is an experimental API that allows users to register sharding
strategies for an operator when the tensor inputs and outputs are DTensor.
It can be useful when: (1) there doesn’t exist a default sharding strategy for ,
e.g. when  is a custom operator that is not supported by ; (2)
when users would like to overwrite default sharding strategies of existing operators.

Parameters
 (OpOverloadOpOverload) – An op or a list of ops to register the customized sharding function.


A function decorator which can be used to wrap a function that defines the sharding
strategy for the operator specified in . The defined sharding strategy will be
registered to DTensor and will override the default sharding strategy if DTensor has
already implemented the operator. The customized sharding function takes the same inputs
as the original op (except that if an arg is a torch.Tensor, it will be
replaced by a tensor-like object that DTensor uses internally). The function should
return a sequence of 2-tuples, each specifying acceptable output placements and its
corresponding intput placements.

(OpOverloadOpOverload) – An op or a list of ops to register the customized sharding function.

================================================================================

# torch.distributed.tensor - Experimental Features (Part 20)

A function decorator which can be used to wrap a function that defines the sharding
strategy for the operator specified in . The defined sharding strategy will be
registered to DTensor and will override the default sharding strategy if DTensor has
already implemented the operator. The customized sharding function takes the same inputs
as the original op (except that if an arg is a torch.Tensor, it will be
replaced by a tensor-like object that DTensor uses internally). The function should
return a sequence of 2-tuples, each specifying acceptable output placements and its
corresponding intput placements.

Code example:
@register_sharding
 custom_softmax_sharding  half_to_float
    softmax_dim          
    acceptable_shardings  

    all_replicate     
    acceptable_shardingsall_replicate

     sharding_dim  
         sharding_dim  softmax_dim
            all_sharded  
                sharding_dim
                sharding_dim  
            
            acceptable_shardingsall_sharded

     acceptable_shardings

This API is currently experimental and subject to change

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Generic Join Context Manager (Part 1)

Created On: Jun 06, 2025 | Last Updated On: Jun 06, 2025

The generic join context manager facilitates distributed training on uneven
inputs. This page outlines the API of the relevant classes: ,
, and . For a tutorial, see
Distributed Training with Uneven Inputs Using the Join Context Manager.

torch.distributed.algorithms., , throw_on_early_termination, 
This class defines the generic join context manager, which allows custom hooks to be called after a process joins.
These hooks should shadow the
collective communications of non-joined processes to prevent hanging and
erroring and to ensure algorithmic correctness. Refer to 
for details about the hook definition.


The context manager requires each participating  to
call the method notify_join_context() before its own per-
iteration collective communications to ensure correctness.



================================================================================

# Generic Join Context Manager (Part 2)

The context manager requires that all process_group attributes in
the  objects are the same. If there are multiple
 objects, then the  of the first is used.
The process group and device information is used for checking for non-
joined processes and for notifying processes to throw an exception if
throw_on_early_termination is enabled, both of which using an all-
reduce.


Parameters

 () – a list of the participating
 s; their hooks are iterated over in the given
order.
 () – a flag enabling uneven input detection; setting to
 disables the context manager’s functionality and should
only be set when the user knows the inputs will not be uneven
(default: ).
throw_on_early_termination () – a flag controlling whether to throw an
exception upon detecting uneven inputs (default: ).




 
 
 torch.distributed  
 torch.multiprocessing  
 torch.nn.parallel.DistributedDataParallel  
 torch.distributed.optim.ZeroRedundancyOptimizer  
 torch.distributed.algorithms.join  

================================================================================

# Generic Join Context Manager (Part 3)

# On each spawned worker
 
    init_process_group  world_size
        device_ids
      parameters  
    # Rank 1 gets one more input than rank 0
            
      
           
              
            
            
    # All ranks reach here without hanging/erroring




notify_join_context
Notifies the join context manager that the calling process has not yet joined.
Then, if throw_on_early_termination=True, checks if uneven inputs have been detected
(i.e. if one process has already joined) and throws an exception if so.
This method should be called from a  object before
its per-iteration collective communications. For example, this should
be called at the beginning of the forward pass in
DistributedDataParallel.
Only the first  object passed into the context
manager performs the collective communications in this method, and
for the others, this method is vacuous.

Parameters
 () – the  object calling this
method.


An async work handle for the all-reduce meant to notify the context
manager that the process has not yet joined if  is the
first one passed into the context manager;  otherwise.

================================================================================

# Generic Join Context Manager (Part 4)

This class defines the generic join context manager, which allows custom hooks to be called after a process joins.

These hooks should shadow the
collective communications of non-joined processes to prevent hanging and
erroring and to ensure algorithmic correctness. Refer to 
for details about the hook definition.

The context manager requires each participating  to
call the method notify_join_context() before its own per-
iteration collective communications to ensure correctness.

The context manager requires that all process_group attributes in
the  objects are the same. If there are multiple
 objects, then the  of the first is used.
The process group and device information is used for checking for non-
joined processes and for notifying processes to throw an exception if
throw_on_early_termination is enabled, both of which using an all-
reduce.

Parameters

================================================================================

# Generic Join Context Manager (Part 5)

 () – a list of the participating
 s; their hooks are iterated over in the given
order.
 () – a flag enabling uneven input detection; setting to
 disables the context manager’s functionality and should
only be set when the user knows the inputs will not be uneven
(default: ).
throw_on_early_termination () – a flag controlling whether to throw an
exception upon detecting uneven inputs (default: ).

List:
() – a list of the participating
 s; their hooks are iterated over in the given
order.
 () – a flag enabling uneven input detection; setting to
 disables the context manager’s functionality and should
only be set when the user knows the inputs will not be uneven
(default: ).
throw_on_early_termination () – a flag controlling whether to throw an
exception upon detecting uneven inputs (default: ).

() – a list of the participating
 s; their hooks are iterated over in the given
order.

() – a flag enabling uneven input detection; setting to
 disables the context manager’s functionality and should
only be set when the user knows the inputs will not be uneven
(default: ).

throw_on_early_termination () – a flag controlling whether to throw an
exception upon detecting uneven inputs (default: ).

================================================================================

# Generic Join Context Manager (Part 6)

Code example:
torch.distributed  
 torch.multiprocessing  
 torch.nn.parallel.DistributedDataParallel  
 torch.distributed.optim.ZeroRedundancyOptimizer  
 torch.distributed.algorithms.join  

# On each spawned worker
 
    init_process_group  world_size
        device_ids
      parameters  
    # Rank 1 gets one more input than rank 0
            
      
           
              
            
            
    # All ranks reach here without hanging/erroring

notify_join_context
Notifies the join context manager that the calling process has not yet joined.
Then, if throw_on_early_termination=True, checks if uneven inputs have been detected
(i.e. if one process has already joined) and throws an exception if so.
This method should be called from a  object before
its per-iteration collective communications. For example, this should
be called at the beginning of the forward pass in
DistributedDataParallel.
Only the first  object passed into the context
manager performs the collective communications in this method, and
for the others, this method is vacuous.

Parameters
 () – the  object calling this
method.

================================================================================

# Generic Join Context Manager (Part 7)


An async work handle for the all-reduce meant to notify the context
manager that the process has not yet joined if  is the
first one passed into the context manager;  otherwise.

Notifies the join context manager that the calling process has not yet joined.

Then, if throw_on_early_termination=True, checks if uneven inputs have been detected
(i.e. if one process has already joined) and throws an exception if so.

This method should be called from a  object before
its per-iteration collective communications. For example, this should
be called at the beginning of the forward pass in
DistributedDataParallel.

Only the first  object passed into the context
manager performs the collective communications in this method, and
for the others, this method is vacuous.

Parameters
 () – the  object calling this
method.


An async work handle for the all-reduce meant to notify the context
manager that the process has not yet joined if  is the
first one passed into the context manager;  otherwise.

() – the  object calling this
method.

================================================================================

# Generic Join Context Manager (Part 8)

An async work handle for the all-reduce meant to notify the context
manager that the process has not yet joined if  is the
first one passed into the context manager;  otherwise.

torch.distributed.algorithms.
This defines an abstract base class for joinable classes.
A joinable class
(inheriting from ) should implement join_hook(),
which returns a  instance, in addition to
join_device() and join_process_group() that return device and
process group information, respectively.



join_device
Return the device from which to perform collective communications needed by the join context manager.




Return a  instance for the given .

Parameters
 () – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .

Return type






join_process_group
Returns the process group for the collective communications needed by the join context manager itself.

This defines an abstract base class for joinable classes.

================================================================================

# Generic Join Context Manager (Part 9)

A joinable class
(inheriting from ) should implement join_hook(),
which returns a  instance, in addition to
join_device() and join_process_group() that return device and
process group information, respectively.

join_device
Return the device from which to perform collective communications needed by the join context manager.

Return the device from which to perform collective communications needed by the join context manager.

Return a  instance for the given .

Parameters
 () – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .

Return type

Return a  instance for the given .

Parameters
 () – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .

Return type

() – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .

================================================================================

# Generic Join Context Manager (Part 10)

join_process_group
Returns the process group for the collective communications needed by the join context manager itself.

Returns the process group for the collective communications needed by the join context manager itself.

torch.distributed.algorithms.
This defines a join hook, which provides two entry points in the join context manager.
Entry points : a main hook, which is called repeatedly while there exists a non-joined
process, and a post-hook, which is called once all processes have joined.
To implement a join hook for the generic join context manager, define a
class that inherits from  and override main_hook() and
post_hook() as appropriate.



Call this hook while there exists a non-joined process to shadow collective communications in a training iteration.
Training iteration i.e., in one forward pass, backward pass, and optimizer step.




is_last_joiner
Call hook after all processes have joined.
It is passed an additional  argument is_last_joiner, which indicates if the rank is one of the last to join.

Parameters
is_last_joiner () –  if the rank is one of the last to
join;  otherwise.

This defines a join hook, which provides two entry points in the join context manager.

================================================================================

# Generic Join Context Manager (Part 11)

Entry points : a main hook, which is called repeatedly while there exists a non-joined
process, and a post-hook, which is called once all processes have joined.

To implement a join hook for the generic join context manager, define a
class that inherits from  and override main_hook() and
post_hook() as appropriate.

Call this hook while there exists a non-joined process to shadow collective communications in a training iteration.
Training iteration i.e., in one forward pass, backward pass, and optimizer step.

Call this hook while there exists a non-joined process to shadow collective communications in a training iteration.

Training iteration i.e., in one forward pass, backward pass, and optimizer step.

is_last_joiner
Call hook after all processes have joined.
It is passed an additional  argument is_last_joiner, which indicates if the rank is one of the last to join.

Parameters
is_last_joiner () –  if the rank is one of the last to
join;  otherwise.

Call hook after all processes have joined.

It is passed an additional  argument is_last_joiner, which indicates if the rank is one of the last to join.

Parameters
is_last_joiner () –  if the rank is one of the last to
join;  otherwise.

================================================================================

# Generic Join Context Manager (Part 12)

is_last_joiner () –  if the rank is one of the last to
join;  otherwise.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Torch Distributed Elastic

Created On: Jun 16, 2025 | Last Updated On: Jun 16, 2025

Makes distributed PyTorch fault-tolerant and elastic.

================================================================================

# Torch Distributed Elastic - Documentation

List:
torchrun (Elastic Launch)
Elastic Agent
Multiprocessing
Error Propagation
Rendezvous
Expiration Timers


Subprocess Handling
Control Plane

List:
TorchElastic Kubernetes

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# FullyShardedDataParallel (Part 1)

Created On: Feb 02, 2022 | Last Updated On: Jun 11, 2025

torch.distributed.fsdp.FullyShardedDataParallel, process_group, sharding_strategy, cpu_offload, auto_wrap_policy, backward_prefetchBackwardPrefetch.BACKWARD_PRE, mixed_precision, ignored_modules, param_init_fn, , sync_module_states, forward_prefetch, limit_all_gathers, use_orig_params, ignored_states, device_mesh
A wrapper for sharding module parameters across data parallel workers.
This is inspired by  as
well as the ZeRO Stage 3 from .
FullyShardedDataParallel is commonly shortened to FSDP.
To understand FSDP internals, refer to the
FSDP Notes.

 
 torch.distributed.fsdp  FullyShardedDataParallel  
set_device
sharded_module  
  sharded_moduleparameters 
  sharded_module  
  




Using FSDP involves wrapping your module and then initializing your
optimizer after. This is required since FSDP changes the parameter
variables.
When setting up FSDP, you need to consider the destination CUDA
device. If the device has an ID (), you have three options:

Place the module on that device
Set the device using torch.cuda.set_device(dev_id)
Pass  into the  constructor argument.

================================================================================

# FullyShardedDataParallel (Part 2)

This ensures that the FSDP instance’s compute device is the
destination device. For option 1 and 3, the FSDP initialization
always occurs on GPU. For option 2, the FSDP initialization
happens on module’s current device, which may be a CPU.
If you’re using the sync_module_states=True flag, you need to
ensure that the module is on a GPU or use the 
argument to specify a CUDA device that FSDP will move the module
to in the FSDP constructor. This is necessary because
sync_module_states=True requires GPU communication.
FSDP also takes care of moving input tensors to the forward method
to the GPU compute device, so you don’t need to manually move them
from CPU.
For use_orig_params=True,
ShardingStrategy.SHARD_GRAD_OP exposes the unsharded
parameters, not the sharded parameters after forward, unlike
ShardingStrategy.FULL_SHARD. If you want
to inspect the gradients, you can use the summon_full_params
method with with_grads=True.
With limit_all_gathers=True, you may see a gap in the FSDP
pre-forward where the CPU thread is not issuing any kernels. This is
intentional and shows the rate limiter in effect. Synchronizing the CPU
thread in that way prevents over-allocating memory for subsequent
all-gathers, and it should not actually delay GPU kernel execution.
FSDP replaces managed modules’ parameters with torch.Tensor
views during forward and backward computation for autograd-related
reasons. If your module’s forward relies on saved references to
the parameters instead of reacquiring the references each
iteration, then it will not see FSDP’s newly created views,
and autograd will not work correctly.
Finally, when using sharding_strategy=ShardingStrategy.HYBRID_SHARD
with the sharding process group being intra-node and the
replication process group being inter-node, setting
NCCL_CROSS_NIC=1 can help improve the all-reduce times over
the replication process group for some cluster setups.
Limitations
There are several limitations to be aware of when using FSDP:

================================================================================

# FullyShardedDataParallel (Part 3)

FSDP currently does not support gradient accumulation outside
 when using CPU offloading. This is because FSDP
uses the newly-reduced gradient instead of accumulating with any
existing gradient, which can lead to incorrect results.
FSDP does not support running the forward pass of a submodule
that is contained in an FSDP instance. This is because the
submodule’s parameters will be sharded, but the submodule itself
is not an FSDP instance, so its forward pass will not all-gather
the full parameters appropriately.
FSDP does not work with double backwards due to the way it
registers backward hooks.
FSDP has some constraints when freezing parameters.
For use_orig_params=False, each FSDP instance must manage
parameters that are all frozen or all non-frozen. For
use_orig_params=True, FSDP supports mixing frozen and
non-frozen parameters, but it’s recommended to avoid doing so to
prevent higher than expected gradient memory usage.
As of PyTorch 1.12, FSDP offers limited support for shared
parameters. If enhanced shared parameter support is needed for
your use case, please post in
this issue.
You should avoid modifying the parameters between forward and
backward without using the summon_full_params context, as
the modifications may not persist.

================================================================================

# FullyShardedDataParallel (Part 4)


Parameters

================================================================================

# FullyShardedDataParallel (Part 5)

 () – This is the module to be wrapped with FSDP.
process_group (ProcessGroupProcessGroupProcessGroup) – This is the process group over which the model is sharded and thus
the one used for FSDP’s all-gather and reduce-scatter collective
communications. If , then FSDP uses the default process
group. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a tuple of
process groups, representing the groups over which to shard and
replicate, respectively. If , then FSDP constructs process
groups for the user to shard intra-node and replicate inter-node.
(Default: )
sharding_strategy (ShardingStrategy) – This configures the sharding strategy, which may trade off memory
saving and communication overhead. See ShardingStrategy
for details. (Default: FULL_SHARD)
cpu_offload (CPUOffload) – This configures CPU offloading. If this is set to , then
no CPU offloading happens. See CPUOffload for details.
(Default: )
auto_wrap_policy (ModuleWrapPolicyCustomPolicy) – This specifies a policy to apply FSDP to submodules of ,
which is needed for communication and computation overlap and thus
affects performance. If , then FSDP only applies to
, and users should manually apply FSDP to parent modules
themselves (proceeding bottom-up). For convenience, this accepts
ModuleWrapPolicy directly, which allows users to specify the
module classes to wrap (e.g. the transformer block). Otherwise,
this should be a callable that takes in three arguments
 ,  , and
nonwrapped_numel:  and should return a  specifying
whether the passed-in  should have FSDP applied if
recurse=False or if the traversal should continue into the
module’s subtree if recurse=True. Users may add additional
arguments to the callable. The size_based_auto_wrap_policy in
torch.distributed.fsdp.wrap.py gives an example callable that
applies FSDP to a module if the parameters in its subtree exceed
100M numel. We recommend printing the model after applying FSDP
and adjusting as needed.

================================================================================

# FullyShardedDataParallel (Part 6)

 custom_auto_wrap_policy
     
     
    nonwrapped_numel 
    # Additional custom arguments
    min_num_params   
  
     nonwrapped_numel  min_num_params
# Configure a custom `min_num_params`
my_auto_wrap_policy  custom_auto_wrap_policy min_num_params



================================================================================

# FullyShardedDataParallel (Part 7)

backward_prefetch (BackwardPrefetch) – This configures explicit backward prefetching of all-gathers. If
, then FSDP does not backward prefetch, and there is no
communication and computation overlap in the backward pass. See
BackwardPrefetch for details. (Default: BACKWARD_PRE)
mixed_precision (MixedPrecision) – This configures native mixed precision for FSDP. If this is set to
, then no mixed precision is used. Otherwise, parameter,
buffer, and gradient reduction dtypes can be set. See
MixedPrecision for details. (Default: )
ignored_modules (torch.nn.Module) – Modules whose
own parameters and child modules’ parameters and buffers are
ignored by this instance. None of the modules directly in
ignored_modules should be FullyShardedDataParallel
instances, and any child modules that are already-constructed
FullyShardedDataParallel instances will not be ignored if
they are nested under this instance. This argument may be used to
avoid sharding specific parameters at module granularity when using an
auto_wrap_policy or if parameters’ sharding is not managed by
FSDP. (Default: )
param_init_fn () – A Callable[torch.nn.Module]   that
specifies how modules that are currently on the meta device should
be initialized onto an actual device. As of v1.12, FSDP detects
modules with parameters or buffers on meta device via 
and either applies param_init_fn if specified or calls
nn.Module.reset_parameters() otherwise. For both cases, the
implementation should  initialize the parameters/buffers of
the module, not those of its submodules. This is to avoid
re-initialization. In addition, FSDP also supports deferred
initialization via torchdistX’s (pytorch/torchdistX)
deferred_init() API, where the deferred modules are initialized
by calling param_init_fn if specified or torchdistX’s default
materialize_module() otherwise. If param_init_fn is
specified, then it is applied to all meta-device modules, meaning
that it should probably case on the module type. FSDP calls the
initialization function before parameter flattening and sharding.

================================================================================

# FullyShardedDataParallel (Part 8)

  
 my_init_fn 
    # E.g. initialize depending on the module type
    
fsdp_model   param_init_fnmy_init_fn auto_wrap_policysize_based_auto_wrap_policy
fsdp_modelparameters # current CUDA device
# With torchdistX
  deferred_initdeferred_init 
# Will initialize via deferred_init.materialize_module().
fsdp_model   auto_wrap_policysize_based_auto_wrap_policy



================================================================================

# FullyShardedDataParallel (Part 9)

 (torch.device) – An  or
torch.device giving the CUDA device on which FSDP
initialization takes place, including the module initialization
if needed and the parameter sharding. This should be specified to
improve initialization speed if  is on CPU. If the
default CUDA device was set (e.g. via torch.cuda.set_device),
then the user may pass torch.cuda.current_device to this.
(Default: )
sync_module_states () – If , then each FSDP module will
broadcast module parameters and buffers from rank 0 to ensure that
they are replicated across ranks (adding communication overhead to
this constructor). This can help load state_dict checkpoints
via load_state_dict in a memory efficient way. See
FullStateDictConfig for an example of this. (Default:
)
forward_prefetch () – If , then FSDP explicitly prefetches
the next forward-pass all-gather before the current forward
computation. This is only useful for CPU-bound workloads, in which
case issuing the next all-gather earlier may improve overlap. This
should only be used for static-graph models since the prefetching
follows the first iteration’s execution order. (Default: )
limit_all_gathers () – If , then FSDP explicitly
synchronizes the CPU thread to ensure GPU memory usage from only
 consecutive FSDP instances (the current instance running
computation and the next instance whose all-gather is prefetched).
If , then FSDP allows the CPU thread to issue all-gathers
without any extra synchronization. (Default: ) We often
refer to this feature as the “rate limiter”. This flag should only
be set to  for specific CPU-bound workloads with low
memory pressure in which case the CPU thread can aggressively issue
all kernels without concern for the GPU memory usage.
use_orig_params () – Setting this to  has FSDP use
 ‘s original parameters. FSDP exposes those original
parameters to the user via nn.Module.named_parameters()
instead of FSDP’s internal FlatParameter s. This means
that the optimizer step runs on the original parameters, enabling
per-original-parameter hyperparameters. FSDP preserves the original
parameter variables and manipulates their data between unsharded
and sharded forms, where they are always views into the underlying
unsharded or sharded FlatParameter, respectively. With the
current algorithm, the sharded form is always 1D, losing the
original tensor structure. An original parameter may have all,
some, or none of its data present for a given rank. In the none
case, its data will be like a size-0 empty tensor. Users should not
author programs relying on what data is present for a given
original parameter in its sharded form.  is required to
use torch.compile(). Setting this to  exposes FSDP’s
internal FlatParameter s to the user via
nn.Module.named_parameters(). (Default: )
ignored_states (torch.nn.Parametertorch.nn.Module) – Ignored parameters or modules that will not be managed by this FSDP
instance, meaning that the parameters are not sharded and their
gradients are not reduced across ranks. This argument unifies with
the existing ignored_modules argument, and we may deprecate
ignored_modules soon. For backward compatibility, we keep both
ignored_states and ignored_modules`, but FSDP only allows one
of them to be specified as not .
device_mesh (DeviceMesh) – DeviceMesh can be used as an alternative to
process_group. When device_mesh is passed, FSDP will use the underlying process
groups for all-gather and reduce-scatter collective communications. Therefore,
these two args need to be mutually exclusive. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a 2D DeviceMesh instead
of a tuple of process groups. For 2D FSDP + TP, users are required to pass in
device_mesh instead of process_group. For more DeviceMesh info, please visit:
https://pytorch.org/tutorials/recipes/distributed_device_mesh.html

================================================================================

# FullyShardedDataParallel (Part 10)






Apply  recursively to every submodule (as returned by .children()) as well as self.
Typical use includes initializing the parameters of a model (see also torch.nn.init).
Compared to torch.nn.Module.apply, this version additionally gathers
the full parameters before applying . It should not be called from
within another summon_full_params context.

Parameters
 ( -> None) – function to be applied to each submodule




Return type






check_is_root
Check if this instance is a root FSDP module.

Return type






clip_grad_norm_, 
Clip the gradient norm of all parameters.
The norm is computed over all parameters’ gradients as viewed as a single vector, and the
gradients are modified in-place.

Parameters

 () – max norm of the gradients
 () – type of the used p-norm. Can be 
for infinity norm.



Total norm of the parameters (viewed as a single vector).

Return type



================================================================================

# FullyShardedDataParallel (Part 11)

If every FSDP instance uses , meaning that no
gradients are sharded across ranks, then you may directly use
torch.nn.utils.clip_grad_norm_().
If at least some FSDP instance uses a sharded strategy (i.e.
one other than ), then you should use this method
instead of torch.nn.utils.clip_grad_norm_() since this method
handles the fact that gradients are sharded across ranks.
The total norm returned will have the “largest” dtype across
all parameters/gradients as defined by PyTorch’s type promotion
semantics. For example, if  parameters/gradients use a low
precision dtype, then the returned norm’s dtype will be that low
precision dtype, but if there exists at least one parameter/
gradient using FP32, then the returned norm’s dtype will be FP32.


This needs to be called on all ranks since it uses
collective communications.




flatten_sharded_optim_state_dictsharded_optim_state_dict, , 
Flatten a sharded optimizer state-dict.
The API is similar to shard_full_optim_state_dict(). The only
difference is that the input sharded_optim_state_dict should be
returned from sharded_optim_state_dict(). Therefore, there will
be all-gather calls on each rank to gather ShardedTensor s.

Parameters

================================================================================

# FullyShardedDataParallel (Part 12)

sharded_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
sharded optimizer state.
 (torch.nn.Module) – Refer to shard_full_optim_state_dict().
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.



Refer to shard_full_optim_state_dict().

Return type
[, ]





, 
Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.

Return type






fsdp_modules, 
Return all nested FSDP instances.
This possibly includes  itself and only includes FSDP root modules if root_only=True.

Parameters

 (torch.nn.Module) – Root module, which may or may not be an
 module.
 () – Whether to return only FSDP root modules.
(Default: )



FSDP modules that are nested in
the input .

Return type
List[FullyShardedDataParallel]





================================================================================

# FullyShardedDataParallel (Part 13)

full_optim_state_dict, , optim_input, rank0_only, 
Return the full optimizer state-dict.
Consolidates the full optimizer state on rank 0 and returns it
as a  following the convention of
torch.optim.Optimizer.state_dict(), i.e. with keys 
and "param_groups". The flattened parameters in  modules
contained in  are mapped back to their unflattened parameters.
This needs to be called on all ranks since it uses
collective communications. However, if rank0_only=True, then
the state dict is only populated on rank 0, and all other ranks
return an empty .
Unlike torch.optim.Optimizer.state_dict(), this method
uses full parameter names as keys instead of parameter IDs.
Like in torch.optim.Optimizer.state_dict(), the tensors
contained in the optimizer state dict are not cloned, so there may
be aliasing surprises. For best practices, consider saving the
returned optimizer state dict immediately, e.g. using
torch.save().

Parameters

================================================================================

# FullyShardedDataParallel (Part 14)

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_input (torch.nn.Parameter) – Input passed into the optimizer  representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
rank0_only () – If , saves the populated 
only on rank 0; if , saves it on all ranks. (Default:
)
 (dist.ProcessGroup) – Model’s process group or  if using
the default process group. (Default: )



A  containing the optimizer state for
 ‘s original unflattened parameters and including keys
“state” and “param_groups” following the convention of
torch.optim.Optimizer.state_dict(). If rank0_only=True,
then nonzero ranks return an empty .

Return type
Dict[, Any]





get_state_dict_type
Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at .
The target module does not have to be an FSDP module.

================================================================================

# FullyShardedDataParallel (Part 15)


A StateDictSettings containing the state_dict_type and
state_dict / optim_state_dict configs that are currently set.



AssertionError` if the StateDictSettings for differen – 
FSDP submodules differ. – 


Return type
StateDictSettings






Return the wrapped module.



named_buffers, 
Return an iterator over module buffers, yielding both the name of the buffer and the buffer itself.
Intercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix
when inside the summon_full_params() context manager.

Return type
[[, torch.Tensor]]





named_parameters, 
Return an iterator over module parameters, yielding both the name of the parameter and the parameter itself.
Intercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix
when inside the summon_full_params() context manager.

Return type
[[, torch.nn.parameter.Parameter]]





================================================================================

# FullyShardedDataParallel (Part 16)


Disable gradient synchronizations across FSDP instances.
Within this context, gradients will be accumulated in module
variables, which will later be synchronized in the first
forward-backward pass after exiting the context. This should only be
used on the root FSDP instance and will recursively apply to all
children FSDP instances.


This likely results in higher memory usage because FSDP will
accumulate the full model gradients (instead of gradient shards)
until the eventual sync.



When used with CPU offloading, the gradients will not be
offloaded to CPU when inside the context manager. Instead, they
will only be offloaded right after the eventual sync.


Return type





================================================================================

# FullyShardedDataParallel (Part 17)


optim_state_dict, , optim_state_dict, 
Transform the state-dict of an optimizer corresponding to a sharded model.
The given state-dict can be transformed to one of three types:
1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.
For full optimizer state_dict, all states are unflattened and not sharded.
Rank0 only and CPU only can be specified via state_dict_type() to
avoid OOM.
For sharded optimizer state_dict, all states are unflattened but sharded.
CPU only can be specified via state_dict_type() to further save
memory.
For local state_dict, no transformation will be performed. But a state
will be converted from nn.Tensor to ShardedTensor to represent its sharding
nature (this is not supported yet).

 torch.distributed.fsdp  FullyShardedDataParallel  
 torch.distributed.fsdp  StateDictType
 torch.distributed.fsdp  FullStateDictConfig
 torch.distributed.fsdp  FullOptimStateDictConfig
# Save a checkpoint
   
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

================================================================================

# FullyShardedDataParallel (Part 18)

state_dict  state_dict
optim_state_dict  optim_state_dict 
save_a_checkpointstate_dict optim_state_dict
# Load a checkpoint
   
state_dict optim_state_dict  load_a_checkpoint
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

load_state_dictstate_dict
optim_state_dict  optim_state_dict_to_load
      optim_state_dict

load_state_dictoptim_state_dict



Parameters

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – the target optimizer state_dict to
transform. If the value is None, optim.state_dict() will be used. (
Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )



A  containing the optimizer state for
. The sharding of the optimizer state is based on
state_dict_type.

Return type
Dict[, Any]





================================================================================

# FullyShardedDataParallel (Part 19)

optim_state_dict_to_load, , optim_state_dict, is_named_optimizer, load_directly, 
Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.
Given a optim_state_dict that is transformed through
optim_state_dict(), it gets converted to the flattened optimizer
state_dict that can be loaded to  which is the optimizer for
.  must be sharded by FullyShardedDataParallel.
 torch.distributed.fsdp  FullyShardedDataParallel  
 torch.distributed.fsdp  StateDictType
 torch.distributed.fsdp  FullStateDictConfig
 torch.distributed.fsdp  FullOptimStateDictConfig
# Save a checkpoint
   
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

state_dict  state_dict
original_osd  state_dict
optim_state_dict  optim_state_dict
    
    
    optim_state_dictoriginal_osd

save_a_checkpointstate_dict optim_state_dict
# Load a checkpoint
   
state_dict optim_state_dict  load_a_checkpoint
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

================================================================================

# FullyShardedDataParallel (Part 20)

load_state_dictstate_dict
optim_state_dict  optim_state_dict_to_load
      optim_state_dict

load_state_dictoptim_state_dict



Parameters

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – The optimizer states to be loaded.
is_named_optimizer () – Is this optimizer a NamedOptimizer or
KeyedOptimizer. Only set to True if  is TorchRec’s
KeyedOptimizer or torch.distributed’s NamedOptimizer.
load_directly () – If this is set to True, this API will also
call optim.load_state_dict(result) before returning the result.
Otherwise, users are responsible to call optim.load_state_dict()
(Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )


Return type
[, ]





================================================================================

# FullyShardedDataParallel (Part 21)

register_comm_hook, 
Register a communication hook.
This is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates
gradients across multiple workers.
This hook can be used to implement several algorithms like
GossipGrad and gradient compression
which involve different communication strategies for
parameter syncs while training with FullyShardedDataParallel.


FSDP communication hook should be registered before running an initial forward pass
and only once.


Parameters

 () – Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in GossipGrad, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.

================================================================================

# FullyShardedDataParallel (Part 22)

 () – Callable, which has one of the following signatures:
1)  Callable[torch.Tensor]  :
This function takes in a Python tensor, which represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units).
It then performs all necessary processing and returns ;
2)  Callable[torch.Tensor, torch.Tensor]  :
This function takes in two Python tensors, the first one represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units). The latter
represents a pre-sized tensor to store a chunk of a sharded gradient after
reduction.
In both cases, callable performs all necessary processing and returns .
Callables with signature 1 are expected to handle gradient communication for a  case.
Callables with signature 2 are expected to handle gradient communication for sharded cases.





================================================================================

# FullyShardedDataParallel (Part 23)


rekey_optim_state_dictoptim_state_dict, optim_state_key_type, , optim_input, 
Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type.
This can be used to achieve compatibility between optimizer state dicts from models with FSDP
instances and ones without.
To re-key an FSDP full optimizer state dict (i.e. from
full_optim_state_dict()) to use parameter IDs and be loadable to
a non-wrapped model:
wrapped_model wrapped_optim  
  full_optim_state_dictwrapped_model wrapped_optim
nonwrapped_model nonwrapped_optim  
rekeyed_osd  rekey_optim_state_dict OptimStateKeyType nonwrapped_model
nonwrapped_optimload_state_dictrekeyed_osd


To re-key a normal optimizer state dict from a non-wrapped model to be
loadable to a wrapped model:
nonwrapped_model nonwrapped_optim  
  nonwrapped_optimstate_dict
rekeyed_osd  rekey_optim_state_dict OptimStateKeyTypePARAM_NAME nonwrapped_model
wrapped_model wrapped_optim  
sharded_osd  shard_full_optim_state_dictrekeyed_osd wrapped_model
wrapped_optimload_state_dictsharded_osd




The optimizer state dict re-keyed using the
parameter keys specified by optim_state_key_type.

Return type
Dict[, Any]





================================================================================

# FullyShardedDataParallel (Part 24)

scatter_full_optim_state_dictfull_optim_state_dict, , optim_input, , 
Scatter the full optimizer state dict from rank 0 to all other ranks.
Returns the sharded optimizer state dict on each rank.
The return value is the same as shard_full_optim_state_dict(), and on rank
0, the first argument should be the return value of
full_optim_state_dict().

 torch.distributed.fsdp  FullyShardedDataParallel  
   
  full_optim_state_dict   # only non-empty on rank 0
# Define new model with possibly different world size
    
sharded_osd  scatter_full_optim_state_dict  
load_state_dictsharded_osd



================================================================================

# FullyShardedDataParallel (Part 25)


Both shard_full_optim_state_dict() and
scatter_full_optim_state_dict() may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.


Parameters

================================================================================

# FullyShardedDataParallel (Part 26)

full_optim_state_dict () – Optimizer state
dict corresponding to the unflattened parameters and holding
the full non-sharded optimizer state if on rank 0; the argument
is ignored on nonzero ranks.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )
 (dist.ProcessGroup) – Model’s process group or  if
using the default process group. (Default: )



The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

Return type
Dict[, Any]





================================================================================

# FullyShardedDataParallel (Part 27)

set_state_dict_type, state_dict_type, state_dict_config, optim_state_dict_config
Set the state_dict_type of all the descendant FSDP modules of the target module.
Also takes (optional) configuration for the model’s and optimizer’s state dict.
The target module does not have to be a FSDP module. If the target
module is a FSDP module, its state_dict_type will also be changed.


This API should be called for only the top-level (root)
module.



This API enables users to transparently use the conventional
state_dict API to take model checkpoints in cases where the
root FSDP module is wrapped by another . For example,
the following will ensure state_dict is called on all non-FSDP
instances, while dispatching into sharded_state_dict implementation
for FSDP:


  
set_state_dict_type
    
    StateDictTypeSHARDED_STATE_DICT
    state_dict_config  ShardedStateDictConfigoffload_to_cpu
    optim_state_dict_config  OptimStateDictConfigoffload_to_cpu

param_state_dict  state_dict
optim_state_dict  optim_state_dict 



Parameters

================================================================================

# FullyShardedDataParallel (Part 28)

 (torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the configuration for the
target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the configuration
for the optimizer state dict.



A StateDictSettings that include the previous state_dict type and
configuration for the module.

Return type
StateDictSettings





shard_full_optim_state_dictfull_optim_state_dict, , optim_input, 
Shard a full optimizer state-dict.
Remaps the state in full_optim_state_dict to flattened parameters instead of unflattened
parameters and restricts to only this rank’s part of the optimizer state.
The first argument should be the return value of full_optim_state_dict().

 torch.distributed.fsdp  FullyShardedDataParallel  
   
  full_optim_state_dict 
 
# Define new model with possibly different world size
   
  
sharded_osd  shard_full_optim_state_dict 
load_state_dictsharded_osd



================================================================================

# FullyShardedDataParallel (Part 29)


Both shard_full_optim_state_dict() and
scatter_full_optim_state_dict() may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.


Parameters

================================================================================

# FullyShardedDataParallel (Part 30)

full_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
full non-sharded optimizer state.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )



The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

Return type
Dict[, Any]





================================================================================

# FullyShardedDataParallel (Part 31)

sharded_optim_state_dict, , 
Return the optimizer state-dict in its sharded form.
The API is similar to full_optim_state_dict() but this API chunks
all non-zero-dimension states to ShardedTensor to save memory.
This API should only be used when the model state_dict is derived
with the context manager  state_dict_type(SHARDED_STATE_DICT):.
For the detailed usage, refer to full_optim_state_dict().


The returned state dict contains ShardedTensor and
cannot be directly used by the regular optim.load_state_dict.


Return type
[, ]





state_dict_type, state_dict_type, state_dict_config, optim_state_dict_config
Set the state_dict_type of all the descendant FSDP modules of the target module.
This context manager has the same functions as set_state_dict_type(). Read the document of
set_state_dict_type() for the detail.

  
 state_dict_type
    
    StateDictTypeSHARDED_STATE_DICT

    checkpoint  state_dict



Parameters

================================================================================

# FullyShardedDataParallel (Part 32)

 (torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the model state_dict
configuration for the target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the optimizer
state_dict configuration for the target state_dict_type.


Return type






summon_full_params, , , rank0_only, offload_to_cpu, with_grads
Expose full params for FSDP instances with this context manager.
Can be useful  forward/backward for a model to get
the params for additional processing or checking. It can take a non-FSDP
module and will summon full params for all contained FSDP modules as
well as their children, depending on the  argument.


This can be used on inner FSDPs.



This can  be used within a forward or backward pass. Nor
can forward and backward be started from within this context.



Parameters will revert to their local shards after the context
manager exits, storage behavior is the same as forward.



================================================================================

# FullyShardedDataParallel (Part 33)

The full parameters can be modified, but only the portion
corresponding to the local param shard will persist after the
context manager exits (unless writeback=False, in which case
changes will be discarded). In the case where FSDP does not shard
the parameters, currently only when world_size  , or 
config, the modification is persisted regardless of .



This method works on modules which are not FSDP themselves but
may contain multiple independent FSDP units. In that case, the given
arguments will apply to all contained FSDP units.



Note that rank0_only=True in conjunction with
writeback=True is not currently supported and will raise an
error. This is because model parameter shapes would be different
across ranks within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.



Note that offload_to_cpu and rank0_only=False will
result in full parameters being redundantly copied to CPU memory for
GPUs that reside on the same machine, which may incur the risk of
CPU OOM. It is recommended to use offload_to_cpu with
rank0_only=True.


Parameters

================================================================================

# FullyShardedDataParallel (Part 34)

 () – recursively summon all params for nested
FSDP instances (default: True).
 () – if , modifications to params are
discarded after the context manager exits;
disabling this can be slightly more efficient (default: True)
rank0_only () – if , full parameters are
materialized on only global rank 0. This means that within the
context, only rank 0 will have full parameters and the other
ranks will have sharded parameters. Note that setting
rank0_only=True with writeback=True is not supported,
as model parameter shapes will be different across ranks
within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.
offload_to_cpu () – If , full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
for world_size = 1 or  config). It is recommended
to use offload_to_cpu with rank0_only=True to avoid
redundant copies of model parameters being offloaded to the same CPU memory.
with_grads () – If , gradients are also
unsharded with the parameters. Currently, this is only
supported when passing use_orig_params=True to the FSDP
constructor and offload_to_cpu=False to this method.
(Default: )

================================================================================

# FullyShardedDataParallel (Part 35)


Return type

A wrapper for sharding module parameters across data parallel workers.

This is inspired by  as
well as the ZeRO Stage 3 from .
FullyShardedDataParallel is commonly shortened to FSDP.

To understand FSDP internals, refer to the
FSDP Notes.

Code example:
torch.distributed.fsdp  FullyShardedDataParallel  
set_device
sharded_module  
  sharded_moduleparameters 
  sharded_module

Using FSDP involves wrapping your module and then initializing your
optimizer after. This is required since FSDP changes the parameter
variables.

When setting up FSDP, you need to consider the destination CUDA
device. If the device has an ID (), you have three options:

List:
Place the module on that device
Set the device using torch.cuda.set_device(dev_id)
Pass  into the  constructor argument.

Place the module on that device

Set the device using torch.cuda.set_device(dev_id)

Pass  into the  constructor argument.

This ensures that the FSDP instance’s compute device is the
destination device. For option 1 and 3, the FSDP initialization
always occurs on GPU. For option 2, the FSDP initialization
happens on module’s current device, which may be a CPU.

================================================================================

# FullyShardedDataParallel (Part 36)

If you’re using the sync_module_states=True flag, you need to
ensure that the module is on a GPU or use the 
argument to specify a CUDA device that FSDP will move the module
to in the FSDP constructor. This is necessary because
sync_module_states=True requires GPU communication.

FSDP also takes care of moving input tensors to the forward method
to the GPU compute device, so you don’t need to manually move them
from CPU.

For use_orig_params=True,
ShardingStrategy.SHARD_GRAD_OP exposes the unsharded
parameters, not the sharded parameters after forward, unlike
ShardingStrategy.FULL_SHARD. If you want
to inspect the gradients, you can use the summon_full_params
method with with_grads=True.

With limit_all_gathers=True, you may see a gap in the FSDP
pre-forward where the CPU thread is not issuing any kernels. This is
intentional and shows the rate limiter in effect. Synchronizing the CPU
thread in that way prevents over-allocating memory for subsequent
all-gathers, and it should not actually delay GPU kernel execution.

================================================================================

# FullyShardedDataParallel (Part 37)

FSDP replaces managed modules’ parameters with torch.Tensor
views during forward and backward computation for autograd-related
reasons. If your module’s forward relies on saved references to
the parameters instead of reacquiring the references each
iteration, then it will not see FSDP’s newly created views,
and autograd will not work correctly.

Finally, when using sharding_strategy=ShardingStrategy.HYBRID_SHARD
with the sharding process group being intra-node and the
replication process group being inter-node, setting
NCCL_CROSS_NIC=1 can help improve the all-reduce times over
the replication process group for some cluster setups.

There are several limitations to be aware of when using FSDP:

================================================================================

# FullyShardedDataParallel (Part 38)

List:
FSDP currently does not support gradient accumulation outside
 when using CPU offloading. This is because FSDP
uses the newly-reduced gradient instead of accumulating with any
existing gradient, which can lead to incorrect results.
FSDP does not support running the forward pass of a submodule
that is contained in an FSDP instance. This is because the
submodule’s parameters will be sharded, but the submodule itself
is not an FSDP instance, so its forward pass will not all-gather
the full parameters appropriately.
FSDP does not work with double backwards due to the way it
registers backward hooks.
FSDP has some constraints when freezing parameters.
For use_orig_params=False, each FSDP instance must manage
parameters that are all frozen or all non-frozen. For
use_orig_params=True, FSDP supports mixing frozen and
non-frozen parameters, but it’s recommended to avoid doing so to
prevent higher than expected gradient memory usage.
As of PyTorch 1.12, FSDP offers limited support for shared
parameters. If enhanced shared parameter support is needed for
your use case, please post in
this issue.
You should avoid modifying the parameters between forward and
backward without using the summon_full_params context, as
the modifications may not persist.

================================================================================

# FullyShardedDataParallel (Part 39)

FSDP currently does not support gradient accumulation outside
 when using CPU offloading. This is because FSDP
uses the newly-reduced gradient instead of accumulating with any
existing gradient, which can lead to incorrect results.

FSDP does not support running the forward pass of a submodule
that is contained in an FSDP instance. This is because the
submodule’s parameters will be sharded, but the submodule itself
is not an FSDP instance, so its forward pass will not all-gather
the full parameters appropriately.

FSDP does not work with double backwards due to the way it
registers backward hooks.

FSDP has some constraints when freezing parameters.
For use_orig_params=False, each FSDP instance must manage
parameters that are all frozen or all non-frozen. For
use_orig_params=True, FSDP supports mixing frozen and
non-frozen parameters, but it’s recommended to avoid doing so to
prevent higher than expected gradient memory usage.

As of PyTorch 1.12, FSDP offers limited support for shared
parameters. If enhanced shared parameter support is needed for
your use case, please post in
this issue.

================================================================================

# FullyShardedDataParallel (Part 40)

You should avoid modifying the parameters between forward and
backward without using the summon_full_params context, as
the modifications may not persist.

Parameters

================================================================================

# FullyShardedDataParallel (Part 41)

 () – This is the module to be wrapped with FSDP.
process_group (ProcessGroupProcessGroupProcessGroup) – This is the process group over which the model is sharded and thus
the one used for FSDP’s all-gather and reduce-scatter collective
communications. If , then FSDP uses the default process
group. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a tuple of
process groups, representing the groups over which to shard and
replicate, respectively. If , then FSDP constructs process
groups for the user to shard intra-node and replicate inter-node.
(Default: )
sharding_strategy (ShardingStrategy) – This configures the sharding strategy, which may trade off memory
saving and communication overhead. See ShardingStrategy
for details. (Default: FULL_SHARD)
cpu_offload (CPUOffload) – This configures CPU offloading. If this is set to , then
no CPU offloading happens. See CPUOffload for details.
(Default: )
auto_wrap_policy (ModuleWrapPolicyCustomPolicy) – This specifies a policy to apply FSDP to submodules of ,
which is needed for communication and computation overlap and thus
affects performance. If , then FSDP only applies to
, and users should manually apply FSDP to parent modules
themselves (proceeding bottom-up). For convenience, this accepts
ModuleWrapPolicy directly, which allows users to specify the
module classes to wrap (e.g. the transformer block). Otherwise,
this should be a callable that takes in three arguments
 ,  , and
nonwrapped_numel:  and should return a  specifying
whether the passed-in  should have FSDP applied if
recurse=False or if the traversal should continue into the
module’s subtree if recurse=True. Users may add additional
arguments to the callable. The size_based_auto_wrap_policy in
torch.distributed.fsdp.wrap.py gives an example callable that
applies FSDP to a module if the parameters in its subtree exceed
100M numel. We recommend printing the model after applying FSDP
and adjusting as needed.

================================================================================

# FullyShardedDataParallel (Part 42)

 custom_auto_wrap_policy
     
     
    nonwrapped_numel 
    # Additional custom arguments
    min_num_params   
  
     nonwrapped_numel  min_num_params
# Configure a custom `min_num_params`
my_auto_wrap_policy  custom_auto_wrap_policy min_num_params



================================================================================

# FullyShardedDataParallel (Part 43)

backward_prefetch (BackwardPrefetch) – This configures explicit backward prefetching of all-gathers. If
, then FSDP does not backward prefetch, and there is no
communication and computation overlap in the backward pass. See
BackwardPrefetch for details. (Default: BACKWARD_PRE)
mixed_precision (MixedPrecision) – This configures native mixed precision for FSDP. If this is set to
, then no mixed precision is used. Otherwise, parameter,
buffer, and gradient reduction dtypes can be set. See
MixedPrecision for details. (Default: )
ignored_modules (torch.nn.Module) – Modules whose
own parameters and child modules’ parameters and buffers are
ignored by this instance. None of the modules directly in
ignored_modules should be FullyShardedDataParallel
instances, and any child modules that are already-constructed
FullyShardedDataParallel instances will not be ignored if
they are nested under this instance. This argument may be used to
avoid sharding specific parameters at module granularity when using an
auto_wrap_policy or if parameters’ sharding is not managed by
FSDP. (Default: )
param_init_fn () – A Callable[torch.nn.Module]   that
specifies how modules that are currently on the meta device should
be initialized onto an actual device. As of v1.12, FSDP detects
modules with parameters or buffers on meta device via 
and either applies param_init_fn if specified or calls
nn.Module.reset_parameters() otherwise. For both cases, the
implementation should  initialize the parameters/buffers of
the module, not those of its submodules. This is to avoid
re-initialization. In addition, FSDP also supports deferred
initialization via torchdistX’s (pytorch/torchdistX)
deferred_init() API, where the deferred modules are initialized
by calling param_init_fn if specified or torchdistX’s default
materialize_module() otherwise. If param_init_fn is
specified, then it is applied to all meta-device modules, meaning
that it should probably case on the module type. FSDP calls the
initialization function before parameter flattening and sharding.

================================================================================

# FullyShardedDataParallel (Part 44)

  
 my_init_fn 
    # E.g. initialize depending on the module type
    
fsdp_model   param_init_fnmy_init_fn auto_wrap_policysize_based_auto_wrap_policy
fsdp_modelparameters # current CUDA device
# With torchdistX
  deferred_initdeferred_init 
# Will initialize via deferred_init.materialize_module().
fsdp_model   auto_wrap_policysize_based_auto_wrap_policy



================================================================================

# FullyShardedDataParallel (Part 45)

 (torch.device) – An  or
torch.device giving the CUDA device on which FSDP
initialization takes place, including the module initialization
if needed and the parameter sharding. This should be specified to
improve initialization speed if  is on CPU. If the
default CUDA device was set (e.g. via torch.cuda.set_device),
then the user may pass torch.cuda.current_device to this.
(Default: )
sync_module_states () – If , then each FSDP module will
broadcast module parameters and buffers from rank 0 to ensure that
they are replicated across ranks (adding communication overhead to
this constructor). This can help load state_dict checkpoints
via load_state_dict in a memory efficient way. See
FullStateDictConfig for an example of this. (Default:
)
forward_prefetch () – If , then FSDP explicitly prefetches
the next forward-pass all-gather before the current forward
computation. This is only useful for CPU-bound workloads, in which
case issuing the next all-gather earlier may improve overlap. This
should only be used for static-graph models since the prefetching
follows the first iteration’s execution order. (Default: )
limit_all_gathers () – If , then FSDP explicitly
synchronizes the CPU thread to ensure GPU memory usage from only
 consecutive FSDP instances (the current instance running
computation and the next instance whose all-gather is prefetched).
If , then FSDP allows the CPU thread to issue all-gathers
without any extra synchronization. (Default: ) We often
refer to this feature as the “rate limiter”. This flag should only
be set to  for specific CPU-bound workloads with low
memory pressure in which case the CPU thread can aggressively issue
all kernels without concern for the GPU memory usage.
use_orig_params () – Setting this to  has FSDP use
 ‘s original parameters. FSDP exposes those original
parameters to the user via nn.Module.named_parameters()
instead of FSDP’s internal FlatParameter s. This means
that the optimizer step runs on the original parameters, enabling
per-original-parameter hyperparameters. FSDP preserves the original
parameter variables and manipulates their data between unsharded
and sharded forms, where they are always views into the underlying
unsharded or sharded FlatParameter, respectively. With the
current algorithm, the sharded form is always 1D, losing the
original tensor structure. An original parameter may have all,
some, or none of its data present for a given rank. In the none
case, its data will be like a size-0 empty tensor. Users should not
author programs relying on what data is present for a given
original parameter in its sharded form.  is required to
use torch.compile(). Setting this to  exposes FSDP’s
internal FlatParameter s to the user via
nn.Module.named_parameters(). (Default: )
ignored_states (torch.nn.Parametertorch.nn.Module) – Ignored parameters or modules that will not be managed by this FSDP
instance, meaning that the parameters are not sharded and their
gradients are not reduced across ranks. This argument unifies with
the existing ignored_modules argument, and we may deprecate
ignored_modules soon. For backward compatibility, we keep both
ignored_states and ignored_modules`, but FSDP only allows one
of them to be specified as not .
device_mesh (DeviceMesh) – DeviceMesh can be used as an alternative to
process_group. When device_mesh is passed, FSDP will use the underlying process
groups for all-gather and reduce-scatter collective communications. Therefore,
these two args need to be mutually exclusive. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a 2D DeviceMesh instead
of a tuple of process groups. For 2D FSDP + TP, users are required to pass in
device_mesh instead of process_group. For more DeviceMesh info, please visit:
https://pytorch.org/tutorials/recipes/distributed_device_mesh.html

================================================================================

# FullyShardedDataParallel (Part 46)

List:
() – This is the module to be wrapped with FSDP.
process_group (ProcessGroupProcessGroupProcessGroup) – This is the process group over which the model is sharded and thus
the one used for FSDP’s all-gather and reduce-scatter collective
communications. If , then FSDP uses the default process
group. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a tuple of
process groups, representing the groups over which to shard and
replicate, respectively. If , then FSDP constructs process
groups for the user to shard intra-node and replicate inter-node.
(Default: )
sharding_strategy (ShardingStrategy) – This configures the sharding strategy, which may trade off memory
saving and communication overhead. See ShardingStrategy
for details. (Default: FULL_SHARD)
cpu_offload (CPUOffload) – This configures CPU offloading. If this is set to , then
no CPU offloading happens. See CPUOffload for details.
(Default: )
auto_wrap_policy (ModuleWrapPolicyCustomPolicy) – This specifies a policy to apply FSDP to submodules of ,
which is needed for communication and computation overlap and thus
affects performance. If , then FSDP only applies to
, and users should manually apply FSDP to parent modules
themselves (proceeding bottom-up). For convenience, this accepts
ModuleWrapPolicy directly, which allows users to specify the
module classes to wrap (e.g. the transformer block). Otherwise,
this should be a callable that takes in three arguments
 ,  , and
nonwrapped_numel:  and should return a  specifying
whether the passed-in  should have FSDP applied if
recurse=False or if the traversal should continue into the
module’s subtree if recurse=True. Users may add additional
arguments to the callable. The size_based_auto_wrap_policy in
torch.distributed.fsdp.wrap.py gives an example callable that
applies FSDP to a module if the parameters in its subtree exceed
100M numel. We recommend printing the model after applying FSDP
and adjusting as needed.

================================================================================

# FullyShardedDataParallel (Part 47)

 custom_auto_wrap_policy
     
     
    nonwrapped_numel 
    # Additional custom arguments
    min_num_params   
  
     nonwrapped_numel  min_num_params
# Configure a custom `min_num_params`
my_auto_wrap_policy  custom_auto_wrap_policy min_num_params



================================================================================

# FullyShardedDataParallel (Part 48)

backward_prefetch (BackwardPrefetch) – This configures explicit backward prefetching of all-gathers. If
, then FSDP does not backward prefetch, and there is no
communication and computation overlap in the backward pass. See
BackwardPrefetch for details. (Default: BACKWARD_PRE)
mixed_precision (MixedPrecision) – This configures native mixed precision for FSDP. If this is set to
, then no mixed precision is used. Otherwise, parameter,
buffer, and gradient reduction dtypes can be set. See
MixedPrecision for details. (Default: )
ignored_modules (torch.nn.Module) – Modules whose
own parameters and child modules’ parameters and buffers are
ignored by this instance. None of the modules directly in
ignored_modules should be FullyShardedDataParallel
instances, and any child modules that are already-constructed
FullyShardedDataParallel instances will not be ignored if
they are nested under this instance. This argument may be used to
avoid sharding specific parameters at module granularity when using an
auto_wrap_policy or if parameters’ sharding is not managed by
FSDP. (Default: )
param_init_fn () – A Callable[torch.nn.Module]   that
specifies how modules that are currently on the meta device should
be initialized onto an actual device. As of v1.12, FSDP detects
modules with parameters or buffers on meta device via 
and either applies param_init_fn if specified or calls
nn.Module.reset_parameters() otherwise. For both cases, the
implementation should  initialize the parameters/buffers of
the module, not those of its submodules. This is to avoid
re-initialization. In addition, FSDP also supports deferred
initialization via torchdistX’s (pytorch/torchdistX)
deferred_init() API, where the deferred modules are initialized
by calling param_init_fn if specified or torchdistX’s default
materialize_module() otherwise. If param_init_fn is
specified, then it is applied to all meta-device modules, meaning
that it should probably case on the module type. FSDP calls the
initialization function before parameter flattening and sharding.

================================================================================

# FullyShardedDataParallel (Part 49)

  
 my_init_fn 
    # E.g. initialize depending on the module type
    
fsdp_model   param_init_fnmy_init_fn auto_wrap_policysize_based_auto_wrap_policy
fsdp_modelparameters # current CUDA device
# With torchdistX
  deferred_initdeferred_init 
# Will initialize via deferred_init.materialize_module().
fsdp_model   auto_wrap_policysize_based_auto_wrap_policy



================================================================================

# FullyShardedDataParallel (Part 50)

 (torch.device) – An  or
torch.device giving the CUDA device on which FSDP
initialization takes place, including the module initialization
if needed and the parameter sharding. This should be specified to
improve initialization speed if  is on CPU. If the
default CUDA device was set (e.g. via torch.cuda.set_device),
then the user may pass torch.cuda.current_device to this.
(Default: )
sync_module_states () – If , then each FSDP module will
broadcast module parameters and buffers from rank 0 to ensure that
they are replicated across ranks (adding communication overhead to
this constructor). This can help load state_dict checkpoints
via load_state_dict in a memory efficient way. See
FullStateDictConfig for an example of this. (Default:
)
forward_prefetch () – If , then FSDP explicitly prefetches
the next forward-pass all-gather before the current forward
computation. This is only useful for CPU-bound workloads, in which
case issuing the next all-gather earlier may improve overlap. This
should only be used for static-graph models since the prefetching
follows the first iteration’s execution order. (Default: )
limit_all_gathers () – If , then FSDP explicitly
synchronizes the CPU thread to ensure GPU memory usage from only
 consecutive FSDP instances (the current instance running
computation and the next instance whose all-gather is prefetched).
If , then FSDP allows the CPU thread to issue all-gathers
without any extra synchronization. (Default: ) We often
refer to this feature as the “rate limiter”. This flag should only
be set to  for specific CPU-bound workloads with low
memory pressure in which case the CPU thread can aggressively issue
all kernels without concern for the GPU memory usage.
use_orig_params () – Setting this to  has FSDP use
 ‘s original parameters. FSDP exposes those original
parameters to the user via nn.Module.named_parameters()
instead of FSDP’s internal FlatParameter s. This means
that the optimizer step runs on the original parameters, enabling
per-original-parameter hyperparameters. FSDP preserves the original
parameter variables and manipulates their data between unsharded
and sharded forms, where they are always views into the underlying
unsharded or sharded FlatParameter, respectively. With the
current algorithm, the sharded form is always 1D, losing the
original tensor structure. An original parameter may have all,
some, or none of its data present for a given rank. In the none
case, its data will be like a size-0 empty tensor. Users should not
author programs relying on what data is present for a given
original parameter in its sharded form.  is required to
use torch.compile(). Setting this to  exposes FSDP’s
internal FlatParameter s to the user via
nn.Module.named_parameters(). (Default: )
ignored_states (torch.nn.Parametertorch.nn.Module) – Ignored parameters or modules that will not be managed by this FSDP
instance, meaning that the parameters are not sharded and their
gradients are not reduced across ranks. This argument unifies with
the existing ignored_modules argument, and we may deprecate
ignored_modules soon. For backward compatibility, we keep both
ignored_states and ignored_modules`, but FSDP only allows one
of them to be specified as not .
device_mesh (DeviceMesh) – DeviceMesh can be used as an alternative to
process_group. When device_mesh is passed, FSDP will use the underlying process
groups for all-gather and reduce-scatter collective communications. Therefore,
these two args need to be mutually exclusive. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a 2D DeviceMesh instead
of a tuple of process groups. For 2D FSDP + TP, users are required to pass in
device_mesh instead of process_group. For more DeviceMesh info, please visit:
https://pytorch.org/tutorials/recipes/distributed_device_mesh.html

================================================================================

# FullyShardedDataParallel (Part 51)

() – This is the module to be wrapped with FSDP.

process_group (ProcessGroupProcessGroupProcessGroup) – This is the process group over which the model is sharded and thus
the one used for FSDP’s all-gather and reduce-scatter collective
communications. If , then FSDP uses the default process
group. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a tuple of
process groups, representing the groups over which to shard and
replicate, respectively. If , then FSDP constructs process
groups for the user to shard intra-node and replicate inter-node.
(Default: )

sharding_strategy (ShardingStrategy) – This configures the sharding strategy, which may trade off memory
saving and communication overhead. See ShardingStrategy
for details. (Default: FULL_SHARD)

cpu_offload (CPUOffload) – This configures CPU offloading. If this is set to , then
no CPU offloading happens. See CPUOffload for details.
(Default: )

================================================================================

# FullyShardedDataParallel (Part 52)

auto_wrap_policy (ModuleWrapPolicyCustomPolicy) – This specifies a policy to apply FSDP to submodules of ,
which is needed for communication and computation overlap and thus
affects performance. If , then FSDP only applies to
, and users should manually apply FSDP to parent modules
themselves (proceeding bottom-up). For convenience, this accepts
ModuleWrapPolicy directly, which allows users to specify the
module classes to wrap (e.g. the transformer block). Otherwise,
this should be a callable that takes in three arguments
 ,  , and
nonwrapped_numel:  and should return a  specifying
whether the passed-in  should have FSDP applied if
recurse=False or if the traversal should continue into the
module’s subtree if recurse=True. Users may add additional
arguments to the callable. The size_based_auto_wrap_policy in
torch.distributed.fsdp.wrap.py gives an example callable that
applies FSDP to a module if the parameters in its subtree exceed
100M numel. We recommend printing the model after applying FSDP
and adjusting as needed.

================================================================================

# FullyShardedDataParallel (Part 53)

 custom_auto_wrap_policy
     
     
    nonwrapped_numel 
    # Additional custom arguments
    min_num_params   
  
     nonwrapped_numel  min_num_params
# Configure a custom `min_num_params`
my_auto_wrap_policy  custom_auto_wrap_policy min_num_params

================================================================================

# FullyShardedDataParallel (Part 54)

This specifies a policy to apply FSDP to submodules of ,
which is needed for communication and computation overlap and thus
affects performance. If , then FSDP only applies to
, and users should manually apply FSDP to parent modules
themselves (proceeding bottom-up). For convenience, this accepts
ModuleWrapPolicy directly, which allows users to specify the
module classes to wrap (e.g. the transformer block). Otherwise,
this should be a callable that takes in three arguments
 ,  , and
nonwrapped_numel:  and should return a  specifying
whether the passed-in  should have FSDP applied if
recurse=False or if the traversal should continue into the
module’s subtree if recurse=True. Users may add additional
arguments to the callable. The size_based_auto_wrap_policy in
torch.distributed.fsdp.wrap.py gives an example callable that
applies FSDP to a module if the parameters in its subtree exceed
100M numel. We recommend printing the model after applying FSDP
and adjusting as needed.

================================================================================

# FullyShardedDataParallel (Part 55)

Code example:
custom_auto_wrap_policy
     
     
    nonwrapped_numel 
    # Additional custom arguments
    min_num_params   
  
     nonwrapped_numel  min_num_params
# Configure a custom `min_num_params`
my_auto_wrap_policy  custom_auto_wrap_policy min_num_params

backward_prefetch (BackwardPrefetch) – This configures explicit backward prefetching of all-gathers. If
, then FSDP does not backward prefetch, and there is no
communication and computation overlap in the backward pass. See
BackwardPrefetch for details. (Default: BACKWARD_PRE)

mixed_precision (MixedPrecision) – This configures native mixed precision for FSDP. If this is set to
, then no mixed precision is used. Otherwise, parameter,
buffer, and gradient reduction dtypes can be set. See
MixedPrecision for details. (Default: )

================================================================================

# FullyShardedDataParallel (Part 56)

ignored_modules (torch.nn.Module) – Modules whose
own parameters and child modules’ parameters and buffers are
ignored by this instance. None of the modules directly in
ignored_modules should be FullyShardedDataParallel
instances, and any child modules that are already-constructed
FullyShardedDataParallel instances will not be ignored if
they are nested under this instance. This argument may be used to
avoid sharding specific parameters at module granularity when using an
auto_wrap_policy or if parameters’ sharding is not managed by
FSDP. (Default: )

================================================================================

# FullyShardedDataParallel (Part 57)

param_init_fn () – A Callable[torch.nn.Module]   that
specifies how modules that are currently on the meta device should
be initialized onto an actual device. As of v1.12, FSDP detects
modules with parameters or buffers on meta device via 
and either applies param_init_fn if specified or calls
nn.Module.reset_parameters() otherwise. For both cases, the
implementation should  initialize the parameters/buffers of
the module, not those of its submodules. This is to avoid
re-initialization. In addition, FSDP also supports deferred
initialization via torchdistX’s (pytorch/torchdistX)
deferred_init() API, where the deferred modules are initialized
by calling param_init_fn if specified or torchdistX’s default
materialize_module() otherwise. If param_init_fn is
specified, then it is applied to all meta-device modules, meaning
that it should probably case on the module type. FSDP calls the
initialization function before parameter flattening and sharding.

================================================================================

# FullyShardedDataParallel (Part 58)

  
 my_init_fn 
    # E.g. initialize depending on the module type
    
fsdp_model   param_init_fnmy_init_fn auto_wrap_policysize_based_auto_wrap_policy
fsdp_modelparameters # current CUDA device
# With torchdistX
  deferred_initdeferred_init 
# Will initialize via deferred_init.materialize_module().
fsdp_model   auto_wrap_policysize_based_auto_wrap_policy

================================================================================

# FullyShardedDataParallel (Part 59)

A Callable[torch.nn.Module]   that
specifies how modules that are currently on the meta device should
be initialized onto an actual device. As of v1.12, FSDP detects
modules with parameters or buffers on meta device via 
and either applies param_init_fn if specified or calls
nn.Module.reset_parameters() otherwise. For both cases, the
implementation should  initialize the parameters/buffers of
the module, not those of its submodules. This is to avoid
re-initialization. In addition, FSDP also supports deferred
initialization via torchdistX’s (pytorch/torchdistX)
deferred_init() API, where the deferred modules are initialized
by calling param_init_fn if specified or torchdistX’s default
materialize_module() otherwise. If param_init_fn is
specified, then it is applied to all meta-device modules, meaning
that it should probably case on the module type. FSDP calls the
initialization function before parameter flattening and sharding.

================================================================================

# FullyShardedDataParallel (Part 60)

Code example:
my_init_fn 
    # E.g. initialize depending on the module type
    
fsdp_model   param_init_fnmy_init_fn auto_wrap_policysize_based_auto_wrap_policy
fsdp_modelparameters # current CUDA device
# With torchdistX
  deferred_initdeferred_init 
# Will initialize via deferred_init.materialize_module().
fsdp_model   auto_wrap_policysize_based_auto_wrap_policy

(torch.device) – An  or
torch.device giving the CUDA device on which FSDP
initialization takes place, including the module initialization
if needed and the parameter sharding. This should be specified to
improve initialization speed if  is on CPU. If the
default CUDA device was set (e.g. via torch.cuda.set_device),
then the user may pass torch.cuda.current_device to this.
(Default: )

sync_module_states () – If , then each FSDP module will
broadcast module parameters and buffers from rank 0 to ensure that
they are replicated across ranks (adding communication overhead to
this constructor). This can help load state_dict checkpoints
via load_state_dict in a memory efficient way. See
FullStateDictConfig for an example of this. (Default:
)

================================================================================

# FullyShardedDataParallel (Part 61)

forward_prefetch () – If , then FSDP explicitly prefetches
the next forward-pass all-gather before the current forward
computation. This is only useful for CPU-bound workloads, in which
case issuing the next all-gather earlier may improve overlap. This
should only be used for static-graph models since the prefetching
follows the first iteration’s execution order. (Default: )

limit_all_gathers () – If , then FSDP explicitly
synchronizes the CPU thread to ensure GPU memory usage from only
 consecutive FSDP instances (the current instance running
computation and the next instance whose all-gather is prefetched).
If , then FSDP allows the CPU thread to issue all-gathers
without any extra synchronization. (Default: ) We often
refer to this feature as the “rate limiter”. This flag should only
be set to  for specific CPU-bound workloads with low
memory pressure in which case the CPU thread can aggressively issue
all kernels without concern for the GPU memory usage.

================================================================================

# FullyShardedDataParallel (Part 62)

use_orig_params () – Setting this to  has FSDP use
 ‘s original parameters. FSDP exposes those original
parameters to the user via nn.Module.named_parameters()
instead of FSDP’s internal FlatParameter s. This means
that the optimizer step runs on the original parameters, enabling
per-original-parameter hyperparameters. FSDP preserves the original
parameter variables and manipulates their data between unsharded
and sharded forms, where they are always views into the underlying
unsharded or sharded FlatParameter, respectively. With the
current algorithm, the sharded form is always 1D, losing the
original tensor structure. An original parameter may have all,
some, or none of its data present for a given rank. In the none
case, its data will be like a size-0 empty tensor. Users should not
author programs relying on what data is present for a given
original parameter in its sharded form.  is required to
use torch.compile(). Setting this to  exposes FSDP’s
internal FlatParameter s to the user via
nn.Module.named_parameters(). (Default: )

================================================================================

# FullyShardedDataParallel (Part 63)

ignored_states (torch.nn.Parametertorch.nn.Module) – Ignored parameters or modules that will not be managed by this FSDP
instance, meaning that the parameters are not sharded and their
gradients are not reduced across ranks. This argument unifies with
the existing ignored_modules argument, and we may deprecate
ignored_modules soon. For backward compatibility, we keep both
ignored_states and ignored_modules`, but FSDP only allows one
of them to be specified as not .

device_mesh (DeviceMesh) – DeviceMesh can be used as an alternative to
process_group. When device_mesh is passed, FSDP will use the underlying process
groups for all-gather and reduce-scatter collective communications. Therefore,
these two args need to be mutually exclusive. For hybrid sharding strategies such as
ShardingStrategy.HYBRID_SHARD, users can pass in a 2D DeviceMesh instead
of a tuple of process groups. For 2D FSDP + TP, users are required to pass in
device_mesh instead of process_group. For more DeviceMesh info, please visit:
https://pytorch.org/tutorials/recipes/distributed_device_mesh.html

================================================================================

# FullyShardedDataParallel (Part 64)

Apply  recursively to every submodule (as returned by .children()) as well as self.
Typical use includes initializing the parameters of a model (see also torch.nn.init).
Compared to torch.nn.Module.apply, this version additionally gathers
the full parameters before applying . It should not be called from
within another summon_full_params context.

Parameters
 ( -> None) – function to be applied to each submodule




Return type

Apply  recursively to every submodule (as returned by .children()) as well as self.

Typical use includes initializing the parameters of a model (see also torch.nn.init).

Compared to torch.nn.Module.apply, this version additionally gathers
the full parameters before applying . It should not be called from
within another summon_full_params context.

Parameters
 ( -> None) – function to be applied to each submodule




Return type

( -> None) – function to be applied to each submodule

check_is_root
Check if this instance is a root FSDP module.

Return type

Check if this instance is a root FSDP module.

clip_grad_norm_, 
Clip the gradient norm of all parameters.
The norm is computed over all parameters’ gradients as viewed as a single vector, and the
gradients are modified in-place.

================================================================================

# FullyShardedDataParallel (Part 65)

Parameters

 () – max norm of the gradients
 () – type of the used p-norm. Can be 
for infinity norm.



Total norm of the parameters (viewed as a single vector).

Return type



If every FSDP instance uses , meaning that no
gradients are sharded across ranks, then you may directly use
torch.nn.utils.clip_grad_norm_().
If at least some FSDP instance uses a sharded strategy (i.e.
one other than ), then you should use this method
instead of torch.nn.utils.clip_grad_norm_() since this method
handles the fact that gradients are sharded across ranks.
The total norm returned will have the “largest” dtype across
all parameters/gradients as defined by PyTorch’s type promotion
semantics. For example, if  parameters/gradients use a low
precision dtype, then the returned norm’s dtype will be that low
precision dtype, but if there exists at least one parameter/
gradient using FP32, then the returned norm’s dtype will be FP32.


This needs to be called on all ranks since it uses
collective communications.

Clip the gradient norm of all parameters.

The norm is computed over all parameters’ gradients as viewed as a single vector, and the
gradients are modified in-place.

Parameters

================================================================================

# FullyShardedDataParallel (Part 66)

 () – max norm of the gradients
 () – type of the used p-norm. Can be 
for infinity norm.



Total norm of the parameters (viewed as a single vector).

Return type

List:
() – max norm of the gradients
 () – type of the used p-norm. Can be 
for infinity norm.

() – max norm of the gradients

() – type of the used p-norm. Can be 
for infinity norm.

Total norm of the parameters (viewed as a single vector).

If every FSDP instance uses , meaning that no
gradients are sharded across ranks, then you may directly use
torch.nn.utils.clip_grad_norm_().

If at least some FSDP instance uses a sharded strategy (i.e.
one other than ), then you should use this method
instead of torch.nn.utils.clip_grad_norm_() since this method
handles the fact that gradients are sharded across ranks.

The total norm returned will have the “largest” dtype across
all parameters/gradients as defined by PyTorch’s type promotion
semantics. For example, if  parameters/gradients use a low
precision dtype, then the returned norm’s dtype will be that low
precision dtype, but if there exists at least one parameter/
gradient using FP32, then the returned norm’s dtype will be FP32.

================================================================================

# FullyShardedDataParallel (Part 67)

This needs to be called on all ranks since it uses
collective communications.

flatten_sharded_optim_state_dictsharded_optim_state_dict, , 
Flatten a sharded optimizer state-dict.
The API is similar to shard_full_optim_state_dict(). The only
difference is that the input sharded_optim_state_dict should be
returned from sharded_optim_state_dict(). Therefore, there will
be all-gather calls on each rank to gather ShardedTensor s.

Parameters

sharded_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
sharded optimizer state.
 (torch.nn.Module) – Refer to shard_full_optim_state_dict().
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.



Refer to shard_full_optim_state_dict().

Return type
[, ]

Flatten a sharded optimizer state-dict.

The API is similar to shard_full_optim_state_dict(). The only
difference is that the input sharded_optim_state_dict should be
returned from sharded_optim_state_dict(). Therefore, there will
be all-gather calls on each rank to gather ShardedTensor s.

Parameters

================================================================================

# FullyShardedDataParallel (Part 68)

sharded_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
sharded optimizer state.
 (torch.nn.Module) – Refer to shard_full_optim_state_dict().
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.



Refer to shard_full_optim_state_dict().

Return type
[, ]

List:
sharded_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
sharded optimizer state.
 (torch.nn.Module) – Refer to shard_full_optim_state_dict().
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.

sharded_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
sharded optimizer state.

(torch.nn.Module) – Refer to shard_full_optim_state_dict().

(torch.optim.Optimizer) – Optimizer for  ‘s
parameters.

Refer to shard_full_optim_state_dict().

, 
Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.

Return type

Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.

================================================================================

# FullyShardedDataParallel (Part 69)

fsdp_modules, 
Return all nested FSDP instances.
This possibly includes  itself and only includes FSDP root modules if root_only=True.

Parameters

 (torch.nn.Module) – Root module, which may or may not be an
 module.
 () – Whether to return only FSDP root modules.
(Default: )



FSDP modules that are nested in
the input .

Return type
List[FullyShardedDataParallel]

Return all nested FSDP instances.

This possibly includes  itself and only includes FSDP root modules if root_only=True.

Parameters

 (torch.nn.Module) – Root module, which may or may not be an
 module.
 () – Whether to return only FSDP root modules.
(Default: )



FSDP modules that are nested in
the input .

Return type
List[FullyShardedDataParallel]

List:
(torch.nn.Module) – Root module, which may or may not be an
 module.
 () – Whether to return only FSDP root modules.
(Default: )

(torch.nn.Module) – Root module, which may or may not be an
 module.

() – Whether to return only FSDP root modules.
(Default: )

FSDP modules that are nested in
the input .

List[FullyShardedDataParallel]

================================================================================

# FullyShardedDataParallel (Part 70)

full_optim_state_dict, , optim_input, rank0_only, 
Return the full optimizer state-dict.
Consolidates the full optimizer state on rank 0 and returns it
as a  following the convention of
torch.optim.Optimizer.state_dict(), i.e. with keys 
and "param_groups". The flattened parameters in  modules
contained in  are mapped back to their unflattened parameters.
This needs to be called on all ranks since it uses
collective communications. However, if rank0_only=True, then
the state dict is only populated on rank 0, and all other ranks
return an empty .
Unlike torch.optim.Optimizer.state_dict(), this method
uses full parameter names as keys instead of parameter IDs.
Like in torch.optim.Optimizer.state_dict(), the tensors
contained in the optimizer state dict are not cloned, so there may
be aliasing surprises. For best practices, consider saving the
returned optimizer state dict immediately, e.g. using
torch.save().

Parameters

================================================================================

# FullyShardedDataParallel (Part 71)

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_input (torch.nn.Parameter) – Input passed into the optimizer  representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
rank0_only () – If , saves the populated 
only on rank 0; if , saves it on all ranks. (Default:
)
 (dist.ProcessGroup) – Model’s process group or  if using
the default process group. (Default: )



A  containing the optimizer state for
 ‘s original unflattened parameters and including keys
“state” and “param_groups” following the convention of
torch.optim.Optimizer.state_dict(). If rank0_only=True,
then nonzero ranks return an empty .

Return type
Dict[, Any]

Return the full optimizer state-dict.

================================================================================

# FullyShardedDataParallel (Part 72)

Consolidates the full optimizer state on rank 0 and returns it
as a  following the convention of
torch.optim.Optimizer.state_dict(), i.e. with keys 
and "param_groups". The flattened parameters in  modules
contained in  are mapped back to their unflattened parameters.

This needs to be called on all ranks since it uses
collective communications. However, if rank0_only=True, then
the state dict is only populated on rank 0, and all other ranks
return an empty .

Unlike torch.optim.Optimizer.state_dict(), this method
uses full parameter names as keys instead of parameter IDs.

Like in torch.optim.Optimizer.state_dict(), the tensors
contained in the optimizer state dict are not cloned, so there may
be aliasing surprises. For best practices, consider saving the
returned optimizer state dict immediately, e.g. using
torch.save().

Parameters

================================================================================

# FullyShardedDataParallel (Part 73)

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_input (torch.nn.Parameter) – Input passed into the optimizer  representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
rank0_only () – If , saves the populated 
only on rank 0; if , saves it on all ranks. (Default:
)
 (dist.ProcessGroup) – Model’s process group or  if using
the default process group. (Default: )



A  containing the optimizer state for
 ‘s original unflattened parameters and including keys
“state” and “param_groups” following the convention of
torch.optim.Optimizer.state_dict(). If rank0_only=True,
then nonzero ranks return an empty .

Return type
Dict[, Any]

================================================================================

# FullyShardedDataParallel (Part 74)

List:
(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_input (torch.nn.Parameter) – Input passed into the optimizer  representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
rank0_only () – If , saves the populated 
only on rank 0; if , saves it on all ranks. (Default:
)
 (dist.ProcessGroup) – Model’s process group or  if using
the default process group. (Default: )

(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .

(torch.optim.Optimizer) – Optimizer for  ‘s
parameters.

optim_input (torch.nn.Parameter) – Input passed into the optimizer  representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )

================================================================================

# FullyShardedDataParallel (Part 75)

rank0_only () – If , saves the populated 
only on rank 0; if , saves it on all ranks. (Default:
)

(dist.ProcessGroup) – Model’s process group or  if using
the default process group. (Default: )

A  containing the optimizer state for
 ‘s original unflattened parameters and including keys
“state” and “param_groups” following the convention of
torch.optim.Optimizer.state_dict(). If rank0_only=True,
then nonzero ranks return an empty .

get_state_dict_type
Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at .
The target module does not have to be an FSDP module.


A StateDictSettings containing the state_dict_type and
state_dict / optim_state_dict configs that are currently set.



AssertionError` if the StateDictSettings for differen – 
FSDP submodules differ. – 


Return type
StateDictSettings

Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at .

The target module does not have to be an FSDP module.

A StateDictSettings containing the state_dict_type and
state_dict / optim_state_dict configs that are currently set.



AssertionError` if the StateDictSettings for differen – 
FSDP submodules differ. – 

================================================================================

# FullyShardedDataParallel (Part 76)


Return type
StateDictSettings

A StateDictSettings containing the state_dict_type and
state_dict / optim_state_dict configs that are currently set.

List:
AssertionError` if the StateDictSettings for differen – 
FSDP submodules differ. –

AssertionError` if the StateDictSettings for differen –

FSDP submodules differ. –

Return the wrapped module.

Return the wrapped module.

named_buffers, 
Return an iterator over module buffers, yielding both the name of the buffer and the buffer itself.
Intercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix
when inside the summon_full_params() context manager.

Return type
[[, torch.Tensor]]

Return an iterator over module buffers, yielding both the name of the buffer and the buffer itself.

Intercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix
when inside the summon_full_params() context manager.

Return type
[[, torch.Tensor]]

================================================================================

# FullyShardedDataParallel (Part 77)

named_parameters, 
Return an iterator over module parameters, yielding both the name of the parameter and the parameter itself.
Intercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix
when inside the summon_full_params() context manager.

Return type
[[, torch.nn.parameter.Parameter]]

Return an iterator over module parameters, yielding both the name of the parameter and the parameter itself.

Intercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix
when inside the summon_full_params() context manager.

Return type
[[, torch.nn.parameter.Parameter]]

[[, torch.nn.parameter.Parameter]]

Disable gradient synchronizations across FSDP instances.
Within this context, gradients will be accumulated in module
variables, which will later be synchronized in the first
forward-backward pass after exiting the context. This should only be
used on the root FSDP instance and will recursively apply to all
children FSDP instances.


This likely results in higher memory usage because FSDP will
accumulate the full model gradients (instead of gradient shards)
until the eventual sync.



================================================================================

# FullyShardedDataParallel (Part 78)

When used with CPU offloading, the gradients will not be
offloaded to CPU when inside the context manager. Instead, they
will only be offloaded right after the eventual sync.


Return type

Disable gradient synchronizations across FSDP instances.

Within this context, gradients will be accumulated in module
variables, which will later be synchronized in the first
forward-backward pass after exiting the context. This should only be
used on the root FSDP instance and will recursively apply to all
children FSDP instances.

This likely results in higher memory usage because FSDP will
accumulate the full model gradients (instead of gradient shards)
until the eventual sync.

When used with CPU offloading, the gradients will not be
offloaded to CPU when inside the context manager. Instead, they
will only be offloaded right after the eventual sync.

================================================================================

# FullyShardedDataParallel (Part 79)

optim_state_dict, , optim_state_dict, 
Transform the state-dict of an optimizer corresponding to a sharded model.
The given state-dict can be transformed to one of three types:
1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.
For full optimizer state_dict, all states are unflattened and not sharded.
Rank0 only and CPU only can be specified via state_dict_type() to
avoid OOM.
For sharded optimizer state_dict, all states are unflattened but sharded.
CPU only can be specified via state_dict_type() to further save
memory.
For local state_dict, no transformation will be performed. But a state
will be converted from nn.Tensor to ShardedTensor to represent its sharding
nature (this is not supported yet).

 torch.distributed.fsdp  FullyShardedDataParallel  
 torch.distributed.fsdp  StateDictType
 torch.distributed.fsdp  FullStateDictConfig
 torch.distributed.fsdp  FullOptimStateDictConfig
# Save a checkpoint
   
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

================================================================================

# FullyShardedDataParallel (Part 80)

state_dict  state_dict
optim_state_dict  optim_state_dict 
save_a_checkpointstate_dict optim_state_dict
# Load a checkpoint
   
state_dict optim_state_dict  load_a_checkpoint
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

load_state_dictstate_dict
optim_state_dict  optim_state_dict_to_load
      optim_state_dict

load_state_dictoptim_state_dict



Parameters

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – the target optimizer state_dict to
transform. If the value is None, optim.state_dict() will be used. (
Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )



A  containing the optimizer state for
. The sharding of the optimizer state is based on
state_dict_type.

Return type
Dict[, Any]

Transform the state-dict of an optimizer corresponding to a sharded model.

================================================================================

# FullyShardedDataParallel (Part 81)

The given state-dict can be transformed to one of three types:
1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.

For full optimizer state_dict, all states are unflattened and not sharded.
Rank0 only and CPU only can be specified via state_dict_type() to
avoid OOM.

For sharded optimizer state_dict, all states are unflattened but sharded.
CPU only can be specified via state_dict_type() to further save
memory.

For local state_dict, no transformation will be performed. But a state
will be converted from nn.Tensor to ShardedTensor to represent its sharding
nature (this is not supported yet).

Code example:
torch.distributed.fsdp  FullyShardedDataParallel  
 torch.distributed.fsdp  StateDictType
 torch.distributed.fsdp  FullStateDictConfig
 torch.distributed.fsdp  FullOptimStateDictConfig
# Save a checkpoint
   
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

================================================================================

# FullyShardedDataParallel (Part 82)

state_dict  state_dict
optim_state_dict  optim_state_dict 
save_a_checkpointstate_dict optim_state_dict
# Load a checkpoint
   
state_dict optim_state_dict  load_a_checkpoint
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

load_state_dictstate_dict
optim_state_dict  optim_state_dict_to_load
      optim_state_dict

load_state_dictoptim_state_dict

Parameters

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – the target optimizer state_dict to
transform. If the value is None, optim.state_dict() will be used. (
Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )



A  containing the optimizer state for
. The sharding of the optimizer state is based on
state_dict_type.

Return type
Dict[, Any]

================================================================================

# FullyShardedDataParallel (Part 83)

List:
(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – the target optimizer state_dict to
transform. If the value is None, optim.state_dict() will be used. (
Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )

(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .

(torch.optim.Optimizer) – Optimizer for  ‘s
parameters.

optim_state_dict () – the target optimizer state_dict to
transform. If the value is None, optim.state_dict() will be used. (
Default: )

(dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )

A  containing the optimizer state for
. The sharding of the optimizer state is based on
state_dict_type.

================================================================================

# FullyShardedDataParallel (Part 84)

optim_state_dict_to_load, , optim_state_dict, is_named_optimizer, load_directly, 
Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.
Given a optim_state_dict that is transformed through
optim_state_dict(), it gets converted to the flattened optimizer
state_dict that can be loaded to  which is the optimizer for
.  must be sharded by FullyShardedDataParallel.
 torch.distributed.fsdp  FullyShardedDataParallel  
 torch.distributed.fsdp  StateDictType
 torch.distributed.fsdp  FullStateDictConfig
 torch.distributed.fsdp  FullOptimStateDictConfig
# Save a checkpoint
   
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

state_dict  state_dict
original_osd  state_dict
optim_state_dict  optim_state_dict
    
    
    optim_state_dictoriginal_osd

save_a_checkpointstate_dict optim_state_dict
# Load a checkpoint
   
state_dict optim_state_dict  load_a_checkpoint
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

================================================================================

# FullyShardedDataParallel (Part 85)

load_state_dictstate_dict
optim_state_dict  optim_state_dict_to_load
      optim_state_dict

load_state_dictoptim_state_dict



Parameters

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – The optimizer states to be loaded.
is_named_optimizer () – Is this optimizer a NamedOptimizer or
KeyedOptimizer. Only set to True if  is TorchRec’s
KeyedOptimizer or torch.distributed’s NamedOptimizer.
load_directly () – If this is set to True, this API will also
call optim.load_state_dict(result) before returning the result.
Otherwise, users are responsible to call optim.load_state_dict()
(Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )


Return type
[, ]

Convert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.

================================================================================

# FullyShardedDataParallel (Part 86)

Given a optim_state_dict that is transformed through
optim_state_dict(), it gets converted to the flattened optimizer
state_dict that can be loaded to  which is the optimizer for
.  must be sharded by FullyShardedDataParallel.

Code example:
torch.distributed.fsdp  FullyShardedDataParallel  
 torch.distributed.fsdp  StateDictType
 torch.distributed.fsdp  FullStateDictConfig
 torch.distributed.fsdp  FullOptimStateDictConfig
# Save a checkpoint
   
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

state_dict  state_dict
original_osd  state_dict
optim_state_dict  optim_state_dict
    
    
    optim_state_dictoriginal_osd

save_a_checkpointstate_dict optim_state_dict
# Load a checkpoint
   
state_dict optim_state_dict  load_a_checkpoint
set_state_dict_type
    
    StateDictTypeFULL_STATE_DICT
    FullStateDictConfigrank0_only
    FullOptimStateDictConfigrank0_only

load_state_dictstate_dict
optim_state_dict  optim_state_dict_to_load
      optim_state_dict

load_state_dictoptim_state_dict

Parameters

================================================================================

# FullyShardedDataParallel (Part 87)

 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – The optimizer states to be loaded.
is_named_optimizer () – Is this optimizer a NamedOptimizer or
KeyedOptimizer. Only set to True if  is TorchRec’s
KeyedOptimizer or torch.distributed’s NamedOptimizer.
load_directly () – If this is set to True, this API will also
call optim.load_state_dict(result) before returning the result.
Otherwise, users are responsible to call optim.load_state_dict()
(Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )


Return type
[, ]

================================================================================

# FullyShardedDataParallel (Part 88)

List:
(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .
 (torch.optim.Optimizer) – Optimizer for  ‘s
parameters.
optim_state_dict () – The optimizer states to be loaded.
is_named_optimizer () – Is this optimizer a NamedOptimizer or
KeyedOptimizer. Only set to True if  is TorchRec’s
KeyedOptimizer or torch.distributed’s NamedOptimizer.
load_directly () – If this is set to True, this API will also
call optim.load_state_dict(result) before returning the result.
Otherwise, users are responsible to call optim.load_state_dict()
(Default: )
 (dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )

(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
were passed into the optimizer .

(torch.optim.Optimizer) – Optimizer for  ‘s
parameters.

optim_state_dict () – The optimizer states to be loaded.

is_named_optimizer () – Is this optimizer a NamedOptimizer or
KeyedOptimizer. Only set to True if  is TorchRec’s
KeyedOptimizer or torch.distributed’s NamedOptimizer.

================================================================================

# FullyShardedDataParallel (Part 89)

load_directly () – If this is set to True, this API will also
call optim.load_state_dict(result) before returning the result.
Otherwise, users are responsible to call optim.load_state_dict()
(Default: )

(dist.ProcessGroup) – Model’s process group across which parameters
are sharded or  if using the default process group. (
Default: )

register_comm_hook, 
Register a communication hook.
This is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates
gradients across multiple workers.
This hook can be used to implement several algorithms like
GossipGrad and gradient compression
which involve different communication strategies for
parameter syncs while training with FullyShardedDataParallel.


FSDP communication hook should be registered before running an initial forward pass
and only once.


Parameters

 () – Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in GossipGrad, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.

================================================================================

# FullyShardedDataParallel (Part 90)

 () – Callable, which has one of the following signatures:
1)  Callable[torch.Tensor]  :
This function takes in a Python tensor, which represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units).
It then performs all necessary processing and returns ;
2)  Callable[torch.Tensor, torch.Tensor]  :
This function takes in two Python tensors, the first one represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units). The latter
represents a pre-sized tensor to store a chunk of a sharded gradient after
reduction.
In both cases, callable performs all necessary processing and returns .
Callables with signature 1 are expected to handle gradient communication for a  case.
Callables with signature 2 are expected to handle gradient communication for sharded cases.

Register a communication hook.

================================================================================

# FullyShardedDataParallel (Part 91)

This is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates
gradients across multiple workers.
This hook can be used to implement several algorithms like
GossipGrad and gradient compression
which involve different communication strategies for
parameter syncs while training with FullyShardedDataParallel.

FSDP communication hook should be registered before running an initial forward pass
and only once.

Parameters

 () – Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in GossipGrad, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.

================================================================================

# FullyShardedDataParallel (Part 92)

 () – Callable, which has one of the following signatures:
1)  Callable[torch.Tensor]  :
This function takes in a Python tensor, which represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units).
It then performs all necessary processing and returns ;
2)  Callable[torch.Tensor, torch.Tensor]  :
This function takes in two Python tensors, the first one represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units). The latter
represents a pre-sized tensor to store a chunk of a sharded gradient after
reduction.
In both cases, callable performs all necessary processing and returns .
Callables with signature 1 are expected to handle gradient communication for a  case.
Callables with signature 2 are expected to handle gradient communication for sharded cases.

================================================================================

# FullyShardedDataParallel (Part 93)

List:
() – Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in GossipGrad, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.

================================================================================

# FullyShardedDataParallel (Part 94)

 () – Callable, which has one of the following signatures:
1)  Callable[torch.Tensor]  :
This function takes in a Python tensor, which represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units).
It then performs all necessary processing and returns ;
2)  Callable[torch.Tensor, torch.Tensor]  :
This function takes in two Python tensors, the first one represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units). The latter
represents a pre-sized tensor to store a chunk of a sharded gradient after
reduction.
In both cases, callable performs all necessary processing and returns .
Callables with signature 1 are expected to handle gradient communication for a  case.
Callables with signature 2 are expected to handle gradient communication for sharded cases.

================================================================================

# FullyShardedDataParallel (Part 95)

() – Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in GossipGrad, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.

Passed to the hook to maintain any state information during the training process.
Examples include error feedback in gradient compression,
peers to communicate with next in GossipGrad, etc.
It is locally stored by each worker
and shared by all the gradient tensors on the worker.

================================================================================

# FullyShardedDataParallel (Part 96)

() – Callable, which has one of the following signatures:
1)  Callable[torch.Tensor]  :
This function takes in a Python tensor, which represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units).
It then performs all necessary processing and returns ;
2)  Callable[torch.Tensor, torch.Tensor]  :
This function takes in two Python tensors, the first one represents
the full, flattened, unsharded gradient with respect to all variables
corresponding to the model this FSDP unit is wrapping
(that are not wrapped by other FSDP sub-units). The latter
represents a pre-sized tensor to store a chunk of a sharded gradient after
reduction.
In both cases, callable performs all necessary processing and returns .
Callables with signature 1 are expected to handle gradient communication for a  case.
Callables with signature 2 are expected to handle gradient communication for sharded cases.

================================================================================

# FullyShardedDataParallel (Part 97)

rekey_optim_state_dictoptim_state_dict, optim_state_key_type, , optim_input, 
Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type.
This can be used to achieve compatibility between optimizer state dicts from models with FSDP
instances and ones without.
To re-key an FSDP full optimizer state dict (i.e. from
full_optim_state_dict()) to use parameter IDs and be loadable to
a non-wrapped model:
wrapped_model wrapped_optim  
  full_optim_state_dictwrapped_model wrapped_optim
nonwrapped_model nonwrapped_optim  
rekeyed_osd  rekey_optim_state_dict OptimStateKeyType nonwrapped_model
nonwrapped_optimload_state_dictrekeyed_osd


To re-key a normal optimizer state dict from a non-wrapped model to be
loadable to a wrapped model:
nonwrapped_model nonwrapped_optim  
  nonwrapped_optimstate_dict
rekeyed_osd  rekey_optim_state_dict OptimStateKeyTypePARAM_NAME nonwrapped_model
wrapped_model wrapped_optim  
sharded_osd  shard_full_optim_state_dictrekeyed_osd wrapped_model
wrapped_optimload_state_dictsharded_osd




The optimizer state dict re-keyed using the
parameter keys specified by optim_state_key_type.

Return type
Dict[, Any]

================================================================================

# FullyShardedDataParallel (Part 98)

Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type.

This can be used to achieve compatibility between optimizer state dicts from models with FSDP
instances and ones without.

To re-key an FSDP full optimizer state dict (i.e. from
full_optim_state_dict()) to use parameter IDs and be loadable to
a non-wrapped model:

Code example:
wrapped_model wrapped_optim  
  full_optim_state_dictwrapped_model wrapped_optim
nonwrapped_model nonwrapped_optim  
rekeyed_osd  rekey_optim_state_dict OptimStateKeyType nonwrapped_model
nonwrapped_optimload_state_dictrekeyed_osd

To re-key a normal optimizer state dict from a non-wrapped model to be
loadable to a wrapped model:

Code example:
nonwrapped_model nonwrapped_optim  
  nonwrapped_optimstate_dict
rekeyed_osd  rekey_optim_state_dict OptimStateKeyTypePARAM_NAME nonwrapped_model
wrapped_model wrapped_optim  
sharded_osd  shard_full_optim_state_dictrekeyed_osd wrapped_model
wrapped_optimload_state_dictsharded_osd

The optimizer state dict re-keyed using the
parameter keys specified by optim_state_key_type.

Return type
Dict[, Any]

The optimizer state dict re-keyed using the
parameter keys specified by optim_state_key_type.

================================================================================

# FullyShardedDataParallel (Part 99)

scatter_full_optim_state_dictfull_optim_state_dict, , optim_input, , 
Scatter the full optimizer state dict from rank 0 to all other ranks.
Returns the sharded optimizer state dict on each rank.
The return value is the same as shard_full_optim_state_dict(), and on rank
0, the first argument should be the return value of
full_optim_state_dict().

 torch.distributed.fsdp  FullyShardedDataParallel  
   
  full_optim_state_dict   # only non-empty on rank 0
# Define new model with possibly different world size
    
sharded_osd  scatter_full_optim_state_dict  
load_state_dictsharded_osd



================================================================================

# FullyShardedDataParallel (Part 100)


Both shard_full_optim_state_dict() and
scatter_full_optim_state_dict() may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.


Parameters

================================================================================

# FullyShardedDataParallel (Part 101)

full_optim_state_dict () – Optimizer state
dict corresponding to the unflattened parameters and holding
the full non-sharded optimizer state if on rank 0; the argument
is ignored on nonzero ranks.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )
 (dist.ProcessGroup) – Model’s process group or  if
using the default process group. (Default: )



The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

Return type
Dict[, Any]

Scatter the full optimizer state dict from rank 0 to all other ranks.

================================================================================

# FullyShardedDataParallel (Part 102)

Returns the sharded optimizer state dict on each rank.
The return value is the same as shard_full_optim_state_dict(), and on rank
0, the first argument should be the return value of
full_optim_state_dict().

Code example:
torch.distributed.fsdp  FullyShardedDataParallel  
   
  full_optim_state_dict   # only non-empty on rank 0
# Define new model with possibly different world size
    
sharded_osd  scatter_full_optim_state_dict  
load_state_dictsharded_osd

Both shard_full_optim_state_dict() and
scatter_full_optim_state_dict() may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.

Parameters

================================================================================

# FullyShardedDataParallel (Part 103)

full_optim_state_dict () – Optimizer state
dict corresponding to the unflattened parameters and holding
the full non-sharded optimizer state if on rank 0; the argument
is ignored on nonzero ranks.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )
 (dist.ProcessGroup) – Model’s process group or  if
using the default process group. (Default: )



The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

Return type
Dict[, Any]

================================================================================

# FullyShardedDataParallel (Part 104)

List:
full_optim_state_dict () – Optimizer state
dict corresponding to the unflattened parameters and holding
the full non-sharded optimizer state if on rank 0; the argument
is ignored on nonzero ranks.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )
 (dist.ProcessGroup) – Model’s process group or  if
using the default process group. (Default: )

full_optim_state_dict () – Optimizer state
dict corresponding to the unflattened parameters and holding
the full non-sharded optimizer state if on rank 0; the argument
is ignored on nonzero ranks.

================================================================================

# FullyShardedDataParallel (Part 105)

(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.

optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )

(torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )

(dist.ProcessGroup) – Model’s process group or  if
using the default process group. (Default: )

The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

================================================================================

# FullyShardedDataParallel (Part 106)

set_state_dict_type, state_dict_type, state_dict_config, optim_state_dict_config
Set the state_dict_type of all the descendant FSDP modules of the target module.
Also takes (optional) configuration for the model’s and optimizer’s state dict.
The target module does not have to be a FSDP module. If the target
module is a FSDP module, its state_dict_type will also be changed.


This API should be called for only the top-level (root)
module.



This API enables users to transparently use the conventional
state_dict API to take model checkpoints in cases where the
root FSDP module is wrapped by another . For example,
the following will ensure state_dict is called on all non-FSDP
instances, while dispatching into sharded_state_dict implementation
for FSDP:


  
set_state_dict_type
    
    StateDictTypeSHARDED_STATE_DICT
    state_dict_config  ShardedStateDictConfigoffload_to_cpu
    optim_state_dict_config  OptimStateDictConfigoffload_to_cpu

param_state_dict  state_dict
optim_state_dict  optim_state_dict 



Parameters

================================================================================

# FullyShardedDataParallel (Part 107)

 (torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the configuration for the
target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the configuration
for the optimizer state dict.



A StateDictSettings that include the previous state_dict type and
configuration for the module.

Return type
StateDictSettings

Set the state_dict_type of all the descendant FSDP modules of the target module.

Also takes (optional) configuration for the model’s and optimizer’s state dict.
The target module does not have to be a FSDP module. If the target
module is a FSDP module, its state_dict_type will also be changed.

This API should be called for only the top-level (root)
module.

This API enables users to transparently use the conventional
state_dict API to take model checkpoints in cases where the
root FSDP module is wrapped by another . For example,
the following will ensure state_dict is called on all non-FSDP
instances, while dispatching into sharded_state_dict implementation
for FSDP:

================================================================================

# FullyShardedDataParallel (Part 108)

Code example:
set_state_dict_type
    
    StateDictTypeSHARDED_STATE_DICT
    state_dict_config  ShardedStateDictConfigoffload_to_cpu
    optim_state_dict_config  OptimStateDictConfigoffload_to_cpu

param_state_dict  state_dict
optim_state_dict  optim_state_dict

Parameters

 (torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the configuration for the
target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the configuration
for the optimizer state dict.



A StateDictSettings that include the previous state_dict type and
configuration for the module.

Return type
StateDictSettings

List:
(torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the configuration for the
target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the configuration
for the optimizer state dict.

(torch.nn.Module) – Root module.

state_dict_type (StateDictType) – the desired state_dict_type to set.

state_dict_config (StateDictConfig) – the configuration for the
target state_dict_type.

================================================================================

# FullyShardedDataParallel (Part 109)

optim_state_dict_config (OptimStateDictConfig) – the configuration
for the optimizer state dict.

A StateDictSettings that include the previous state_dict type and
configuration for the module.

shard_full_optim_state_dictfull_optim_state_dict, , optim_input, 
Shard a full optimizer state-dict.
Remaps the state in full_optim_state_dict to flattened parameters instead of unflattened
parameters and restricts to only this rank’s part of the optimizer state.
The first argument should be the return value of full_optim_state_dict().

 torch.distributed.fsdp  FullyShardedDataParallel  
   
  full_optim_state_dict 
 
# Define new model with possibly different world size
   
  
sharded_osd  shard_full_optim_state_dict 
load_state_dictsharded_osd



================================================================================

# FullyShardedDataParallel (Part 110)


Both shard_full_optim_state_dict() and
scatter_full_optim_state_dict() may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.


Parameters

================================================================================

# FullyShardedDataParallel (Part 111)

full_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
full non-sharded optimizer state.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )



The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

Return type
Dict[, Any]

Shard a full optimizer state-dict.

================================================================================

# FullyShardedDataParallel (Part 112)

Remaps the state in full_optim_state_dict to flattened parameters instead of unflattened
parameters and restricts to only this rank’s part of the optimizer state.
The first argument should be the return value of full_optim_state_dict().

Code example:
torch.distributed.fsdp  FullyShardedDataParallel  
   
  full_optim_state_dict 
 
# Define new model with possibly different world size
   
  
sharded_osd  shard_full_optim_state_dict 
load_state_dictsharded_osd

Both shard_full_optim_state_dict() and
scatter_full_optim_state_dict() may be used to get the
sharded optimizer state dict to load. Assuming that the full
optimizer state dict resides in CPU memory, the former requires
each rank to have the full dict in CPU memory, where each rank
individually shards the dict without any communication, while the
latter requires only rank 0 to have the full dict in CPU memory,
where rank 0 moves each shard to GPU memory (for NCCL) and
communicates it to ranks appropriately. Hence, the former has
higher aggregate CPU memory cost, while the latter has higher
communication cost.

Parameters

================================================================================

# FullyShardedDataParallel (Part 113)

full_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
full non-sharded optimizer state.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )



The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

Return type
Dict[, Any]

================================================================================

# FullyShardedDataParallel (Part 114)

List:
full_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
full non-sharded optimizer state.
 (torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.
optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )
 (torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )

full_optim_state_dict () – Optimizer state dict
corresponding to the unflattened parameters and holding the
full non-sharded optimizer state.

(torch.nn.Module) – Root module (which may or may not be a
FullyShardedDataParallel instance) whose parameters
correspond to the optimizer state in full_optim_state_dict.

================================================================================

# FullyShardedDataParallel (Part 115)

optim_input (torch.nn.Parameter) – Input passed into the optimizer representing either a
 of parameter groups or an iterable of parameters;
if , then this method assumes the input was
model.parameters(). This argument is deprecated, and there
is no need to pass it in anymore. (Default: )

(torch.optim.Optimizer) – Optimizer that will load
the state dict returned by this method. This is the preferred
argument to use over optim_input. (Default: )

The full optimizer state dict now remapped to
flattened parameters instead of unflattened parameters and
restricted to only include this rank’s part of the optimizer state.

sharded_optim_state_dict, , 
Return the optimizer state-dict in its sharded form.
The API is similar to full_optim_state_dict() but this API chunks
all non-zero-dimension states to ShardedTensor to save memory.
This API should only be used when the model state_dict is derived
with the context manager  state_dict_type(SHARDED_STATE_DICT):.
For the detailed usage, refer to full_optim_state_dict().


The returned state dict contains ShardedTensor and
cannot be directly used by the regular optim.load_state_dict.


Return type
[, ]

Return the optimizer state-dict in its sharded form.

================================================================================

# FullyShardedDataParallel (Part 116)

The API is similar to full_optim_state_dict() but this API chunks
all non-zero-dimension states to ShardedTensor to save memory.
This API should only be used when the model state_dict is derived
with the context manager  state_dict_type(SHARDED_STATE_DICT):.

For the detailed usage, refer to full_optim_state_dict().

The returned state dict contains ShardedTensor and
cannot be directly used by the regular optim.load_state_dict.

state_dict_type, state_dict_type, state_dict_config, optim_state_dict_config
Set the state_dict_type of all the descendant FSDP modules of the target module.
This context manager has the same functions as set_state_dict_type(). Read the document of
set_state_dict_type() for the detail.

  
 state_dict_type
    
    StateDictTypeSHARDED_STATE_DICT

    checkpoint  state_dict



Parameters

 (torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the model state_dict
configuration for the target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the optimizer
state_dict configuration for the target state_dict_type.


Return type

================================================================================

# FullyShardedDataParallel (Part 117)

Set the state_dict_type of all the descendant FSDP modules of the target module.

This context manager has the same functions as set_state_dict_type(). Read the document of
set_state_dict_type() for the detail.

Code example:
state_dict_type
    
    StateDictTypeSHARDED_STATE_DICT

    checkpoint  state_dict

Parameters

 (torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the model state_dict
configuration for the target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the optimizer
state_dict configuration for the target state_dict_type.


Return type

List:
(torch.nn.Module) – Root module.
state_dict_type (StateDictType) – the desired state_dict_type to set.
state_dict_config (StateDictConfig) – the model state_dict
configuration for the target state_dict_type.
optim_state_dict_config (OptimStateDictConfig) – the optimizer
state_dict configuration for the target state_dict_type.

(torch.nn.Module) – Root module.

state_dict_type (StateDictType) – the desired state_dict_type to set.

state_dict_config (StateDictConfig) – the model state_dict
configuration for the target state_dict_type.

================================================================================

# FullyShardedDataParallel (Part 118)

optim_state_dict_config (OptimStateDictConfig) – the optimizer
state_dict configuration for the target state_dict_type.

summon_full_params, , , rank0_only, offload_to_cpu, with_grads
Expose full params for FSDP instances with this context manager.
Can be useful  forward/backward for a model to get
the params for additional processing or checking. It can take a non-FSDP
module and will summon full params for all contained FSDP modules as
well as their children, depending on the  argument.


This can be used on inner FSDPs.



This can  be used within a forward or backward pass. Nor
can forward and backward be started from within this context.



Parameters will revert to their local shards after the context
manager exits, storage behavior is the same as forward.



The full parameters can be modified, but only the portion
corresponding to the local param shard will persist after the
context manager exits (unless writeback=False, in which case
changes will be discarded). In the case where FSDP does not shard
the parameters, currently only when world_size  , or 
config, the modification is persisted regardless of .



================================================================================

# FullyShardedDataParallel (Part 119)

This method works on modules which are not FSDP themselves but
may contain multiple independent FSDP units. In that case, the given
arguments will apply to all contained FSDP units.



Note that rank0_only=True in conjunction with
writeback=True is not currently supported and will raise an
error. This is because model parameter shapes would be different
across ranks within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.



Note that offload_to_cpu and rank0_only=False will
result in full parameters being redundantly copied to CPU memory for
GPUs that reside on the same machine, which may incur the risk of
CPU OOM. It is recommended to use offload_to_cpu with
rank0_only=True.


Parameters

================================================================================

# FullyShardedDataParallel (Part 120)

 () – recursively summon all params for nested
FSDP instances (default: True).
 () – if , modifications to params are
discarded after the context manager exits;
disabling this can be slightly more efficient (default: True)
rank0_only () – if , full parameters are
materialized on only global rank 0. This means that within the
context, only rank 0 will have full parameters and the other
ranks will have sharded parameters. Note that setting
rank0_only=True with writeback=True is not supported,
as model parameter shapes will be different across ranks
within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.
offload_to_cpu () – If , full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
for world_size = 1 or  config). It is recommended
to use offload_to_cpu with rank0_only=True to avoid
redundant copies of model parameters being offloaded to the same CPU memory.
with_grads () – If , gradients are also
unsharded with the parameters. Currently, this is only
supported when passing use_orig_params=True to the FSDP
constructor and offload_to_cpu=False to this method.
(Default: )

================================================================================

# FullyShardedDataParallel (Part 121)


Return type

Expose full params for FSDP instances with this context manager.

Can be useful  forward/backward for a model to get
the params for additional processing or checking. It can take a non-FSDP
module and will summon full params for all contained FSDP modules as
well as their children, depending on the  argument.

This can be used on inner FSDPs.

This can  be used within a forward or backward pass. Nor
can forward and backward be started from within this context.

Parameters will revert to their local shards after the context
manager exits, storage behavior is the same as forward.

The full parameters can be modified, but only the portion
corresponding to the local param shard will persist after the
context manager exits (unless writeback=False, in which case
changes will be discarded). In the case where FSDP does not shard
the parameters, currently only when world_size  , or 
config, the modification is persisted regardless of .

This method works on modules which are not FSDP themselves but
may contain multiple independent FSDP units. In that case, the given
arguments will apply to all contained FSDP units.

================================================================================

# FullyShardedDataParallel (Part 122)

Note that rank0_only=True in conjunction with
writeback=True is not currently supported and will raise an
error. This is because model parameter shapes would be different
across ranks within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.

Note that offload_to_cpu and rank0_only=False will
result in full parameters being redundantly copied to CPU memory for
GPUs that reside on the same machine, which may incur the risk of
CPU OOM. It is recommended to use offload_to_cpu with
rank0_only=True.

Parameters

================================================================================

# FullyShardedDataParallel (Part 123)

 () – recursively summon all params for nested
FSDP instances (default: True).
 () – if , modifications to params are
discarded after the context manager exits;
disabling this can be slightly more efficient (default: True)
rank0_only () – if , full parameters are
materialized on only global rank 0. This means that within the
context, only rank 0 will have full parameters and the other
ranks will have sharded parameters. Note that setting
rank0_only=True with writeback=True is not supported,
as model parameter shapes will be different across ranks
within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.
offload_to_cpu () – If , full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
for world_size = 1 or  config). It is recommended
to use offload_to_cpu with rank0_only=True to avoid
redundant copies of model parameters being offloaded to the same CPU memory.
with_grads () – If , gradients are also
unsharded with the parameters. Currently, this is only
supported when passing use_orig_params=True to the FSDP
constructor and offload_to_cpu=False to this method.
(Default: )

================================================================================

# FullyShardedDataParallel (Part 124)


Return type

================================================================================

# FullyShardedDataParallel (Part 125)

List:
() – recursively summon all params for nested
FSDP instances (default: True).
 () – if , modifications to params are
discarded after the context manager exits;
disabling this can be slightly more efficient (default: True)
rank0_only () – if , full parameters are
materialized on only global rank 0. This means that within the
context, only rank 0 will have full parameters and the other
ranks will have sharded parameters. Note that setting
rank0_only=True with writeback=True is not supported,
as model parameter shapes will be different across ranks
within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.
offload_to_cpu () – If , full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
for world_size = 1 or  config). It is recommended
to use offload_to_cpu with rank0_only=True to avoid
redundant copies of model parameters being offloaded to the same CPU memory.
with_grads () – If , gradients are also
unsharded with the parameters. Currently, this is only
supported when passing use_orig_params=True to the FSDP
constructor and offload_to_cpu=False to this method.
(Default: )

================================================================================

# FullyShardedDataParallel (Part 126)

() – recursively summon all params for nested
FSDP instances (default: True).

() – if , modifications to params are
discarded after the context manager exits;
disabling this can be slightly more efficient (default: True)

rank0_only () – if , full parameters are
materialized on only global rank 0. This means that within the
context, only rank 0 will have full parameters and the other
ranks will have sharded parameters. Note that setting
rank0_only=True with writeback=True is not supported,
as model parameter shapes will be different across ranks
within the context, and writing to them can lead to
inconsistency across ranks when the context is exited.

offload_to_cpu () – If , full parameters are
offloaded to CPU. Note that this offloading currently only
occurs if the parameter is sharded (which is only not the case
for world_size = 1 or  config). It is recommended
to use offload_to_cpu with rank0_only=True to avoid
redundant copies of model parameters being offloaded to the same CPU memory.

================================================================================

# FullyShardedDataParallel (Part 127)

with_grads () – If , gradients are also
unsharded with the parameters. Currently, this is only
supported when passing use_orig_params=True to the FSDP
constructor and offload_to_cpu=False to this method.
(Default: )

torch.distributed.fsdp.BackwardPrefetch
This configures explicit backward prefetching, which improves throughput by
enabling communication and computation overlap in the backward pass at the
cost of slightly increased memory usage.

================================================================================

# FullyShardedDataParallel (Part 128)

BACKWARD_PRE: This enables the most overlap but increases memory
usage the most. This prefetches the next set of parameters  the
current set of parameters’ gradient computation. This overlaps the next
all-gather and the current gradient computation, and at the peak, it
holds the current set of parameters, next set of parameters, and current
set of gradients in memory.
BACKWARD_POST: This enables less overlap but requires less memory
usage. This prefetches the next set of parameters  the current
set of parameters’ gradient computation. This overlaps the current
reduce-scatter and the next gradient computation, and it frees the
current set of parameters before allocating memory for the next set of
parameters, only holding the next set of parameters and current set of
gradients in memory at the peak.
FSDP’s backward_prefetch argument accepts , which disables
the backward prefetching altogether. This has no overlap and does not
increase memory usage. In general, we do not recommend this setting since
it may degrade throughput significantly.

================================================================================

# FullyShardedDataParallel (Part 129)

For more technical context: For a single process group using NCCL backend,
any collectives, even if issued from different streams, contend for the
same per-device NCCL stream, which implies that the relative order in which
the collectives are issued matters for overlapping. The two backward
prefetching values correspond to different issue orders.

This configures explicit backward prefetching, which improves throughput by
enabling communication and computation overlap in the backward pass at the
cost of slightly increased memory usage.

================================================================================

# FullyShardedDataParallel (Part 130)

List:
BACKWARD_PRE: This enables the most overlap but increases memory
usage the most. This prefetches the next set of parameters  the
current set of parameters’ gradient computation. This overlaps the next
all-gather and the current gradient computation, and at the peak, it
holds the current set of parameters, next set of parameters, and current
set of gradients in memory.
BACKWARD_POST: This enables less overlap but requires less memory
usage. This prefetches the next set of parameters  the current
set of parameters’ gradient computation. This overlaps the current
reduce-scatter and the next gradient computation, and it frees the
current set of parameters before allocating memory for the next set of
parameters, only holding the next set of parameters and current set of
gradients in memory at the peak.
FSDP’s backward_prefetch argument accepts , which disables
the backward prefetching altogether. This has no overlap and does not
increase memory usage. In general, we do not recommend this setting since
it may degrade throughput significantly.

================================================================================

# FullyShardedDataParallel (Part 131)

BACKWARD_PRE: This enables the most overlap but increases memory
usage the most. This prefetches the next set of parameters  the
current set of parameters’ gradient computation. This overlaps the next
all-gather and the current gradient computation, and at the peak, it
holds the current set of parameters, next set of parameters, and current
set of gradients in memory.

BACKWARD_POST: This enables less overlap but requires less memory
usage. This prefetches the next set of parameters  the current
set of parameters’ gradient computation. This overlaps the current
reduce-scatter and the next gradient computation, and it frees the
current set of parameters before allocating memory for the next set of
parameters, only holding the next set of parameters and current set of
gradients in memory at the peak.

FSDP’s backward_prefetch argument accepts , which disables
the backward prefetching altogether. This has no overlap and does not
increase memory usage. In general, we do not recommend this setting since
it may degrade throughput significantly.

================================================================================

# FullyShardedDataParallel (Part 132)

For more technical context: For a single process group using NCCL backend,
any collectives, even if issued from different streams, contend for the
same per-device NCCL stream, which implies that the relative order in which
the collectives are issued matters for overlapping. The two backward
prefetching values correspond to different issue orders.

torch.distributed.fsdp.ShardingStrategy
This specifies the sharding strategy to be used for distributed training by
FullyShardedDataParallel.

================================================================================

# FullyShardedDataParallel (Part 133)

FULL_SHARD: Parameters, gradients, and optimizer states are sharded.
For the parameters, this strategy unshards (via all-gather) before the
forward, reshards after the forward, unshards before the backward
computation, and reshards after the backward computation. For gradients,
it synchronizes and shards them (via reduce-scatter) after the backward
computation. The sharded optimizer states are updated locally per rank.
SHARD_GRAD_OP: Gradients and optimizer states are sharded during
computation, and additionally, parameters are sharded outside
computation. For the parameters, this strategy unshards before the
forward, does not reshard them after the forward, and only reshards them
after the backward computation. The sharded optimizer states are updated
locally per rank. Inside , the parameters are not resharded
after the backward computation.
: Parameters, gradients, and optimizer states are not sharded
but instead replicated across ranks similar to PyTorch’s
DistributedDataParallel API. For gradients, this strategy
synchronizes them (via all-reduce) after the backward computation. The
unsharded optimizer states are updated locally per rank.
HYBRID_SHARD: Apply FULL_SHARD within a node, and replicate parameters across
nodes. This results in reduced communication volume as expensive all-gathers and
reduce-scatters are only done within a node, which can be more performant for medium
-sized models.
_HYBRID_SHARD_ZERO2: Apply SHARD_GRAD_OP within a node, and replicate parameters across
nodes. This is like HYBRID_SHARD, except this may provide even higher throughput
since the unsharded parameters are not freed after the forward pass, saving the
all-gathers in the pre-backward.

================================================================================

# FullyShardedDataParallel (Part 134)

This specifies the sharding strategy to be used for distributed training by
FullyShardedDataParallel.

================================================================================

# FullyShardedDataParallel (Part 135)

List:
FULL_SHARD: Parameters, gradients, and optimizer states are sharded.
For the parameters, this strategy unshards (via all-gather) before the
forward, reshards after the forward, unshards before the backward
computation, and reshards after the backward computation. For gradients,
it synchronizes and shards them (via reduce-scatter) after the backward
computation. The sharded optimizer states are updated locally per rank.
SHARD_GRAD_OP: Gradients and optimizer states are sharded during
computation, and additionally, parameters are sharded outside
computation. For the parameters, this strategy unshards before the
forward, does not reshard them after the forward, and only reshards them
after the backward computation. The sharded optimizer states are updated
locally per rank. Inside , the parameters are not resharded
after the backward computation.
: Parameters, gradients, and optimizer states are not sharded
but instead replicated across ranks similar to PyTorch’s
DistributedDataParallel API. For gradients, this strategy
synchronizes them (via all-reduce) after the backward computation. The
unsharded optimizer states are updated locally per rank.
HYBRID_SHARD: Apply FULL_SHARD within a node, and replicate parameters across
nodes. This results in reduced communication volume as expensive all-gathers and
reduce-scatters are only done within a node, which can be more performant for medium
-sized models.
_HYBRID_SHARD_ZERO2: Apply SHARD_GRAD_OP within a node, and replicate parameters across
nodes. This is like HYBRID_SHARD, except this may provide even higher throughput
since the unsharded parameters are not freed after the forward pass, saving the
all-gathers in the pre-backward.

================================================================================

# FullyShardedDataParallel (Part 136)

FULL_SHARD: Parameters, gradients, and optimizer states are sharded.
For the parameters, this strategy unshards (via all-gather) before the
forward, reshards after the forward, unshards before the backward
computation, and reshards after the backward computation. For gradients,
it synchronizes and shards them (via reduce-scatter) after the backward
computation. The sharded optimizer states are updated locally per rank.

SHARD_GRAD_OP: Gradients and optimizer states are sharded during
computation, and additionally, parameters are sharded outside
computation. For the parameters, this strategy unshards before the
forward, does not reshard them after the forward, and only reshards them
after the backward computation. The sharded optimizer states are updated
locally per rank. Inside , the parameters are not resharded
after the backward computation.

: Parameters, gradients, and optimizer states are not sharded
but instead replicated across ranks similar to PyTorch’s
DistributedDataParallel API. For gradients, this strategy
synchronizes them (via all-reduce) after the backward computation. The
unsharded optimizer states are updated locally per rank.

================================================================================

# FullyShardedDataParallel (Part 137)

HYBRID_SHARD: Apply FULL_SHARD within a node, and replicate parameters across
nodes. This results in reduced communication volume as expensive all-gathers and
reduce-scatters are only done within a node, which can be more performant for medium
-sized models.

_HYBRID_SHARD_ZERO2: Apply SHARD_GRAD_OP within a node, and replicate parameters across
nodes. This is like HYBRID_SHARD, except this may provide even higher throughput
since the unsharded parameters are not freed after the forward pass, saving the
all-gathers in the pre-backward.

torch.distributed.fsdp.MixedPrecisionparam_dtype=None, reduce_dtype=None, buffer_dtype=None, keep_low_precision_grads=False, cast_forward_inputs=False, cast_root_forward_inputs=True, _module_classes_to_ignore=(<class 'torch.nn.modules.batchnorm._BatchNorm'>, 
This configures FSDP-native mixed precision training.



================================================================================

# FullyShardedDataParallel (Part 138)

param_dtype (torch.dtype) – This specifies the dtype for model
parameters during forward and backward and thus the dtype for
forward and backward computation. Outside forward and backward, the
 parameters are kept in full precision (e.g. for the
optimizer step), and for model checkpointing, the parameters are
always saved in full precision. (Default: )
reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then this takes on
the param_dtype value, still running gradient reduction in low
precision. This is permitted to differ from param_dtype, e.g.
to force gradient reduction to run in full precision. (Default:
)
buffer_dtype (torch.dtype) – This specifies the dtype for
buffers. FSDP does not shard buffers. Rather, FSDP casts them to
buffer_dtype in the first forward pass and keeps them in that
dtype thereafter. For model checkpointing, the buffers are saved
in full precision except for LOCAL_STATE_DICT. (Default:
)
keep_low_precision_grads () – If , then FSDP upcasts
gradients to full precision after the backward pass in preparation
for the optimizer step. If , then FSDP keeps the gradients
in the dtype used for gradient reduction, which can save memory if
using a custom optimizer that supports running in low precision.
(Default: )
cast_forward_inputs () – If , then this FSDP module casts
its forward args and kwargs to param_dtype. This is to ensure
that parameter and input dtypes match for forward computation, as
required by many ops. This may need to be set to  when only
applying mixed precision to some but not all FSDP modules, in which
case a mixed-precision FSDP submodule needs to recast its inputs.
(Default: )
cast_root_forward_inputs () – If , then the root FSDP module
casts its forward args and kwargs to param_dtype, overriding
the value of cast_forward_inputs. For non-root FSDP modules,
this does not do anything. (Default: )
_module_classes_to_ignore (collections.abc.Sequencetorch.nn.modules.module.Module) – (Sequence[Type[nn.Module]]): This specifies
module classes to ignore for mixed precision when using an
auto_wrap_policy: Modules of these classes will have FSDP
applied to them separately with mixed precision disabled (meaning
that the final FSDP construction would deviate from the specified
policy). If auto_wrap_policy is not specified, then this does
not do anything. This API is experimental and subject to change.
(Default: (_BatchNorm,))

================================================================================

# FullyShardedDataParallel (Part 139)





This API is experimental and subject to change.



Only floating point tensors are cast to their specified dtypes.



In summon_full_params, parameters are forced to full
precision, but buffers are not.



Layer norm and batch norm accumulate in  even when
their inputs are in a low precision like  or .
Disabling FSDP’s mixed precision for those norm modules only means that
the affine parameters are kept in . However, this incurs
separate all-gathers and reduce-scatters for those norm modules, which
may be inefficient, so if the workload permits, the user should prefer
to still apply mixed precision to those modules.



By default, if the user passes a model with any _BatchNorm
modules and specifies an auto_wrap_policy, then the batch norm
modules will have FSDP applied to them separately with mixed precision
disabled. See the _module_classes_to_ignore argument.



================================================================================

# FullyShardedDataParallel (Part 140)

MixedPrecision has cast_root_forward_inputs=True and
cast_forward_inputs=False by default. For the root FSDP instance,
its cast_root_forward_inputs takes precedence over its
cast_forward_inputs. For non-root FSDP instances, their
cast_root_forward_inputs values are ignored. The default setting is
sufficient for the typical case where each FSDP instance has the same
MixedPrecision configuration and only needs to cast inputs to the
param_dtype at the beginning of the model’s forward pass.



For nested FSDP instances with different MixedPrecision
configurations, we recommend setting individual cast_forward_inputs
values to configure casting inputs or not before each instance’s
forward. In such a case, since the casts happen before each FSDP
instance’s forward, a parent FSDP instance should have its non-FSDP
submodules run before its FSDP submodules to avoid the activation dtype
being changed due to a different MixedPrecision configuration.

  Sequential   
  
    
    mixed_precisionMixedPrecisionparam_dtype cast_forward_inputs

  
    
    mixed_precisionMixedPrecisionparam_dtype cast_forward_inputs



================================================================================

# FullyShardedDataParallel (Part 141)

The above shows a working example. On the other hand, if 
were replaced with , meaning that the submodule using
different MixedPrecision ran its forward first, then 
would incorrectly see  activations instead of 
ones.

This configures FSDP-native mixed precision training.

================================================================================

# FullyShardedDataParallel (Part 142)

param_dtype (torch.dtype) – This specifies the dtype for model
parameters during forward and backward and thus the dtype for
forward and backward computation. Outside forward and backward, the
 parameters are kept in full precision (e.g. for the
optimizer step), and for model checkpointing, the parameters are
always saved in full precision. (Default: )
reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then this takes on
the param_dtype value, still running gradient reduction in low
precision. This is permitted to differ from param_dtype, e.g.
to force gradient reduction to run in full precision. (Default:
)
buffer_dtype (torch.dtype) – This specifies the dtype for
buffers. FSDP does not shard buffers. Rather, FSDP casts them to
buffer_dtype in the first forward pass and keeps them in that
dtype thereafter. For model checkpointing, the buffers are saved
in full precision except for LOCAL_STATE_DICT. (Default:
)
keep_low_precision_grads () – If , then FSDP upcasts
gradients to full precision after the backward pass in preparation
for the optimizer step. If , then FSDP keeps the gradients
in the dtype used for gradient reduction, which can save memory if
using a custom optimizer that supports running in low precision.
(Default: )
cast_forward_inputs () – If , then this FSDP module casts
its forward args and kwargs to param_dtype. This is to ensure
that parameter and input dtypes match for forward computation, as
required by many ops. This may need to be set to  when only
applying mixed precision to some but not all FSDP modules, in which
case a mixed-precision FSDP submodule needs to recast its inputs.
(Default: )
cast_root_forward_inputs () – If , then the root FSDP module
casts its forward args and kwargs to param_dtype, overriding
the value of cast_forward_inputs. For non-root FSDP modules,
this does not do anything. (Default: )
_module_classes_to_ignore (collections.abc.Sequencetorch.nn.modules.module.Module) – (Sequence[Type[nn.Module]]): This specifies
module classes to ignore for mixed precision when using an
auto_wrap_policy: Modules of these classes will have FSDP
applied to them separately with mixed precision disabled (meaning
that the final FSDP construction would deviate from the specified
policy). If auto_wrap_policy is not specified, then this does
not do anything. This API is experimental and subject to change.
(Default: (_BatchNorm,))

================================================================================

# FullyShardedDataParallel (Part 143)

List:
param_dtype (torch.dtype) – This specifies the dtype for model
parameters during forward and backward and thus the dtype for
forward and backward computation. Outside forward and backward, the
 parameters are kept in full precision (e.g. for the
optimizer step), and for model checkpointing, the parameters are
always saved in full precision. (Default: )
reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then this takes on
the param_dtype value, still running gradient reduction in low
precision. This is permitted to differ from param_dtype, e.g.
to force gradient reduction to run in full precision. (Default:
)
buffer_dtype (torch.dtype) – This specifies the dtype for
buffers. FSDP does not shard buffers. Rather, FSDP casts them to
buffer_dtype in the first forward pass and keeps them in that
dtype thereafter. For model checkpointing, the buffers are saved
in full precision except for LOCAL_STATE_DICT. (Default:
)
keep_low_precision_grads () – If , then FSDP upcasts
gradients to full precision after the backward pass in preparation
for the optimizer step. If , then FSDP keeps the gradients
in the dtype used for gradient reduction, which can save memory if
using a custom optimizer that supports running in low precision.
(Default: )
cast_forward_inputs () – If , then this FSDP module casts
its forward args and kwargs to param_dtype. This is to ensure
that parameter and input dtypes match for forward computation, as
required by many ops. This may need to be set to  when only
applying mixed precision to some but not all FSDP modules, in which
case a mixed-precision FSDP submodule needs to recast its inputs.
(Default: )
cast_root_forward_inputs () – If , then the root FSDP module
casts its forward args and kwargs to param_dtype, overriding
the value of cast_forward_inputs. For non-root FSDP modules,
this does not do anything. (Default: )
_module_classes_to_ignore (collections.abc.Sequencetorch.nn.modules.module.Module) – (Sequence[Type[nn.Module]]): This specifies
module classes to ignore for mixed precision when using an
auto_wrap_policy: Modules of these classes will have FSDP
applied to them separately with mixed precision disabled (meaning
that the final FSDP construction would deviate from the specified
policy). If auto_wrap_policy is not specified, then this does
not do anything. This API is experimental and subject to change.
(Default: (_BatchNorm,))

================================================================================

# FullyShardedDataParallel (Part 144)

param_dtype (torch.dtype) – This specifies the dtype for model
parameters during forward and backward and thus the dtype for
forward and backward computation. Outside forward and backward, the
 parameters are kept in full precision (e.g. for the
optimizer step), and for model checkpointing, the parameters are
always saved in full precision. (Default: )

reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then this takes on
the param_dtype value, still running gradient reduction in low
precision. This is permitted to differ from param_dtype, e.g.
to force gradient reduction to run in full precision. (Default:
)

buffer_dtype (torch.dtype) – This specifies the dtype for
buffers. FSDP does not shard buffers. Rather, FSDP casts them to
buffer_dtype in the first forward pass and keeps them in that
dtype thereafter. For model checkpointing, the buffers are saved
in full precision except for LOCAL_STATE_DICT. (Default:
)

================================================================================

# FullyShardedDataParallel (Part 145)

keep_low_precision_grads () – If , then FSDP upcasts
gradients to full precision after the backward pass in preparation
for the optimizer step. If , then FSDP keeps the gradients
in the dtype used for gradient reduction, which can save memory if
using a custom optimizer that supports running in low precision.
(Default: )

cast_forward_inputs () – If , then this FSDP module casts
its forward args and kwargs to param_dtype. This is to ensure
that parameter and input dtypes match for forward computation, as
required by many ops. This may need to be set to  when only
applying mixed precision to some but not all FSDP modules, in which
case a mixed-precision FSDP submodule needs to recast its inputs.
(Default: )

cast_root_forward_inputs () – If , then the root FSDP module
casts its forward args and kwargs to param_dtype, overriding
the value of cast_forward_inputs. For non-root FSDP modules,
this does not do anything. (Default: )

================================================================================

# FullyShardedDataParallel (Part 146)

_module_classes_to_ignore (collections.abc.Sequencetorch.nn.modules.module.Module) – (Sequence[Type[nn.Module]]): This specifies
module classes to ignore for mixed precision when using an
auto_wrap_policy: Modules of these classes will have FSDP
applied to them separately with mixed precision disabled (meaning
that the final FSDP construction would deviate from the specified
policy). If auto_wrap_policy is not specified, then this does
not do anything. This API is experimental and subject to change.
(Default: (_BatchNorm,))

This API is experimental and subject to change.

Only floating point tensors are cast to their specified dtypes.

In summon_full_params, parameters are forced to full
precision, but buffers are not.

Layer norm and batch norm accumulate in  even when
their inputs are in a low precision like  or .
Disabling FSDP’s mixed precision for those norm modules only means that
the affine parameters are kept in . However, this incurs
separate all-gathers and reduce-scatters for those norm modules, which
may be inefficient, so if the workload permits, the user should prefer
to still apply mixed precision to those modules.

================================================================================

# FullyShardedDataParallel (Part 147)

By default, if the user passes a model with any _BatchNorm
modules and specifies an auto_wrap_policy, then the batch norm
modules will have FSDP applied to them separately with mixed precision
disabled. See the _module_classes_to_ignore argument.

MixedPrecision has cast_root_forward_inputs=True and
cast_forward_inputs=False by default. For the root FSDP instance,
its cast_root_forward_inputs takes precedence over its
cast_forward_inputs. For non-root FSDP instances, their
cast_root_forward_inputs values are ignored. The default setting is
sufficient for the typical case where each FSDP instance has the same
MixedPrecision configuration and only needs to cast inputs to the
param_dtype at the beginning of the model’s forward pass.

For nested FSDP instances with different MixedPrecision
configurations, we recommend setting individual cast_forward_inputs
values to configure casting inputs or not before each instance’s
forward. In such a case, since the casts happen before each FSDP
instance’s forward, a parent FSDP instance should have its non-FSDP
submodules run before its FSDP submodules to avoid the activation dtype
being changed due to a different MixedPrecision configuration.

================================================================================

# FullyShardedDataParallel (Part 148)

Code example:
Sequential   
  
    
    mixed_precisionMixedPrecisionparam_dtype cast_forward_inputs

  
    
    mixed_precisionMixedPrecisionparam_dtype cast_forward_inputs

The above shows a working example. On the other hand, if 
were replaced with , meaning that the submodule using
different MixedPrecision ran its forward first, then 
would incorrectly see  activations instead of 
ones.

torch.distributed.fsdp.CPUOffloadoffload_params
This configures CPU offloading.


offload_params () – This specifies whether to offload parameters to
CPU when not involved in computation. If , then this
offloads gradients to CPU as well, meaning that the optimizer step
runs on CPU.

This configures CPU offloading.

offload_params () – This specifies whether to offload parameters to
CPU when not involved in computation. If , then this
offloads gradients to CPU as well, meaning that the optimizer step
runs on CPU.

offload_params () – This specifies whether to offload parameters to
CPU when not involved in computation. If , then this
offloads gradients to CPU as well, meaning that the optimizer step
runs on CPU.

================================================================================

# FullyShardedDataParallel (Part 149)

torch.distributed.fsdp.StateDictConfigoffload_to_cpu
StateDictConfig is the base class for all state_dict configuration
classes. Users should instantiate a child class (e.g.
FullStateDictConfig) in order to configure settings for the
corresponding state_dict type supported by FSDP.


offload_to_cpu () – If , then FSDP offloads the state dict
values to CPU, and if , then FSDP keeps them on GPU.
(Default: )

StateDictConfig is the base class for all state_dict configuration
classes. Users should instantiate a child class (e.g.
FullStateDictConfig) in order to configure settings for the
corresponding state_dict type supported by FSDP.

offload_to_cpu () – If , then FSDP offloads the state dict
values to CPU, and if , then FSDP keeps them on GPU.
(Default: )

offload_to_cpu () – If , then FSDP offloads the state dict
values to CPU, and if , then FSDP keeps them on GPU.
(Default: )

================================================================================

# FullyShardedDataParallel (Part 150)

torch.distributed.fsdp.FullStateDictConfigoffload_to_cpu, rank0_only
FullStateDictConfig is a config class meant to be used with
StateDictType.FULL_STATE_DICT. We recommend enabling both
offload_to_cpu=True and rank0_only=True when saving full state
dicts to save GPU memory and CPU memory, respectively. This config class
is meant to be used via the state_dict_type() context manager as
follows:
 torch.distributed.fsdp  FullyShardedDataParallel  
   auto_wrap_policy
  FullStateDictConfigoffload_to_cpu rank0_only
 state_dict_type StateDictTypeFULL_STATE_DICT 
      state_dict
# `state` will be empty on non rank 0 and contain CPU tensors on rank 0.
# To reload checkpoint for inference, finetuning, transfer learning, etc:
    # Initialize model in preparation for wrapping with FSDP
   
# Load checkpoint only on rank 0 to avoid memory redundancy
    state_dict  "my_checkpoint.pt"
    load_state_dictstate_dict
# All ranks initialize FSDP module as usual. `sync_module_states` argument
# communicates loaded checkpoint states from rank 0 to rest of the world.
  
    
    current_device
    auto_wrap_policy
    sync_module_states

================================================================================

# FullyShardedDataParallel (Part 151)

# After this point, all ranks have FSDP model with loaded checkpoint.




rank0_only () – If , then only rank 0 saves the full state
dict, and nonzero ranks save an empty dict. If , then all
ranks save the full state dict. (Default: )

FullStateDictConfig is a config class meant to be used with
StateDictType.FULL_STATE_DICT. We recommend enabling both
offload_to_cpu=True and rank0_only=True when saving full state
dicts to save GPU memory and CPU memory, respectively. This config class
is meant to be used via the state_dict_type() context manager as
follows:

================================================================================

# FullyShardedDataParallel (Part 152)

Code example:
torch.distributed.fsdp  FullyShardedDataParallel  
   auto_wrap_policy
  FullStateDictConfigoffload_to_cpu rank0_only
 state_dict_type StateDictTypeFULL_STATE_DICT 
      state_dict
# `state` will be empty on non rank 0 and contain CPU tensors on rank 0.
# To reload checkpoint for inference, finetuning, transfer learning, etc:
    # Initialize model in preparation for wrapping with FSDP
   
# Load checkpoint only on rank 0 to avoid memory redundancy
    state_dict  "my_checkpoint.pt"
    load_state_dictstate_dict
# All ranks initialize FSDP module as usual. `sync_module_states` argument
# communicates loaded checkpoint states from rank 0 to rest of the world.
  
    
    current_device
    auto_wrap_policy
    sync_module_states

# After this point, all ranks have FSDP model with loaded checkpoint.

rank0_only () – If , then only rank 0 saves the full state
dict, and nonzero ranks save an empty dict. If , then all
ranks save the full state dict. (Default: )

rank0_only () – If , then only rank 0 saves the full state
dict, and nonzero ranks save an empty dict. If , then all
ranks save the full state dict. (Default: )

================================================================================

# FullyShardedDataParallel (Part 153)

torch.distributed.fsdp.ShardedStateDictConfigoffload_to_cpu, _use_dtensor
ShardedStateDictConfig is a config class meant to be used with
StateDictType.SHARDED_STATE_DICT.


_use_dtensor () – If , then FSDP saves the state dict values
as , and if , then FSDP saves them as
ShardedTensor. (Default: )




_use_dtensor is a private field of ShardedStateDictConfig
and it is used by FSDP to determine the type of state dict values. Users should not
manually modify _use_dtensor.

ShardedStateDictConfig is a config class meant to be used with
StateDictType.SHARDED_STATE_DICT.

_use_dtensor () – If , then FSDP saves the state dict values
as , and if , then FSDP saves them as
ShardedTensor. (Default: )

_use_dtensor () – If , then FSDP saves the state dict values
as , and if , then FSDP saves them as
ShardedTensor. (Default: )

_use_dtensor is a private field of ShardedStateDictConfig
and it is used by FSDP to determine the type of state dict values. Users should not
manually modify _use_dtensor.

torch.distributed.fsdp.LocalStateDictConfigoffload_to_cpu

================================================================================

# FullyShardedDataParallel (Part 154)

torch.distributed.fsdp.OptimStateDictConfigoffload_to_cpu
OptimStateDictConfig is the base class for all optim_state_dict
configuration classes.  Users should instantiate a child class (e.g.
FullOptimStateDictConfig) in order to configure settings for the
corresponding optim_state_dict type supported by FSDP.


offload_to_cpu () – If , then FSDP offloads the state dict’s
tensor values to CPU, and if , then FSDP keeps them on the
original device (which is GPU unless parameter CPU offloading is
enabled). (Default: )

OptimStateDictConfig is the base class for all optim_state_dict
configuration classes.  Users should instantiate a child class (e.g.
FullOptimStateDictConfig) in order to configure settings for the
corresponding optim_state_dict type supported by FSDP.

offload_to_cpu () – If , then FSDP offloads the state dict’s
tensor values to CPU, and if , then FSDP keeps them on the
original device (which is GPU unless parameter CPU offloading is
enabled). (Default: )

offload_to_cpu () – If , then FSDP offloads the state dict’s
tensor values to CPU, and if , then FSDP keeps them on the
original device (which is GPU unless parameter CPU offloading is
enabled). (Default: )

================================================================================

# FullyShardedDataParallel (Part 155)

torch.distributed.fsdp.FullOptimStateDictConfigoffload_to_cpu, rank0_only


rank0_only () – If , then only rank 0 saves the full state
dict, and nonzero ranks save an empty dict. If , then all
ranks save the full state dict. (Default: )

rank0_only () – If , then only rank 0 saves the full state
dict, and nonzero ranks save an empty dict. If , then all
ranks save the full state dict. (Default: )

rank0_only () – If , then only rank 0 saves the full state
dict, and nonzero ranks save an empty dict. If , then all
ranks save the full state dict. (Default: )

torch.distributed.fsdp.ShardedOptimStateDictConfigoffload_to_cpu, _use_dtensor
ShardedOptimStateDictConfig is a config class meant to be used with
StateDictType.SHARDED_STATE_DICT.


_use_dtensor () – If , then FSDP saves the state dict values
as , and if , then FSDP saves them as
ShardedTensor. (Default: )




_use_dtensor is a private field of ShardedOptimStateDictConfig
and it is used by FSDP to determine the type of state dict values. Users should not
manually modify _use_dtensor.

ShardedOptimStateDictConfig is a config class meant to be used with
StateDictType.SHARDED_STATE_DICT.

================================================================================

# FullyShardedDataParallel (Part 156)

_use_dtensor () – If , then FSDP saves the state dict values
as , and if , then FSDP saves them as
ShardedTensor. (Default: )

_use_dtensor () – If , then FSDP saves the state dict values
as , and if , then FSDP saves them as
ShardedTensor. (Default: )

_use_dtensor is a private field of ShardedOptimStateDictConfig
and it is used by FSDP to determine the type of state dict values. Users should not
manually modify _use_dtensor.

torch.distributed.fsdp.LocalOptimStateDictConfigoffload_to_cpu

torch.distributed.fsdp.StateDictSettingsstate_dict_typetorch.distributed.fsdp.api.StateDictType, state_dict_configtorch.distributed.fsdp.api.StateDictConfig, optim_state_dict_configtorch.distributed.fsdp.api.OptimStateDictConfig

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.distributed.fsdp.fully_shard

Created On: Dec 04, 2024 | Last Updated On: Jun 16, 2025

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 1)

PyTorch FSDP2 () provides
a fully sharded data parallelism (FSDP) implementation targeting performant
eager-mode while using per-parameter sharding for improved usability

List:
See the Getting Started with FSDP2
tutorial for more information.
If you are currently using FSDP1, consider migrating to FSDP2 using our
migration guide.

See the Getting Started with FSDP2
tutorial for more information.

If you are currently using FSDP1, consider migrating to FSDP2 using our
migration guide.

The user contract for fully_shard(model) is as follows

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 2)

List:
For model initialization, fully_shard converts model.parameters() from
plain torch.Tensor to DTensor in-place. The parameters are moved to the
appropriate device according to the device mesh.
Before forward and backward passes, pre-forward/backward hooks are
responsible for all-gathering the parameters and converting model.parameters()
from DTensor to plain torch.Tensor.
After forward and backward passes, post-forward/backward hooks free
the unsharded parameters (no communication needed) and convert
model.parameters() from plain torch.Tensor back to DTensor.
For the optimizer, it must be initialized with the DTensor model.parameters(),
and the optimizer step should be performed on DTensor parameters.
Call model(input) instead of model.forward(input) to trigger pre-forward
hooks to all-gather parameters. To make model.forward(input) work, users must
either call model.unshard() explicitly or use register_fsdp_forward_method(model, "forward")
to register the forward method for hooking.
fully_shard groups parameters together for a single all-gather. User should apply
fully_shard in a bottom-up manner. For example, in a Transformer model, fully_shard
should be applied to each layer before applying it to the root model. When applied
to the root model, fully_shard excludes model.parameters() from each layer and groups
the remaining parameters (e.g., embeddings, output projection) into a single
all-gather group.
type(model) is “unioned” with FSDPModule in-place. For example, if model
is originally of type nn.Linear, then fully_shard changes type(model) from
nn.Linear to FSDPLinear in-place. FSDPLinear is an instance of both
nn.Linear and FSDPModule. It retains all methods of nn.Linear while also
exposing FSDP2-specific APIs under FSDPModule, such as  and
.
Fully Qualified Names (FQNs) for parameters remain unchanged. If we call
model.state_dict(), the FQNs are the same before and after applying
fully_shard. This is because fully_shard does not wrap the module but only
registers hooks to the original module.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 3)

For model initialization, fully_shard converts model.parameters() from
plain torch.Tensor to DTensor in-place. The parameters are moved to the
appropriate device according to the device mesh.

Before forward and backward passes, pre-forward/backward hooks are
responsible for all-gathering the parameters and converting model.parameters()
from DTensor to plain torch.Tensor.

After forward and backward passes, post-forward/backward hooks free
the unsharded parameters (no communication needed) and convert
model.parameters() from plain torch.Tensor back to DTensor.

For the optimizer, it must be initialized with the DTensor model.parameters(),
and the optimizer step should be performed on DTensor parameters.

Call model(input) instead of model.forward(input) to trigger pre-forward
hooks to all-gather parameters. To make model.forward(input) work, users must
either call model.unshard() explicitly or use register_fsdp_forward_method(model, "forward")
to register the forward method for hooking.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 4)

fully_shard groups parameters together for a single all-gather. User should apply
fully_shard in a bottom-up manner. For example, in a Transformer model, fully_shard
should be applied to each layer before applying it to the root model. When applied
to the root model, fully_shard excludes model.parameters() from each layer and groups
the remaining parameters (e.g., embeddings, output projection) into a single
all-gather group.

type(model) is “unioned” with FSDPModule in-place. For example, if model
is originally of type nn.Linear, then fully_shard changes type(model) from
nn.Linear to FSDPLinear in-place. FSDPLinear is an instance of both
nn.Linear and FSDPModule. It retains all methods of nn.Linear while also
exposing FSDP2-specific APIs under FSDPModule, such as  and
.

Fully Qualified Names (FQNs) for parameters remain unchanged. If we call
model.state_dict(), the FQNs are the same before and after applying
fully_shard. This is because fully_shard does not wrap the module but only
registers hooks to the original module.

Compared to PyTorch FSDP1 (FullyShardedDataParallel):

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 5)

List:
FSDP2 uses -based dim-0 per-parameter sharding for a simpler
sharding representation compared to FSDP1’s flat-parameter sharding, while
preserving similar throughput performance. More specifically, FSDP2 chunks
each parameter on dim-0 across the data parallel workers (using
torch.chunk(dim=0)), whereas FSDP1 flattens, concatenates, and chunks a
group of tensors together, making reasoning about what data is present on
each worker and resharding to different parallelisms complex. Per-parameter
sharding provides a more intuitive user experience, relaxes constraints
around frozen parameters, and allows for communication-free (sharded) state
dicts, which otherwise require all-gathers in FSDP1.
FSDP2 implements a different memory management approach to handle the
multi-stream usages that avoids torch.Tensor.record_stream. This ensures
deterministic and expected memory usage and does not require blocking the CPU
like in FSDP1’s limit_all_gathers=True.
FSDP2 exposes APIs for manual control over prefetching and collective
scheduling, allowing power users more customization. See the methods on
FSDPModule below for details.
FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly
support full state dicts. Instead, users can reshard the sharded state dicts
containing  s to full state dicts themselves using 
APIs like DTensor.full_tensor() or by using higher-level APIs like
PyTorch Distributed Checkpoint ‘s
distributed state dict APIs. Also, some other args have been removed; see
 for
details.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 6)

FSDP2 uses -based dim-0 per-parameter sharding for a simpler
sharding representation compared to FSDP1’s flat-parameter sharding, while
preserving similar throughput performance. More specifically, FSDP2 chunks
each parameter on dim-0 across the data parallel workers (using
torch.chunk(dim=0)), whereas FSDP1 flattens, concatenates, and chunks a
group of tensors together, making reasoning about what data is present on
each worker and resharding to different parallelisms complex. Per-parameter
sharding provides a more intuitive user experience, relaxes constraints
around frozen parameters, and allows for communication-free (sharded) state
dicts, which otherwise require all-gathers in FSDP1.

FSDP2 implements a different memory management approach to handle the
multi-stream usages that avoids torch.Tensor.record_stream. This ensures
deterministic and expected memory usage and does not require blocking the CPU
like in FSDP1’s limit_all_gathers=True.

FSDP2 exposes APIs for manual control over prefetching and collective
scheduling, allowing power users more customization. See the methods on
FSDPModule below for details.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 7)

FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly
support full state dicts. Instead, users can reshard the sharded state dicts
containing  s to full state dicts themselves using 
APIs like DTensor.full_tensor() or by using higher-level APIs like
PyTorch Distributed Checkpoint ‘s
distributed state dict APIs. Also, some other args have been removed; see
 for
details.

The frontend API is fully_shard that can be called on a :

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 8)

torch.distributed.fsdp.fully_shard, , , reshard_after_forward, shard_placement_fn, MixedPrecisionPolicy(param_dtype=None, reduce_dtype=None, output_dtype=None, cast_forward_inputs=True), offload_policyOffloadPolicy(), ignored_params
Apply fully sharded data parallelism (FSDP) to , where FSDP
shards module parameters, gradients, and optimizer states across data
parallel workers to save memory at the cost of communication.
At initialization, FSDP shards the module’s parameters across the data
parallel workers given by . Before forward, FSDP all-gathers the
sharded parameters across the data-parallel workers to get the unsharded
parameters for forward computation. If reshard_after_forward is
, then FSDP frees the unsharded parameters after forward and
re-all-gathers them in backward before gradient computation. After gradient
computation, FSDP frees the unsharded parameters and reduce-scatters the
unsharded gradients across data-parallel workers.
This implementation represents the sharded parameters as  s
sharded on dim-0, while the unsharded parameters will be like the original
parameters on  (e.g. torch.Tensor if originally
torch.Tensor). A module
forward pre-hook
on  all-gathers the parameters, and a module
forward hook
on  frees them (if needed). Similar backward hooks all-gather
parameters and later free parameters and reduce-scatter gradients.
Since grouping multiple tensors together for one collective is critical for
communication efficiency, this implementation makes this grouping first
class. Calling fully_shard() on  constructs one group that
includes the parameters in module.parameters() except those already
assigned to a group from an earlier call on a submodule. This means that
fully_shard() should be called bottom-up on your model. Each group’s
parameters are all-gathered in one collective, and its gradients are
reduce-scattered in one collective. Partitioning the model into multiple
groups (“layer by layer”) allows for peak memory savings and communication/computation
overlap. Users generally should  call fully_shard() only on the
topmost root module.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 9)

Parameters

 () – The module or modules to
shard with FSDP and group together for communication.
 (DeviceMesh) – This data parallel mesh defines the
sharding and device. If 1D, then parameters are fully sharded
across the 1D mesh (FSDP) with (Shard(0),) placement. If 2D,
then parameters are sharded across the 1st dim and replicated
across the 0th dim (HSDP) with (Replicate(), 
placement. The mesh’s device type gives the device type used for
communication; if a CUDA or CUDA-like device type, then we use the
current device.
reshard_after_forward () – This controls the parameter
behavior after forward and can trade off memory and communication:

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 10)

If , then this reshards parameters after forward and
re-all-gathers in backward.
If , then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward. For best performance,
we usually set  for the root module, because the root module
is typically required immediately when the backward pass begins.
If , it is set to  for non-root modules and 
for root modules.
If an , then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the 
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. torch.cuda.device_count()).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to .
After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
; unsharded parameters if ; and the parameters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For  or an , this can be
done by manually resharding via .

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 11)


shard_placement_fn (nn.Parameter) – This callable can be used to override the sharding placement for a
parameter to shard a parameter on a dimension other than dim-0. If
this callable returns a  placement (not ),
then FSDP will shard according to that placement (e.g. ).
If sharding on a nonzero dim, we currently require even sharding,
i.e. the tensor dim size on that dim must be divisible by the FSDP
shard mesh size.
 (MixedPrecisionPolicy) – This controls the mixed precision
policy, which offers parameter/reduction mixed precision for this
module. See MixedPrecisionPolicy for details.
offload_policy (OffloadPolicy) – This controls the offloading policy,
which offers parameter/gradient/optimizer state offloading. See
OffloadPolicy and its subclasses for details.
ignored_params (nn.Parameter) – Optional(Set[nn.Parameter]): The set of parameters to be
ignored by FSDP. They will not be sharded, nor moved to the device
during init, nor have their gradients reduced in backward.



The module with FSDP applied (in-place).

Return type
FSDPModule

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 12)

Apply fully sharded data parallelism (FSDP) to , where FSDP
shards module parameters, gradients, and optimizer states across data
parallel workers to save memory at the cost of communication.

At initialization, FSDP shards the module’s parameters across the data
parallel workers given by . Before forward, FSDP all-gathers the
sharded parameters across the data-parallel workers to get the unsharded
parameters for forward computation. If reshard_after_forward is
, then FSDP frees the unsharded parameters after forward and
re-all-gathers them in backward before gradient computation. After gradient
computation, FSDP frees the unsharded parameters and reduce-scatters the
unsharded gradients across data-parallel workers.

This implementation represents the sharded parameters as  s
sharded on dim-0, while the unsharded parameters will be like the original
parameters on  (e.g. torch.Tensor if originally
torch.Tensor). A module
forward pre-hook
on  all-gathers the parameters, and a module
forward hook
on  frees them (if needed). Similar backward hooks all-gather
parameters and later free parameters and reduce-scatter gradients.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 13)

Since grouping multiple tensors together for one collective is critical for
communication efficiency, this implementation makes this grouping first
class. Calling fully_shard() on  constructs one group that
includes the parameters in module.parameters() except those already
assigned to a group from an earlier call on a submodule. This means that
fully_shard() should be called bottom-up on your model. Each group’s
parameters are all-gathered in one collective, and its gradients are
reduce-scattered in one collective. Partitioning the model into multiple
groups (“layer by layer”) allows for peak memory savings and communication/computation
overlap. Users generally should  call fully_shard() only on the
topmost root module.

Parameters

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 14)

 () – The module or modules to
shard with FSDP and group together for communication.
 (DeviceMesh) – This data parallel mesh defines the
sharding and device. If 1D, then parameters are fully sharded
across the 1D mesh (FSDP) with (Shard(0),) placement. If 2D,
then parameters are sharded across the 1st dim and replicated
across the 0th dim (HSDP) with (Replicate(), 
placement. The mesh’s device type gives the device type used for
communication; if a CUDA or CUDA-like device type, then we use the
current device.
reshard_after_forward () – This controls the parameter
behavior after forward and can trade off memory and communication:

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 15)

If , then this reshards parameters after forward and
re-all-gathers in backward.
If , then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward. For best performance,
we usually set  for the root module, because the root module
is typically required immediately when the backward pass begins.
If , it is set to  for non-root modules and 
for root modules.
If an , then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the 
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. torch.cuda.device_count()).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to .
After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
; unsharded parameters if ; and the parameters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For  or an , this can be
done by manually resharding via .

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 16)


shard_placement_fn (nn.Parameter) – This callable can be used to override the sharding placement for a
parameter to shard a parameter on a dimension other than dim-0. If
this callable returns a  placement (not ),
then FSDP will shard according to that placement (e.g. ).
If sharding on a nonzero dim, we currently require even sharding,
i.e. the tensor dim size on that dim must be divisible by the FSDP
shard mesh size.
 (MixedPrecisionPolicy) – This controls the mixed precision
policy, which offers parameter/reduction mixed precision for this
module. See MixedPrecisionPolicy for details.
offload_policy (OffloadPolicy) – This controls the offloading policy,
which offers parameter/gradient/optimizer state offloading. See
OffloadPolicy and its subclasses for details.
ignored_params (nn.Parameter) – Optional(Set[nn.Parameter]): The set of parameters to be
ignored by FSDP. They will not be sharded, nor moved to the device
during init, nor have their gradients reduced in backward.



The module with FSDP applied (in-place).

Return type
FSDPModule

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 17)

List:
() – The module or modules to
shard with FSDP and group together for communication.
 (DeviceMesh) – This data parallel mesh defines the
sharding and device. If 1D, then parameters are fully sharded
across the 1D mesh (FSDP) with (Shard(0),) placement. If 2D,
then parameters are sharded across the 1st dim and replicated
across the 0th dim (HSDP) with (Replicate(), 
placement. The mesh’s device type gives the device type used for
communication; if a CUDA or CUDA-like device type, then we use the
current device.
reshard_after_forward () – This controls the parameter
behavior after forward and can trade off memory and communication:

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 18)

If , then this reshards parameters after forward and
re-all-gathers in backward.
If , then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward. For best performance,
we usually set  for the root module, because the root module
is typically required immediately when the backward pass begins.
If , it is set to  for non-root modules and 
for root modules.
If an , then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the 
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. torch.cuda.device_count()).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to .
After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
; unsharded parameters if ; and the parameters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For  or an , this can be
done by manually resharding via .

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 19)


shard_placement_fn (nn.Parameter) – This callable can be used to override the sharding placement for a
parameter to shard a parameter on a dimension other than dim-0. If
this callable returns a  placement (not ),
then FSDP will shard according to that placement (e.g. ).
If sharding on a nonzero dim, we currently require even sharding,
i.e. the tensor dim size on that dim must be divisible by the FSDP
shard mesh size.
 (MixedPrecisionPolicy) – This controls the mixed precision
policy, which offers parameter/reduction mixed precision for this
module. See MixedPrecisionPolicy for details.
offload_policy (OffloadPolicy) – This controls the offloading policy,
which offers parameter/gradient/optimizer state offloading. See
OffloadPolicy and its subclasses for details.
ignored_params (nn.Parameter) – Optional(Set[nn.Parameter]): The set of parameters to be
ignored by FSDP. They will not be sharded, nor moved to the device
during init, nor have their gradients reduced in backward.

() – The module or modules to
shard with FSDP and group together for communication.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 20)

(DeviceMesh) – This data parallel mesh defines the
sharding and device. If 1D, then parameters are fully sharded
across the 1D mesh (FSDP) with (Shard(0),) placement. If 2D,
then parameters are sharded across the 1st dim and replicated
across the 0th dim (HSDP) with (Replicate(), 
placement. The mesh’s device type gives the device type used for
communication; if a CUDA or CUDA-like device type, then we use the
current device.

reshard_after_forward () – This controls the parameter
behavior after forward and can trade off memory and communication:

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 21)

If , then this reshards parameters after forward and
re-all-gathers in backward.
If , then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward. For best performance,
we usually set  for the root module, because the root module
is typically required immediately when the backward pass begins.
If , it is set to  for non-root modules and 
for root modules.
If an , then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the 
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. torch.cuda.device_count()).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to .
After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
; unsharded parameters if ; and the parameters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For  or an , this can be
done by manually resharding via .

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 22)

This controls the parameter
behavior after forward and can trade off memory and communication:

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 23)

List:
If , then this reshards parameters after forward and
re-all-gathers in backward.
If , then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward. For best performance,
we usually set  for the root module, because the root module
is typically required immediately when the backward pass begins.
If , it is set to  for non-root modules and 
for root modules.
If an , then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the 
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. torch.cuda.device_count()).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to .
After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
; unsharded parameters if ; and the parameters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For  or an , this can be
done by manually resharding via .

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 24)

If , then this reshards parameters after forward and
re-all-gathers in backward.

If , then this keeps the unsharded parameters in memory
after forward and avoids the all-gather in backward. For best performance,
we usually set  for the root module, because the root module
is typically required immediately when the backward pass begins.

If , it is set to  for non-root modules and 
for root modules.

If an , then this represents the world size to reshard to
after forward. It should be a non-trivial divisor of the 
shard dim size (i.e. excluding 1 and the dim size itself). A
choice may be the intra-node size (e.g. torch.cuda.device_count()).
This allows the all-gather in backward to be over a smaller world
size at the cost of higher memory usage than setting to .

After forward, the parameters registered to the module depend on
to this: The registered parameters are the sharded parameters if
; unsharded parameters if ; and the parameters
resharded to the smaller mesh otherwise. To modify the parameters
between forward and backward, the registered parameters must be
the sharded parameters. For  or an , this can be
done by manually resharding via .

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 25)

shard_placement_fn (nn.Parameter) – This callable can be used to override the sharding placement for a
parameter to shard a parameter on a dimension other than dim-0. If
this callable returns a  placement (not ),
then FSDP will shard according to that placement (e.g. ).
If sharding on a nonzero dim, we currently require even sharding,
i.e. the tensor dim size on that dim must be divisible by the FSDP
shard mesh size.

(MixedPrecisionPolicy) – This controls the mixed precision
policy, which offers parameter/reduction mixed precision for this
module. See MixedPrecisionPolicy for details.

offload_policy (OffloadPolicy) – This controls the offloading policy,
which offers parameter/gradient/optimizer state offloading. See
OffloadPolicy and its subclasses for details.

ignored_params (nn.Parameter) – Optional(Set[nn.Parameter]): The set of parameters to be
ignored by FSDP. They will not be sharded, nor moved to the device
during init, nor have their gradients reduced in backward.

The module with FSDP applied (in-place).

torch.distributed.fsdp.FSDPModule, 



================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 26)

Reshards the module’s parameters, freeing the unsharded parameters if
they are allocated and registering the sharded parameters to the
module. This method is  recursive.




set_all_reduce_hook, , 

Parameters

 (torch.Tensor) – User-defined all-reduce hook
with expected signature hook(reduce_output: torch.Tensor)  
where reduce_output is the reduce-scatter output if only
using FSDP or the all-reduce output if using native HSDP.
 (torch.cuda.Stream) – Stream to run the all-reduce
hook in. This should only be set if not using native HSDP. If
using native HSDP, the hook will run in the internally defined
all-reduce stream used by the native HSDP all-reduce.






set_allocate_memory_from_process_group_for_comm
Sets whether the temporary staging buffers used to send and receive data
over collective communications should be allocated using the custom
optimized allocator provided by the ProcessGroup itself (if any). This
might allow the ProcessGroup to be more efficient. For example, when
using NCCL, this enables it to leverage zero-copy transfers over SHARP
(for NVLink and/or InfiniBand).

Parameters
 () – Whether to turn on ProcessGroup allocation.





================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 27)

set_force_sum_reduction_for_comms
Sets whether to require the low-level collective communication
primitives to exclusively use “sum”-type reductions, even if it comes
at the cost of separate additional pre- or post-scaling operations.
This is needed for example because NCCL currently supports zero-copy
transfers only for this kind of collectives.
NB: for MTIA devices, this is always implicitly enabled.
NB: if set_all_reduce_hook is used under FSDP setup, the caller needs
to ensure the custom all-reduce across FSDP units follow this strategy
as well, as FSDP can no longer automatically handle that.

Parameters
 () – Whether to only ever use ReduceOp.SUM for comms.





set_gradient_divide_factor
Sets a custom divide factor for the gradient reduction. This might use
a custom reduce op using NCCL’s PreMulSum, which allows multiplying by
the factor before reduction.

Parameters
 () – Custom divide factor.





set_is_last_backwardis_last_backward
Sets whether the next backward is the last one. On the last backward,
FSDP waits on pending gradient reduction and clears internal data
data structures for backward prefetching. This can be useful for
microbatching.



================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 28)


set_modules_to_backward_prefetch
Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in backward. This overrides the default backward
pretching implementation that prefetches the next FSDP module based on
the reverse post-forward order.
Passing a singleton list containing the previous FSDP module gives the
same all-gather overlap behavior as the default overlap behavior.
Passing a list with at least length two is required for more aggressive
overlap and will use more reserved memory.

Parameters
 (FSDPModule) – FSDP modules to prefetch.





set_modules_to_forward_prefetch
Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in forward. The prefetching runs after this
module’s all-gather copy-out.
Passing a singleton list containing the next FSDP module gives the same
all-gather overlap behavior as the default overlap behavior, except the
prefetched all-gather is issued earlier from the CPU. Passing a list
with at least length two is required for more aggressive overlap and
will use more reserved memory.

Parameters
 (FSDPModule) – FSDP modules to prefetch.





================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 29)

set_post_optim_event
Sets a post-optimizer-step event for the root FSDP module to wait the
all-gather streams on.
By default, the root FSDP module waits the all-gather streams on the
current stream to ensure that the optimizer step has finished before
all-gathering. However, this may introduce false dependencies if
there is unrelated computation after the optimizer step. This API
allows the user to provide their own event to wait on. After the root
waits on the event, the event is discarded, so this API should be
called with a new event each iteration.

Parameters
 (torch.Event) – Event recorded after the optimizer step
to wait all-gather streams on.





set_reduce_scatter_divide_factor
Use set_gradient_divide_factor() instead




set_requires_all_reducerequires_all_reduce, , 
Sets if the module should all-reduce gradients. This can be used to
implement gradient accumulation with only reduce-scatter but not
all-reduce for HSDP.



================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 30)


set_requires_gradient_syncrequires_gradient_sync, , 
Sets if the module should sync gradients. This can be used to implement
gradient accumulation without communication. For HSDP, this controls
both reduce-scatter and all-reduce together. This is the equivalence of
 in FSDP1.

Parameters

requires_gradient_sync () – Whether to reduce gradients for the
module’s parameters.
 () – Whether to set for all FSDP submodules or just the
passed-in module.






set_reshard_after_backwardreshard_after_backward, , 
Sets if the module should reshard parameters after backward. This can
be used during gradient accumulation to trade off higher memory for
reduced communication since the unsharded parameters do not need to be
re-all-gathered before the next forward.

Parameters

reshard_after_backward () – Whether to reshard parameters after
backward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.





================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 31)


set_reshard_after_forwardreshard_after_forward, 
Sets if the module should reshard parameters after forward. This can be
used to change the reshard_after_forward FSDP arg at runtime. For
example, this can be used to set the FSDP root module’s value to
 (since it is otherwise specially set to ), or it can
set an FSDP module’s value to  for running evals and set back
to  for training.

Parameters

reshard_after_forward () – Whether to reshard parameters after
forward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.






set_unshard_in_backwardunshard_in_backward
Sets whether the FSDP module’s parameters need to be unsharded in
backward. This can be used in expert cases when the user knows that all
parameters in this FSDP module’s parameter group are not needed for
backward computation (e.g. embedding).





Unshards the module’s parameters by allocating memory and all-gathering
the parameters. This method is  recursive. The unshard follows the
MixedPrecisionPolicy, so it will all-gather following
param_dtype if set.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 32)

Parameters
 () – If , then returns a UnshardHandle
that has a  method to wait on the unshard op. If
, then returns  and waits on the handle inside
this function.

Return type
[UnshardHandle]




If async_op=True, then FSDP will wait on the pending
unshard in the module’s pre-forward for the user. The user only
needs to call  explicitly if the wait should happen
before pre-forward.

Reshards the module’s parameters, freeing the unsharded parameters if
they are allocated and registering the sharded parameters to the
module. This method is  recursive.

Reshards the module’s parameters, freeing the unsharded parameters if
they are allocated and registering the sharded parameters to the
module. This method is  recursive.

set_all_reduce_hook, , 

Parameters

 (torch.Tensor) – User-defined all-reduce hook
with expected signature hook(reduce_output: torch.Tensor)  
where reduce_output is the reduce-scatter output if only
using FSDP or the all-reduce output if using native HSDP.
 (torch.cuda.Stream) – Stream to run the all-reduce
hook in. This should only be set if not using native HSDP. If
using native HSDP, the hook will run in the internally defined
all-reduce stream used by the native HSDP all-reduce.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 33)

Parameters

 (torch.Tensor) – User-defined all-reduce hook
with expected signature hook(reduce_output: torch.Tensor)  
where reduce_output is the reduce-scatter output if only
using FSDP or the all-reduce output if using native HSDP.
 (torch.cuda.Stream) – Stream to run the all-reduce
hook in. This should only be set if not using native HSDP. If
using native HSDP, the hook will run in the internally defined
all-reduce stream used by the native HSDP all-reduce.

List:
(torch.Tensor) – User-defined all-reduce hook
with expected signature hook(reduce_output: torch.Tensor)  
where reduce_output is the reduce-scatter output if only
using FSDP or the all-reduce output if using native HSDP.
 (torch.cuda.Stream) – Stream to run the all-reduce
hook in. This should only be set if not using native HSDP. If
using native HSDP, the hook will run in the internally defined
all-reduce stream used by the native HSDP all-reduce.

(torch.Tensor) – User-defined all-reduce hook
with expected signature hook(reduce_output: torch.Tensor)  
where reduce_output is the reduce-scatter output if only
using FSDP or the all-reduce output if using native HSDP.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 34)

(torch.cuda.Stream) – Stream to run the all-reduce
hook in. This should only be set if not using native HSDP. If
using native HSDP, the hook will run in the internally defined
all-reduce stream used by the native HSDP all-reduce.

set_allocate_memory_from_process_group_for_comm
Sets whether the temporary staging buffers used to send and receive data
over collective communications should be allocated using the custom
optimized allocator provided by the ProcessGroup itself (if any). This
might allow the ProcessGroup to be more efficient. For example, when
using NCCL, this enables it to leverage zero-copy transfers over SHARP
(for NVLink and/or InfiniBand).

Parameters
 () – Whether to turn on ProcessGroup allocation.

Sets whether the temporary staging buffers used to send and receive data
over collective communications should be allocated using the custom
optimized allocator provided by the ProcessGroup itself (if any). This
might allow the ProcessGroup to be more efficient. For example, when
using NCCL, this enables it to leverage zero-copy transfers over SHARP
(for NVLink and/or InfiniBand).

Parameters
 () – Whether to turn on ProcessGroup allocation.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 35)

() – Whether to turn on ProcessGroup allocation.

set_force_sum_reduction_for_comms
Sets whether to require the low-level collective communication
primitives to exclusively use “sum”-type reductions, even if it comes
at the cost of separate additional pre- or post-scaling operations.
This is needed for example because NCCL currently supports zero-copy
transfers only for this kind of collectives.
NB: for MTIA devices, this is always implicitly enabled.
NB: if set_all_reduce_hook is used under FSDP setup, the caller needs
to ensure the custom all-reduce across FSDP units follow this strategy
as well, as FSDP can no longer automatically handle that.

Parameters
 () – Whether to only ever use ReduceOp.SUM for comms.

Sets whether to require the low-level collective communication
primitives to exclusively use “sum”-type reductions, even if it comes
at the cost of separate additional pre- or post-scaling operations.
This is needed for example because NCCL currently supports zero-copy
transfers only for this kind of collectives.

NB: for MTIA devices, this is always implicitly enabled.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 36)

NB: if set_all_reduce_hook is used under FSDP setup, the caller needs
to ensure the custom all-reduce across FSDP units follow this strategy
as well, as FSDP can no longer automatically handle that.

Parameters
 () – Whether to only ever use ReduceOp.SUM for comms.

() – Whether to only ever use ReduceOp.SUM for comms.

set_gradient_divide_factor
Sets a custom divide factor for the gradient reduction. This might use
a custom reduce op using NCCL’s PreMulSum, which allows multiplying by
the factor before reduction.

Parameters
 () – Custom divide factor.

Sets a custom divide factor for the gradient reduction. This might use
a custom reduce op using NCCL’s PreMulSum, which allows multiplying by
the factor before reduction.

Parameters
 () – Custom divide factor.

() – Custom divide factor.

set_is_last_backwardis_last_backward
Sets whether the next backward is the last one. On the last backward,
FSDP waits on pending gradient reduction and clears internal data
data structures for backward prefetching. This can be useful for
microbatching.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 37)

Sets whether the next backward is the last one. On the last backward,
FSDP waits on pending gradient reduction and clears internal data
data structures for backward prefetching. This can be useful for
microbatching.

set_modules_to_backward_prefetch
Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in backward. This overrides the default backward
pretching implementation that prefetches the next FSDP module based on
the reverse post-forward order.
Passing a singleton list containing the previous FSDP module gives the
same all-gather overlap behavior as the default overlap behavior.
Passing a list with at least length two is required for more aggressive
overlap and will use more reserved memory.

Parameters
 (FSDPModule) – FSDP modules to prefetch.

Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in backward. This overrides the default backward
pretching implementation that prefetches the next FSDP module based on
the reverse post-forward order.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 38)

Passing a singleton list containing the previous FSDP module gives the
same all-gather overlap behavior as the default overlap behavior.
Passing a list with at least length two is required for more aggressive
overlap and will use more reserved memory.

Parameters
 (FSDPModule) – FSDP modules to prefetch.

(FSDPModule) – FSDP modules to prefetch.

set_modules_to_forward_prefetch
Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in forward. The prefetching runs after this
module’s all-gather copy-out.
Passing a singleton list containing the next FSDP module gives the same
all-gather overlap behavior as the default overlap behavior, except the
prefetched all-gather is issued earlier from the CPU. Passing a list
with at least length two is required for more aggressive overlap and
will use more reserved memory.

Parameters
 (FSDPModule) – FSDP modules to prefetch.

Sets the FSDP modules for which this FSDP module should explicitly
prefetch all-gathers in forward. The prefetching runs after this
module’s all-gather copy-out.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 39)

Passing a singleton list containing the next FSDP module gives the same
all-gather overlap behavior as the default overlap behavior, except the
prefetched all-gather is issued earlier from the CPU. Passing a list
with at least length two is required for more aggressive overlap and
will use more reserved memory.

Parameters
 (FSDPModule) – FSDP modules to prefetch.

(FSDPModule) – FSDP modules to prefetch.

set_post_optim_event
Sets a post-optimizer-step event for the root FSDP module to wait the
all-gather streams on.
By default, the root FSDP module waits the all-gather streams on the
current stream to ensure that the optimizer step has finished before
all-gathering. However, this may introduce false dependencies if
there is unrelated computation after the optimizer step. This API
allows the user to provide their own event to wait on. After the root
waits on the event, the event is discarded, so this API should be
called with a new event each iteration.

Parameters
 (torch.Event) – Event recorded after the optimizer step
to wait all-gather streams on.

Sets a post-optimizer-step event for the root FSDP module to wait the
all-gather streams on.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 40)

By default, the root FSDP module waits the all-gather streams on the
current stream to ensure that the optimizer step has finished before
all-gathering. However, this may introduce false dependencies if
there is unrelated computation after the optimizer step. This API
allows the user to provide their own event to wait on. After the root
waits on the event, the event is discarded, so this API should be
called with a new event each iteration.

Parameters
 (torch.Event) – Event recorded after the optimizer step
to wait all-gather streams on.

(torch.Event) – Event recorded after the optimizer step
to wait all-gather streams on.

set_reduce_scatter_divide_factor
Use set_gradient_divide_factor() instead

Use set_gradient_divide_factor() instead

set_requires_all_reducerequires_all_reduce, , 
Sets if the module should all-reduce gradients. This can be used to
implement gradient accumulation with only reduce-scatter but not
all-reduce for HSDP.

Sets if the module should all-reduce gradients. This can be used to
implement gradient accumulation with only reduce-scatter but not
all-reduce for HSDP.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 41)

set_requires_gradient_syncrequires_gradient_sync, , 
Sets if the module should sync gradients. This can be used to implement
gradient accumulation without communication. For HSDP, this controls
both reduce-scatter and all-reduce together. This is the equivalence of
 in FSDP1.

Parameters

requires_gradient_sync () – Whether to reduce gradients for the
module’s parameters.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

Sets if the module should sync gradients. This can be used to implement
gradient accumulation without communication. For HSDP, this controls
both reduce-scatter and all-reduce together. This is the equivalence of
 in FSDP1.

Parameters

requires_gradient_sync () – Whether to reduce gradients for the
module’s parameters.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

List:
requires_gradient_sync () – Whether to reduce gradients for the
module’s parameters.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

requires_gradient_sync () – Whether to reduce gradients for the
module’s parameters.

() – Whether to set for all FSDP submodules or just the
passed-in module.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 42)

set_reshard_after_backwardreshard_after_backward, , 
Sets if the module should reshard parameters after backward. This can
be used during gradient accumulation to trade off higher memory for
reduced communication since the unsharded parameters do not need to be
re-all-gathered before the next forward.

Parameters

reshard_after_backward () – Whether to reshard parameters after
backward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

Sets if the module should reshard parameters after backward. This can
be used during gradient accumulation to trade off higher memory for
reduced communication since the unsharded parameters do not need to be
re-all-gathered before the next forward.

Parameters

reshard_after_backward () – Whether to reshard parameters after
backward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

List:
reshard_after_backward () – Whether to reshard parameters after
backward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

reshard_after_backward () – Whether to reshard parameters after
backward.

() – Whether to set for all FSDP submodules or just the
passed-in module.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 43)

set_reshard_after_forwardreshard_after_forward, 
Sets if the module should reshard parameters after forward. This can be
used to change the reshard_after_forward FSDP arg at runtime. For
example, this can be used to set the FSDP root module’s value to
 (since it is otherwise specially set to ), or it can
set an FSDP module’s value to  for running evals and set back
to  for training.

Parameters

reshard_after_forward () – Whether to reshard parameters after
forward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

Sets if the module should reshard parameters after forward. This can be
used to change the reshard_after_forward FSDP arg at runtime. For
example, this can be used to set the FSDP root module’s value to
 (since it is otherwise specially set to ), or it can
set an FSDP module’s value to  for running evals and set back
to  for training.

Parameters

reshard_after_forward () – Whether to reshard parameters after
forward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

List:
reshard_after_forward () – Whether to reshard parameters after
forward.
 () – Whether to set for all FSDP submodules or just the
passed-in module.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 44)

reshard_after_forward () – Whether to reshard parameters after
forward.

() – Whether to set for all FSDP submodules or just the
passed-in module.

set_unshard_in_backwardunshard_in_backward
Sets whether the FSDP module’s parameters need to be unsharded in
backward. This can be used in expert cases when the user knows that all
parameters in this FSDP module’s parameter group are not needed for
backward computation (e.g. embedding).

Sets whether the FSDP module’s parameters need to be unsharded in
backward. This can be used in expert cases when the user knows that all
parameters in this FSDP module’s parameter group are not needed for
backward computation (e.g. embedding).

Unshards the module’s parameters by allocating memory and all-gathering
the parameters. This method is  recursive. The unshard follows the
MixedPrecisionPolicy, so it will all-gather following
param_dtype if set.

Parameters
 () – If , then returns a UnshardHandle
that has a  method to wait on the unshard op. If
, then returns  and waits on the handle inside
this function.

Return type
[UnshardHandle]



================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 45)


If async_op=True, then FSDP will wait on the pending
unshard in the module’s pre-forward for the user. The user only
needs to call  explicitly if the wait should happen
before pre-forward.

Unshards the module’s parameters by allocating memory and all-gathering
the parameters. This method is  recursive. The unshard follows the
MixedPrecisionPolicy, so it will all-gather following
param_dtype if set.

Parameters
 () – If , then returns a UnshardHandle
that has a  method to wait on the unshard op. If
, then returns  and waits on the handle inside
this function.

Return type
[UnshardHandle]

() – If , then returns a UnshardHandle
that has a  method to wait on the unshard op. If
, then returns  and waits on the handle inside
this function.

If async_op=True, then FSDP will wait on the pending
unshard in the module’s pre-forward for the user. The user only
needs to call  explicitly if the wait should happen
before pre-forward.

torch.distributed.fsdp.UnshardHandle
A handle to wait on a FSDPModule.unshard() op.



Waits on the unshard op. This ensures that the current stream can use
the unsharded parameters, which are now registered to the module.

A handle to wait on a FSDPModule.unshard() op.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 46)

Waits on the unshard op. This ensures that the current stream can use
the unsharded parameters, which are now registered to the module.

Waits on the unshard op. This ensures that the current stream can use
the unsharded parameters, which are now registered to the module.

torch.distributed.fsdp.register_fsdp_forward_method, method_name
Registers a method on  to be considered a forward method for
FSDP.
FSDP all-gathers parameters pre-forward and optionally frees parameters
post-forward (depending on reshard_after_forward). FSDP only knows to
do this for nn.Module.forward() by default. This function patches a
user-specified method to run the pre/post-forward hooks before/after the
method, respectively. If  is not an FSDPModule, then
this is a no-op.

Parameters

 () – Module to register the forward method on.
method_name () – Name of the forward method.

Registers a method on  to be considered a forward method for
FSDP.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 47)

FSDP all-gathers parameters pre-forward and optionally frees parameters
post-forward (depending on reshard_after_forward). FSDP only knows to
do this for nn.Module.forward() by default. This function patches a
user-specified method to run the pre/post-forward hooks before/after the
method, respectively. If  is not an FSDPModule, then
this is a no-op.

Parameters

 () – Module to register the forward method on.
method_name () – Name of the forward method.

List:
() – Module to register the forward method on.
method_name () – Name of the forward method.

() – Module to register the forward method on.

method_name () – Name of the forward method.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 48)

torch.distributed.fsdp.MixedPrecisionPolicyparam_dtype, reduce_dtype, output_dtype, cast_forward_inputs
This configures FSDP’s mixed precision. Unlike autocast, this applies mixed
precision at the module level, not op level, which means low-precision
activations are saved for backward and high-to-low-precision casts are
incurred only at module boundaries.
FSDP works well with module-level mixed precision since it keeps the
high-precision sharded parameters in memory anyway. In other words, FSDP
does not require any extra memory to keep a high-precision copy of the
parameters for the optimizer step.



================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 49)

param_dtype (torch.dtype) – This specifies the dtype for
the unsharded parameter and hence the dtype for forward/backward
computation and the parameter all-gather. If this is , then
the unsharded parameter uses the original dtype. The optimizer step
uses the sharded parameter in the original dtype. (Default:
)
reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then the reduction
uses the compute dtype. This can be used to run gradient reduction
in full precision while using low precision for compute. If also
gradient reduction is disabled via set_requires_gradient_sync(),
then FSDP will accumulate gradients using reduce_dtype.
(Default: )
output_dtype (torch.dtype) – This specifies the dtype for
casting floating-point forward outputs. This can be used to
help implement cases where different modules have different mixed
precision policies. (Default: )
cast_forward_inputs () – This specifies whether FSDP should cast the
forward’s floating-point input tensors to param_dtype or not.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 50)

This configures FSDP’s mixed precision. Unlike autocast, this applies mixed
precision at the module level, not op level, which means low-precision
activations are saved for backward and high-to-low-precision casts are
incurred only at module boundaries.

FSDP works well with module-level mixed precision since it keeps the
high-precision sharded parameters in memory anyway. In other words, FSDP
does not require any extra memory to keep a high-precision copy of the
parameters for the optimizer step.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 51)

param_dtype (torch.dtype) – This specifies the dtype for
the unsharded parameter and hence the dtype for forward/backward
computation and the parameter all-gather. If this is , then
the unsharded parameter uses the original dtype. The optimizer step
uses the sharded parameter in the original dtype. (Default:
)
reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then the reduction
uses the compute dtype. This can be used to run gradient reduction
in full precision while using low precision for compute. If also
gradient reduction is disabled via set_requires_gradient_sync(),
then FSDP will accumulate gradients using reduce_dtype.
(Default: )
output_dtype (torch.dtype) – This specifies the dtype for
casting floating-point forward outputs. This can be used to
help implement cases where different modules have different mixed
precision policies. (Default: )
cast_forward_inputs () – This specifies whether FSDP should cast the
forward’s floating-point input tensors to param_dtype or not.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 52)

List:
param_dtype (torch.dtype) – This specifies the dtype for
the unsharded parameter and hence the dtype for forward/backward
computation and the parameter all-gather. If this is , then
the unsharded parameter uses the original dtype. The optimizer step
uses the sharded parameter in the original dtype. (Default:
)
reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then the reduction
uses the compute dtype. This can be used to run gradient reduction
in full precision while using low precision for compute. If also
gradient reduction is disabled via set_requires_gradient_sync(),
then FSDP will accumulate gradients using reduce_dtype.
(Default: )
output_dtype (torch.dtype) – This specifies the dtype for
casting floating-point forward outputs. This can be used to
help implement cases where different modules have different mixed
precision policies. (Default: )
cast_forward_inputs () – This specifies whether FSDP should cast the
forward’s floating-point input tensors to param_dtype or not.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 53)

param_dtype (torch.dtype) – This specifies the dtype for
the unsharded parameter and hence the dtype for forward/backward
computation and the parameter all-gather. If this is , then
the unsharded parameter uses the original dtype. The optimizer step
uses the sharded parameter in the original dtype. (Default:
)

reduce_dtype (torch.dtype) – This specifies the dtype for
gradient reduction (i.e. reduce-scatter or all-reduce). If this is
 but param_dtype is not , then the reduction
uses the compute dtype. This can be used to run gradient reduction
in full precision while using low precision for compute. If also
gradient reduction is disabled via set_requires_gradient_sync(),
then FSDP will accumulate gradients using reduce_dtype.
(Default: )

output_dtype (torch.dtype) – This specifies the dtype for
casting floating-point forward outputs. This can be used to
help implement cases where different modules have different mixed
precision policies. (Default: )

cast_forward_inputs () – This specifies whether FSDP should cast the
forward’s floating-point input tensors to param_dtype or not.

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 54)

torch.distributed.fsdp.OffloadPolicy
This base class represents the policy of no offloading and is only used as
the default value for the offload_policy arg.

This base class represents the policy of no offloading and is only used as
the default value for the offload_policy arg.

torch.distributed.fsdp.CPUOffloadPolicypin_memory
This offload policy offloads parameters, gradients, and optimizer states to
CPU. Sharded parameters are copied host-to-device before all-gather. The
all-gathered parameters are freed according to reshard_after_forward.
Sharded gradients are copied device-to-host in backward, and the optimizer
step runs on CPU with CPU optimizer states.


pin_memory () – Whether to pin sharded parameter and gradient
memory. Pinning memory allows both more efficient H2D/D2H copies
and for the copies to overlap with compute. However, the pinned
memory cannot be used by other processes. Set this to  if
you have insufficient CPU memory. (Default: )

================================================================================

# torch.distributed.fsdp.fully_shard - PyTorch FSDP2 (fully_shard) (Part 55)

This offload policy offloads parameters, gradients, and optimizer states to
CPU. Sharded parameters are copied host-to-device before all-gather. The
all-gathered parameters are freed according to reshard_after_forward.
Sharded gradients are copied device-to-host in backward, and the optimizer
step runs on CPU with CPU optimizer states.

pin_memory () – Whether to pin sharded parameter and gradient
memory. Pinning memory allows both more efficient H2D/D2H copies
and for the copies to overlap with compute. However, the pinned
memory cannot be used by other processes. Set this to  if
you have insufficient CPU memory. (Default: )

pin_memory () – Whether to pin sharded parameter and gradient
memory. Pinning memory allows both more efficient H2D/D2H copies
and for the copies to overlap with compute. However, the pinned
memory cannot be used by other processes. Set this to  if
you have insufficient CPU memory. (Default: )

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 1)

Created On: Jun 13, 2025 | Last Updated On: Jun 13, 2025

Tensor Parallelism(TP) is built on top of the PyTorch DistributedTensor
(DTensor)[https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md]
and provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism.

Tensor Parallelism APIs are experimental and subject to change.

The entrypoint to parallelize your  using Tensor Parallelism is:

torch.distributed.tensor.parallel.parallelize_module, device_mesh, parallelize_plan, , src_data_rank
Apply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan.
We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan contains
ParallelStyle, which indicates how user wants the module or sub_module
to be parallelized.
User can also specify different parallel style per module fully qualified name (FQN).
Note that parallelize_module only accepts a 1-D DeviceMesh, if you have a 2-D or N-D DeviceMesh,
slice the DeviceMesh to a 1-D sub DeviceMesh first then pass to this API(i.e. device_mesh["tp"])

Parameters

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 2)

 () – Module to be parallelized.
device_mesh (DeviceMesh, optional) – Object which describes the mesh topology of devices for the DTensor.
If not specified, the call must be under a DeviceMesh context.
parallelize_plan (Union[ParallelStyle, Dict[str, ParallelStyle]], optional) – The plan used to parallelize the module. It can be either a
ParallelStyle object which contains how we prepare
input/output for Tensor Parallelism or it can be a dict of module
FQN and its corresponding ParallelStyle object. If not
specified, the call will do nothing at the moment.


Keyword Arguments
src_data_rank () – the rank of the source data for the logical/global tensor, it is used by
distribute_tensor() to scatter/broadcast the shards/replicas to other ranks. By default,
we use group_rank=0 on each DeviceMesh dimension as the source data to preserve the single-device
semantic. If passing  explicitly, parallelize_module() simply uses its local data instead
of trying to preserve the single-device semantic via scatter/broadcast. Default: 0


A  object parallelized.

Return type




 torch.distributed.tensor.parallel  parallelize_module ColwiseParallel
 torch.distributed.device_mesh  init_device_mesh

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 3)

# Define the module.
  
  init_device_mesh 
  parallelize_module   ColwiseParallel  RowwiseParallel







For complex module architecture like Attention, MLP layers, we recommend composing
different ParallelStyles together (i.e. ColwiseParallel and RowwiseParallel) and pass
as a parallelize_plan, to achieves the desired sharding computation.

Apply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan.

We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan contains
ParallelStyle, which indicates how user wants the module or sub_module
to be parallelized.

User can also specify different parallel style per module fully qualified name (FQN).

Note that parallelize_module only accepts a 1-D DeviceMesh, if you have a 2-D or N-D DeviceMesh,
slice the DeviceMesh to a 1-D sub DeviceMesh first then pass to this API(i.e. device_mesh["tp"])

Parameters

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 4)

 () – Module to be parallelized.
device_mesh (DeviceMesh, optional) – Object which describes the mesh topology of devices for the DTensor.
If not specified, the call must be under a DeviceMesh context.
parallelize_plan (Union[ParallelStyle, Dict[str, ParallelStyle]], optional) – The plan used to parallelize the module. It can be either a
ParallelStyle object which contains how we prepare
input/output for Tensor Parallelism or it can be a dict of module
FQN and its corresponding ParallelStyle object. If not
specified, the call will do nothing at the moment.


Keyword Arguments
src_data_rank () – the rank of the source data for the logical/global tensor, it is used by
distribute_tensor() to scatter/broadcast the shards/replicas to other ranks. By default,
we use group_rank=0 on each DeviceMesh dimension as the source data to preserve the single-device
semantic. If passing  explicitly, parallelize_module() simply uses its local data instead
of trying to preserve the single-device semantic via scatter/broadcast. Default: 0


A  object parallelized.

Return type

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 5)

List:
() – Module to be parallelized.
device_mesh (DeviceMesh, optional) – Object which describes the mesh topology of devices for the DTensor.
If not specified, the call must be under a DeviceMesh context.
parallelize_plan (Union[ParallelStyle, Dict[str, ParallelStyle]], optional) – The plan used to parallelize the module. It can be either a
ParallelStyle object which contains how we prepare
input/output for Tensor Parallelism or it can be a dict of module
FQN and its corresponding ParallelStyle object. If not
specified, the call will do nothing at the moment.

() – Module to be parallelized.

device_mesh (DeviceMesh, optional) – Object which describes the mesh topology of devices for the DTensor.
If not specified, the call must be under a DeviceMesh context.

parallelize_plan (Union[ParallelStyle, Dict[str, ParallelStyle]], optional) – The plan used to parallelize the module. It can be either a
ParallelStyle object which contains how we prepare
input/output for Tensor Parallelism or it can be a dict of module
FQN and its corresponding ParallelStyle object. If not
specified, the call will do nothing at the moment.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 6)

src_data_rank () – the rank of the source data for the logical/global tensor, it is used by
distribute_tensor() to scatter/broadcast the shards/replicas to other ranks. By default,
we use group_rank=0 on each DeviceMesh dimension as the source data to preserve the single-device
semantic. If passing  explicitly, parallelize_module() simply uses its local data instead
of trying to preserve the single-device semantic via scatter/broadcast. Default: 0

A  object parallelized.

torch.distributed.tensor.parallel  parallelize_module ColwiseParallel
 torch.distributed.device_mesh  init_device_mesh

# Define the module.
  
  init_device_mesh 
  parallelize_module   ColwiseParallel  RowwiseParallel

Code example:
torch.distributed.tensor.parallel  parallelize_module ColwiseParallel
 torch.distributed.device_mesh  init_device_mesh

# Define the module.
  
  init_device_mesh 
  parallelize_module   ColwiseParallel  RowwiseParallel

For complex module architecture like Attention, MLP layers, we recommend composing
different ParallelStyles together (i.e. ColwiseParallel and RowwiseParallel) and pass
as a parallelize_plan, to achieves the desired sharding computation.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 7)

Tensor Parallelism supports the following parallel styles:

torch.distributed.tensor.parallel.ColwiseParallel, input_layouts, output_layouts, use_local_output
Partition a compatible nn.Module in a column-wise fashion. Currently supports nn.Linear and nn.Embedding.
Users can compose it together with RowwiseParallel to achieve the sharding of more complicated modules.
(i.e. MLP, Attention)

Keyword Arguments

input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be replicated.
output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is sharded on the last dimension.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.



A ParallelStyle object that represents Colwise sharding of the nn.Module.



 torch.distributed.tensor.parallel  parallelize_module ColwiseParallel
 torch.distributed.device_mesh  init_device_mesh

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 8)

    # m is a nn.Module that contains a "w1" nn.Linear submodule
  init_device_mesh 

# By default, the input of the "w1" Linear will be converted to Replicated DTensor
# and the output of "w1" will return :class:`torch.Tensor` that shards on the last dim.

sharded_mod  parallelize_module   ColwiseParallel







By default ColwiseParallel output is sharded on the last dimension if the output_layouts not
specified, if there’re operators that require specific tensor shape (i.e. before the paired RowwiseParallel),
keep in mind that if the output is sharded the operator might need to be adjusted to the sharded size.

Partition a compatible nn.Module in a column-wise fashion. Currently supports nn.Linear and nn.Embedding.
Users can compose it together with RowwiseParallel to achieve the sharding of more complicated modules.
(i.e. MLP, Attention)

Keyword Arguments

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 9)

input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be replicated.
output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is sharded on the last dimension.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.



A ParallelStyle object that represents Colwise sharding of the nn.Module.

List:
input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be replicated.
output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is sharded on the last dimension.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 10)

input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be replicated.

output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is sharded on the last dimension.

use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.

A ParallelStyle object that represents Colwise sharding of the nn.Module.

torch.distributed.tensor.parallel  parallelize_module ColwiseParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "w1" nn.Linear submodule
  init_device_mesh 

# By default, the input of the "w1" Linear will be converted to Replicated DTensor
# and the output of "w1" will return :class:`torch.Tensor` that shards on the last dim.

sharded_mod  parallelize_module   ColwiseParallel

Code example:
torch.distributed.tensor.parallel  parallelize_module ColwiseParallel
 torch.distributed.device_mesh  init_device_mesh

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 11)

    # m is a nn.Module that contains a "w1" nn.Linear submodule
  init_device_mesh 

# By default, the input of the "w1" Linear will be converted to Replicated DTensor
# and the output of "w1" will return :class:`torch.Tensor` that shards on the last dim.

sharded_mod  parallelize_module   ColwiseParallel

By default ColwiseParallel output is sharded on the last dimension if the output_layouts not
specified, if there’re operators that require specific tensor shape (i.e. before the paired RowwiseParallel),
keep in mind that if the output is sharded the operator might need to be adjusted to the sharded size.

torch.distributed.tensor.parallel.RowwiseParallel, input_layouts, output_layouts, use_local_output
Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.
Users can compose it with ColwiseParallel to achieve the sharding of more complicated modules.
(i.e. MLP, Attention)

Keyword Arguments

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 12)

input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.
output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is replicated.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.



A ParallelStyle object that represents Rowwise sharding of the nn.Module.



 torch.distributed.tensor.parallel  parallelize_module RowwiseParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "w2" nn.Linear submodule
  init_device_mesh 

# By default, the input of the "w2" Linear will be converted to DTensor that shards on the last dim
# and the output of "w2" will return a replicated :class:`torch.Tensor`.

sharded_mod  parallelize_module   RowwiseParallel

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 13)

Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.
Users can compose it with ColwiseParallel to achieve the sharding of more complicated modules.
(i.e. MLP, Attention)

Keyword Arguments

input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.
output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is replicated.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.



A ParallelStyle object that represents Rowwise sharding of the nn.Module.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 14)

List:
input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.
output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is replicated.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.

input_layouts () – The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to
become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.

output_layouts () – The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module
with the user desired layout. If not specified, the output tensor is replicated.

use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: True.

A ParallelStyle object that represents Rowwise sharding of the nn.Module.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 15)

torch.distributed.tensor.parallel  parallelize_module RowwiseParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "w2" nn.Linear submodule
  init_device_mesh 

# By default, the input of the "w2" Linear will be converted to DTensor that shards on the last dim
# and the output of "w2" will return a replicated :class:`torch.Tensor`.

sharded_mod  parallelize_module   RowwiseParallel

Code example:
torch.distributed.tensor.parallel  parallelize_module RowwiseParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "w2" nn.Linear submodule
  init_device_mesh 

# By default, the input of the "w2" Linear will be converted to DTensor that shards on the last dim
# and the output of "w2" will return a replicated :class:`torch.Tensor`.

sharded_mod  parallelize_module   RowwiseParallel

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 16)

torch.distributed.tensor.parallel.SequenceParallel, sequence_dim, use_local_output
SequenceParallel replicates a compatible  parameters and runs the sharded computation with
input sharded on the sequence dimension. This currently supports nn.LayerNorm, nn.Dropout, and the
RMSNorm python implementation
This style implements the operation that is described in the paper
Reducing Activation Recomputation in Large Transformer Models
If the input passed in to this  is a torch.Tensor, it assumes that the input is already sharded
on the sequence dimension and converts the input to a  sharded on the sequence dimension. If the input
passed in to this  is already a  but is not sharded on the sequence dimension, it would
redistribute the input to be sharded on the sequence dimension.
The output of the  will be sharded on the sequence dimension.

Keyword Arguments

sequence_dim () – The sequence dimension of the input tensor for the , this is used to annotate the input tensor to
become a DTensor that is sharded on the sequence dimension, default: 1.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: False.



================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 17)

A ParallelStyle object that represents Sequence Parallel of the .



 torch.distributed.tensor.parallel  parallelize_module SequenceParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "norm" nn.LayerNorm submodule
  init_device_mesh 

# By default, the input of the "norm" will be converted to DTensor that shards on the sequence dim
# and the output of "norm" will return a sharded on sequence dimension :class:`DTensor`.

sharded_mod  parallelize_module   SequenceParallel







SequenceParallel style assumes ones initialization if there are weights in the nn.Module (i.e.
nn.LayerNorm or , and they by default have ones initialization). If you have custom
inits for the weights on those modules, you need to broadcast the weights before/after parallelizing
to ensure that they are replicated.

SequenceParallel replicates a compatible  parameters and runs the sharded computation with
input sharded on the sequence dimension. This currently supports nn.LayerNorm, nn.Dropout, and the
RMSNorm python implementation

This style implements the operation that is described in the paper
Reducing Activation Recomputation in Large Transformer Models

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 18)

If the input passed in to this  is a torch.Tensor, it assumes that the input is already sharded
on the sequence dimension and converts the input to a  sharded on the sequence dimension. If the input
passed in to this  is already a  but is not sharded on the sequence dimension, it would
redistribute the input to be sharded on the sequence dimension.

The output of the  will be sharded on the sequence dimension.

Keyword Arguments

sequence_dim () – The sequence dimension of the input tensor for the , this is used to annotate the input tensor to
become a DTensor that is sharded on the sequence dimension, default: 1.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: False.



A ParallelStyle object that represents Sequence Parallel of the .

List:
sequence_dim () – The sequence dimension of the input tensor for the , this is used to annotate the input tensor to
become a DTensor that is sharded on the sequence dimension, default: 1.
use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: False.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 19)

sequence_dim () – The sequence dimension of the input tensor for the , this is used to annotate the input tensor to
become a DTensor that is sharded on the sequence dimension, default: 1.

use_local_output () – Whether to use local torch.Tensor instead of  for the module output, default: False.

A ParallelStyle object that represents Sequence Parallel of the .

torch.distributed.tensor.parallel  parallelize_module SequenceParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "norm" nn.LayerNorm submodule
  init_device_mesh 

# By default, the input of the "norm" will be converted to DTensor that shards on the sequence dim
# and the output of "norm" will return a sharded on sequence dimension :class:`DTensor`.

sharded_mod  parallelize_module   SequenceParallel

Code example:
torch.distributed.tensor.parallel  parallelize_module SequenceParallel
 torch.distributed.device_mesh  init_device_mesh

    # m is a nn.Module that contains a "norm" nn.LayerNorm submodule
  init_device_mesh 

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 20)

# By default, the input of the "norm" will be converted to DTensor that shards on the sequence dim
# and the output of "norm" will return a sharded on sequence dimension :class:`DTensor`.

sharded_mod  parallelize_module   SequenceParallel

SequenceParallel style assumes ones initialization if there are weights in the nn.Module (i.e.
nn.LayerNorm or , and they by default have ones initialization). If you have custom
inits for the weights on those modules, you need to broadcast the weights before/after parallelizing
to ensure that they are replicated.

To simply configure the nn.Module’s inputs and outputs with DTensor layouts
and perform necessary layout redistributions, without distribute the module
parameters to DTensors, the following ParallelStyle s can be used in
the parallelize_plan when calling parallelize_module:

torch.distributed.tensor.parallel.PrepareModuleInput, input_layouts, desired_input_layouts, input_kwarg_layouts, desired_input_kwarg_layouts, use_local_output
Configure the nn.Module’s inputs to convert the input tensors of the nn.Module to DTensors at runtime according to
input_layouts, and perform layout redistribution according to the desired_input_layouts.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 21)

Keyword Arguments

input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.
desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.
input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None
desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.
use_local_output () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.



A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs.



================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 22)

 torch.distributed.tensor.parallel  parallelize_module PrepareModuleInput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the first input of attn will be annotated to Sharded DTensor
# and then redistributed to Replicated DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan
         PrepareModuleInput
            input_layouts   
            desired_input_layouts

Configure the nn.Module’s inputs to convert the input tensors of the nn.Module to DTensors at runtime according to
input_layouts, and perform layout redistribution according to the desired_input_layouts.

Keyword Arguments

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 23)

input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.
desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.
input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None
desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.
use_local_output () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.



A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 24)

List:
input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.
desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.
input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None
desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.
use_local_output () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 25)

input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.

desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.

input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None

desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.

use_local_output () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.

A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs.

torch.distributed.tensor.parallel  parallelize_module PrepareModuleInput
 torch.distributed.device_mesh  init_device_mesh

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 26)

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the first input of attn will be annotated to Sharded DTensor
# and then redistributed to Replicated DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan
         PrepareModuleInput
            input_layouts   
            desired_input_layouts

Code example:
torch.distributed.tensor.parallel  parallelize_module PrepareModuleInput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the first input of attn will be annotated to Sharded DTensor
# and then redistributed to Replicated DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan
         PrepareModuleInput
            input_layouts   
            desired_input_layouts

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 27)

torch.distributed.tensor.parallel.PrepareModuleOutput, output_layouts, desired_output_layouts, use_local_output
Configure the nn.Module’s outputs to convert the output tensors of the nn.Module to DTensors at runtime according to
output_layouts, and perform layout redistribution according to the desired_output_layouts.

Keyword Arguments

output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.
desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.
use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.



A ParallelStyle object that prepares the sharding layouts of the nn.Module’s outputs.



 torch.distributed.tensor.parallel  parallelize_module PrepareModuleOutput
 torch.distributed.device_mesh  init_device_mesh

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 28)

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor
# and then redistributed to Sharded DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan  PrepareModuleOutput
        output_layouts
        desired_output_layouts

Configure the nn.Module’s outputs to convert the output tensors of the nn.Module to DTensors at runtime according to
output_layouts, and perform layout redistribution according to the desired_output_layouts.

Keyword Arguments

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 29)

output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.
desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.
use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.



A ParallelStyle object that prepares the sharding layouts of the nn.Module’s outputs.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 30)

List:
output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.
desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.
use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.

output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.

desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.

use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.

A ParallelStyle object that prepares the sharding layouts of the nn.Module’s outputs.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 31)

torch.distributed.tensor.parallel  parallelize_module PrepareModuleOutput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor
# and then redistributed to Sharded DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan  PrepareModuleOutput
        output_layouts
        desired_output_layouts

Code example:
torch.distributed.tensor.parallel  parallelize_module PrepareModuleOutput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor
# and then redistributed to Sharded DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan  PrepareModuleOutput
        output_layouts
        desired_output_layouts

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 32)

torch.distributed.tensor.parallel.PrepareModuleInputOutput, input_layouts, desired_input_layouts, input_kwarg_layouts, desired_input_kwarg_layouts, use_local_input, output_layouts, desired_output_layouts, use_local_output
Configure the nn.Module’s inputs (and outputs) to convert the input tensors (and output tensors, respectively) of the nn.Module
to DTensors at runtime according to input_layouts (and output_layouts, respectively), and perform layout redistribution
according to the desired_input_layouts (and desired_output_layouts, respectively). This is a combination of
PrepareModuleInput and PrepareModuleOutput.

Keyword Arguments

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 33)

input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.
desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.
input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None
desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.
use_local_input () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.
output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.
desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.
use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 34)



A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs and outputs.



 torch.distributed.tensor.parallel  parallelize_module PrepareModuleInputOutput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

# According to the style specified below, the first input of attn will be annotated as Sharded DTensor
# and then redistributed to Replicated DTensor, and the output of the TransformerBlock will be annotated
# as Replicated DTensor and then redistributed to Sharded DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan
         PrepareModuleInputOutput
            input_layouts   
            desired_input_layouts   
            output_layouts
            desired_output_layouts

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 35)

Configure the nn.Module’s inputs (and outputs) to convert the input tensors (and output tensors, respectively) of the nn.Module
to DTensors at runtime according to input_layouts (and output_layouts, respectively), and perform layout redistribution
according to the desired_input_layouts (and desired_output_layouts, respectively). This is a combination of
PrepareModuleInput and PrepareModuleOutput.

Keyword Arguments

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 36)

input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.
desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.
input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None
desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.
use_local_input () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.
output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.
desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.
use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 37)



A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs and outputs.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 38)

List:
input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.
desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.
input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None
desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.
use_local_input () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.
output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.
desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.
use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 39)

input_layouts () – The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to
DTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,  need to be specified
as a placeholder. default: None.

desired_input_layouts () – The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. This argument needs to have the same length with input_layouts. default: None.

input_kwarg_layouts () – The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.
default: None

desired_input_kwarg_layouts – (Dict[str, Placement]):
The desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module
have the desired DTensor layouts. default: None.

use_local_input () – Whether to use local torch.Tensor instead of  for the module inputs, default: False.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 40)

output_layouts () – The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to
DTensors if they are torch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,
 need to be specified as a placeholder.

desired_output_layouts () – The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module
have the desired DTensor layouts.

use_local_output () – Whether to use local torch.Tensor instead of  for the module outputs, default: True.

A ParallelStyle object that prepares the sharding layouts of the nn.Module’s inputs and outputs.

torch.distributed.tensor.parallel  parallelize_module PrepareModuleInputOutput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 41)

# According to the style specified below, the first input of attn will be annotated as Sharded DTensor
# and then redistributed to Replicated DTensor, and the output of the TransformerBlock will be annotated
# as Replicated DTensor and then redistributed to Sharded DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan
         PrepareModuleInputOutput
            input_layouts   
            desired_input_layouts   
            output_layouts
            desired_output_layouts

Code example:
torch.distributed.tensor.parallel  parallelize_module PrepareModuleInputOutput
 torch.distributed.device_mesh  init_device_mesh

  TransformerBlock  # block is a nn.Module that contains an "attn" Attention submodule
  init_device_mesh 

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 42)

# According to the style specified below, the first input of attn will be annotated as Sharded DTensor
# and then redistributed to Replicated DTensor, and the output of the TransformerBlock will be annotated
# as Replicated DTensor and then redistributed to Sharded DTensor.
parallelize_module
     # this can be a submodule or module
    
    parallelize_plan
         PrepareModuleInputOutput
            input_layouts   
            desired_input_layouts   
            output_layouts
            desired_output_layouts

when using the Shard(dim) as the input/output layouts for the above
ParallelStyle s, we assume the input/output activation tensors are evenly sharded on
the tensor dimension  on the DeviceMesh that TP operates on. For instance,
since RowwiseParallel accepts input that is sharded on the last dimension, it assumes
the input tensor has already been evenly sharded on the last dimension. For the case of uneven sharded activation tensors, one could pass in DTensor directly to the partitioned modules, and use use_local_output=False to return DTensor after each ParallelStyle, where DTensor could track the uneven sharding information.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 43)

For models like Transformer, we recommend users to use ColwiseParallel
and RowwiseParallel together in the parallelize_plan for achieve the desired
sharding for the entire model (i.e. Attention and MLP).

Parallelized cross-entropy loss computation (loss parallelism), is supported via the following context manager:

torch.distributed.tensor.parallel.loss_parallel
A context manager that enables loss parallelism, where efficient parallelized loss computation
can be performed when the input is sharded on the class dimension. Currently only the cross-entropy
loss is supported.
Within this context manager, one can use cross_entropy() or
CrossEntropyLoss as usual, with the following assumptions on the input parameters.
The corresponding backward() call, if any, also needs to happen under this context manager.

Parameters

 () – Input logits. Assumed to be sharded on the class dimension.
 (Union[torch.Tensor, ]) – Must be ground truth class indices (class probabilities currently not supported).
Assumed to be replicated across the DeviceMesh.
 (Union[torch.Tensor, ], optional) – If given, assumed to be replicated across the DeviceMesh.
label_smoothing – Currently not supported.



A replicated .



================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 44)

A sharded DTensor is manually created here to showcase the usage.
In practice, it is usually the output of a TP module.
 torch.distributed.tensor.parallel  loss_parallel
 torch.distributed.device_mesh  init_device_mesh

device_mesh  init_device_mesh 
     requires_grad
dist_input  distribute_tensor device_mesh placements
    
 loss_parallel
      cross_entropydist_input

A context manager that enables loss parallelism, where efficient parallelized loss computation
can be performed when the input is sharded on the class dimension. Currently only the cross-entropy
loss is supported.

Within this context manager, one can use cross_entropy() or
CrossEntropyLoss as usual, with the following assumptions on the input parameters.
The corresponding backward() call, if any, also needs to happen under this context manager.

Parameters

 () – Input logits. Assumed to be sharded on the class dimension.
 (Union[torch.Tensor, ]) – Must be ground truth class indices (class probabilities currently not supported).
Assumed to be replicated across the DeviceMesh.
 (Union[torch.Tensor, ], optional) – If given, assumed to be replicated across the DeviceMesh.
label_smoothing – Currently not supported.



================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 45)

A replicated .

List:
() – Input logits. Assumed to be sharded on the class dimension.
 (Union[torch.Tensor, ]) – Must be ground truth class indices (class probabilities currently not supported).
Assumed to be replicated across the DeviceMesh.
 (Union[torch.Tensor, ], optional) – If given, assumed to be replicated across the DeviceMesh.
label_smoothing – Currently not supported.

() – Input logits. Assumed to be sharded on the class dimension.

(Union[torch.Tensor, ]) – Must be ground truth class indices (class probabilities currently not supported).
Assumed to be replicated across the DeviceMesh.

(Union[torch.Tensor, ], optional) – If given, assumed to be replicated across the DeviceMesh.

label_smoothing – Currently not supported.

A sharded DTensor is manually created here to showcase the usage.
In practice, it is usually the output of a TP module.

Code example:
torch.distributed.tensor.parallel  loss_parallel
 torch.distributed.device_mesh  init_device_mesh

device_mesh  init_device_mesh 
     requires_grad
dist_input  distribute_tensor device_mesh placements
    
 loss_parallel
      cross_entropydist_input

Code example:
The loss_parallel API is experimental and subject to change.

================================================================================

# Tensor Parallelism - torch.distributed.tensor.parallel (Part 46)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Distributed Optimizers (Part 1)

Created On: Mar 01, 2021 | Last Updated On: Jun 16, 2025

Distributed optimizer is not currently supported when using CUDA tensors

torch.distributed.optim exposes DistributedOptimizer, which takes a list
of remote parameters () and runs the
optimizer locally on the workers where the parameters live.  The distributed
optimizer can use any of the local optimizer Base class to
apply the gradients on each worker.

================================================================================

# Distributed Optimizers (Part 2)

torch.distributed.optim.DistributedOptimizeroptimizer_class, params_rref, , 
DistributedOptimizer takes remote references to parameters scattered
across workers and applies the given optimizer locally for each parameter.
This class uses get_gradients() in order
to retrieve the gradients for specific parameters.
Concurrent calls to
,
either from the same or different clients, will
be serialized on each worker – as each worker’s optimizer can only work
on one set of gradients at a time. However, there is no guarantee that
the full forward-backward-optimizer sequence will execute for one client
at a time. This means that the gradients being applied may not correspond
to the latest forward pass executed on a given worker. Also, there is no
guaranteed ordering across workers.
DistributedOptimizer creates the local optimizer with TorchScript enabled
by default, so that optimizer updates are not blocked by the Python Global
Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed
Model Parallel). This feature is currently enabled for most optimizers. You
can also follow the recipe in PyTorch tutorials to enable TorchScript support
for your own custom optimizers.

================================================================================

# Distributed Optimizers (Part 3)

Parameters

optimizer_class (optim.Optimizer) – the class of optimizer to
instantiate on each worker.
params_rref () – list of RRefs to local or remote parameters
to optimize.
 – arguments to pass to the optimizer constructor on each worker.
 – arguments to pass to the optimizer constructor on each worker.




 torch.distributed.autograd  dist_autograd
 torch.distributed.rpc  
   
 torch.distributed.optim  DistributedOptimizer

 dist_autograd  context_id
  # Forward pass.
       
       
      

  # Backward pass.
  dist_autogradcontext_id 

  # Optimizer.
  dist_optim  DistributedOptimizer
     
      
     
  
  dist_optimcontext_id






context_id
Performs a single optimization step.
This will call torch.optim.Optimizer.step() on each worker
containing parameters to be optimized, and will block until all workers
return. The provided context_id will be used to retrieve the
corresponding  that
contains the gradients that should be applied to the parameters.

Parameters
context_id – the autograd context id for which we should run the
optimizer step.

DistributedOptimizer takes remote references to parameters scattered
across workers and applies the given optimizer locally for each parameter.

================================================================================

# Distributed Optimizers (Part 4)

This class uses get_gradients() in order
to retrieve the gradients for specific parameters.

Concurrent calls to
,
either from the same or different clients, will
be serialized on each worker – as each worker’s optimizer can only work
on one set of gradients at a time. However, there is no guarantee that
the full forward-backward-optimizer sequence will execute for one client
at a time. This means that the gradients being applied may not correspond
to the latest forward pass executed on a given worker. Also, there is no
guaranteed ordering across workers.

DistributedOptimizer creates the local optimizer with TorchScript enabled
by default, so that optimizer updates are not blocked by the Python Global
Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed
Model Parallel). This feature is currently enabled for most optimizers. You
can also follow the recipe in PyTorch tutorials to enable TorchScript support
for your own custom optimizers.

Parameters

================================================================================

# Distributed Optimizers (Part 5)

optimizer_class (optim.Optimizer) – the class of optimizer to
instantiate on each worker.
params_rref () – list of RRefs to local or remote parameters
to optimize.
 – arguments to pass to the optimizer constructor on each worker.
 – arguments to pass to the optimizer constructor on each worker.

List:
optimizer_class (optim.Optimizer) – the class of optimizer to
instantiate on each worker.
params_rref () – list of RRefs to local or remote parameters
to optimize.
 – arguments to pass to the optimizer constructor on each worker.
 – arguments to pass to the optimizer constructor on each worker.

optimizer_class (optim.Optimizer) – the class of optimizer to
instantiate on each worker.

params_rref () – list of RRefs to local or remote parameters
to optimize.

– arguments to pass to the optimizer constructor on each worker.

– arguments to pass to the optimizer constructor on each worker.

torch.distributed.autograd  dist_autograd
 torch.distributed.rpc  
   
 torch.distributed.optim  DistributedOptimizer

 dist_autograd  context_id
  # Forward pass.
       
       
      

  # Backward pass.
  dist_autogradcontext_id 

================================================================================

# Distributed Optimizers (Part 6)

  # Optimizer.
  dist_optim  DistributedOptimizer
     
      
     
  
  dist_optimcontext_id

Code example:
torch.distributed.autograd  dist_autograd
 torch.distributed.rpc  
   
 torch.distributed.optim  DistributedOptimizer

 dist_autograd  context_id
  # Forward pass.
       
       
      

  # Backward pass.
  dist_autogradcontext_id 

  # Optimizer.
  dist_optim  DistributedOptimizer
     
      
     
  
  dist_optimcontext_id

context_id
Performs a single optimization step.
This will call torch.optim.Optimizer.step() on each worker
containing parameters to be optimized, and will block until all workers
return. The provided context_id will be used to retrieve the
corresponding  that
contains the gradients that should be applied to the parameters.

Parameters
context_id – the autograd context id for which we should run the
optimizer step.

Performs a single optimization step.

This will call torch.optim.Optimizer.step() on each worker
containing parameters to be optimized, and will block until all workers
return. The provided context_id will be used to retrieve the
corresponding  that
contains the gradients that should be applied to the parameters.

================================================================================

# Distributed Optimizers (Part 7)

Parameters
context_id – the autograd context id for which we should run the
optimizer step.

context_id – the autograd context id for which we should run the
optimizer step.

torch.distributed.optim.PostLocalSGDOptimizer, 
Wraps an arbitrary torch.optim.Optimizer and runs post-local SGD,
This optimizer runs local optimizer at every step.
After the warm-up stage, it averages parameters periodically after the local optimizer is applied.

Parameters

 () – The local optimizer.
 (ModelAverager) – A model averager instance to run post-localSGD algorithm.




 
 torch.distributed  
 torch.distributed.algorithms.model_averaging.averagers  
   
 torch.distributed.optim  PostLocalSGDOptimizer
 torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook  
  PostLocalSGDState
  post_localSGD_hook


  DistributedDataParallel
    device_ids output_device


# Register a post-localSGD communication hook.
  PostLocalSGDStateprocess_group  start_localSGD_iter
register_comm_hook post_localSGD_hook

================================================================================

# Distributed Optimizers (Part 8)

# Create a post-localSGD optimizer that wraps a local optimizer.
# Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as
# ``start_localSGD_iter`` used in ``PostLocalSGDState``.
local_optim  parameters 
  PostLocalSGDOptimizer
    local_optim
    PeriodicModelAverager warmup_steps


# In the first 100 steps, DDP runs global gradient averaging at every step.
# After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),
# and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.
    
   
      
   
   




load_state_dictstate_dict
This is the same as torch.optim.Optimizer load_state_dict(),
but also restores model averager’s step value to the one
saved in the provided state_dict.
If there is no  entry in state_dict,
it will raise a warning and initialize the model averager’s step to 0.



state_dict
This is the same as torch.optim.Optimizer state_dict(),
but adds an extra entry to record model averager’s step to the checkpoint
to ensure reload does not cause unnecessary warm up again.




Performs a single optimization step (parameter update).

================================================================================

# Distributed Optimizers (Part 9)

Wraps an arbitrary torch.optim.Optimizer and runs post-local SGD,
This optimizer runs local optimizer at every step.
After the warm-up stage, it averages parameters periodically after the local optimizer is applied.

Parameters

 () – The local optimizer.
 (ModelAverager) – A model averager instance to run post-localSGD algorithm.

List:
() – The local optimizer.
 (ModelAverager) – A model averager instance to run post-localSGD algorithm.

() – The local optimizer.

(ModelAverager) – A model averager instance to run post-localSGD algorithm.

Code example:
torch.distributed  
 torch.distributed.algorithms.model_averaging.averagers  
   
 torch.distributed.optim  PostLocalSGDOptimizer
 torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook  
  PostLocalSGDState
  post_localSGD_hook


  DistributedDataParallel
    device_ids output_device


# Register a post-localSGD communication hook.
  PostLocalSGDStateprocess_group  start_localSGD_iter
register_comm_hook post_localSGD_hook

================================================================================

# Distributed Optimizers (Part 10)

# Create a post-localSGD optimizer that wraps a local optimizer.
# Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as
# ``start_localSGD_iter`` used in ``PostLocalSGDState``.
local_optim  parameters 
  PostLocalSGDOptimizer
    local_optim
    PeriodicModelAverager warmup_steps


# In the first 100 steps, DDP runs global gradient averaging at every step.
# After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),
# and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.

load_state_dictstate_dict
This is the same as torch.optim.Optimizer load_state_dict(),
but also restores model averager’s step value to the one
saved in the provided state_dict.
If there is no  entry in state_dict,
it will raise a warning and initialize the model averager’s step to 0.

This is the same as torch.optim.Optimizer load_state_dict(),
but also restores model averager’s step value to the one
saved in the provided state_dict.

If there is no  entry in state_dict,
it will raise a warning and initialize the model averager’s step to 0.

================================================================================

# Distributed Optimizers (Part 11)

state_dict
This is the same as torch.optim.Optimizer state_dict(),
but adds an extra entry to record model averager’s step to the checkpoint
to ensure reload does not cause unnecessary warm up again.

This is the same as torch.optim.Optimizer state_dict(),
but adds an extra entry to record model averager’s step to the checkpoint
to ensure reload does not cause unnecessary warm up again.

Performs a single optimization step (parameter update).

Performs a single optimization step (parameter update).

================================================================================

# Distributed Optimizers (Part 12)

torch.distributed.optim.ZeroRedundancyOptimizer, optimizer_class, process_group, parameters_as_bucket_view, overlap_with_ddp, 
Wrap an arbitrary optim.Optimizer and shards its states across ranks in the group.
The sharing is done as described by .
The local optimizer instance in each rank is only
responsible for updating approximately   world_size parameters and
hence only needs to keep   world_size optimizer states. After
parameters are updated locally, each rank will broadcast its parameters to
all other peers to keep all model replicas in the same state.
ZeroRedundancyOptimizer can be used in conjunction with
torch.nn.parallel.DistributedDataParallel to reduce per-rank peak
memory consumption.
ZeroRedundancyOptimizer uses a sorted-greedy algorithm to pack a number
of parameters at each rank. Each parameter belongs to a single rank and is
not divided among ranks. The partition is arbitrary and might not match the
the parameter registration or usage order.

Parameters
 () – an  of torch.Tensor s
or  s giving all parameters, which will be sharded
across ranks.

Keyword Arguments

================================================================================

# Distributed Optimizers (Part 13)

optimizer_class (torch.nn.Optimizer) – the class of the local
optimizer.
process_group (ProcessGroup, optional) – torch.distributed
ProcessGroup (default: dist.group.WORLD initialized by
torch.distributed.init_process_group()).
parameters_as_bucket_view () – if , parameters are
packed into buckets to speed up communication, and param.data
fields point to bucket views at different offsets; if ,
each individual parameter is communicated separately, and each
params.data stays intact (default: ).
overlap_with_ddp () – if ,  is
overlapped with DistributedDataParallel ‘s gradient
synchronization; this requires (1) either a functional optimizer
for the optimizer_class argument or one with a functional
equivalent and (2) registering a DDP communication hook
constructed from one of the functions in ddp_zero_hook.py;
parameters are packed into buckets matching those in
DistributedDataParallel, meaning that the
parameters_as_bucket_view argument is ignored.
If ,  runs disjointly after the backward pass
(per normal).
(default: )
**defaults – any trailing arguments, which are forwarded to the local
optimizer.



================================================================================

# Distributed Optimizers (Part 14)


   
 torch.distributed.optim  ZeroRedundancyOptimizer
 torch.nn.parallel  DistributedDataParallel  
  Sequential     
   device_ids
  ZeroRedundancyOptimizer
    parameters
    optimizer_class
    







Currently, ZeroRedundancyOptimizer requires that all of the
passed-in parameters are the same dense type.



If you pass overlap_with_ddp=True, be wary of the following: Given
the way that overlapping DistributedDataParallel with
ZeroRedundancyOptimizer is currently implemented, the first
two or three training iterations do not perform parameter updates in
the optimizer step, depending on if static_graph=False or
static_graph=True, respectively. This is because it needs
information about the gradient bucketing strategy used by
DistributedDataParallel, which is not finalized until the
second forward pass if static_graph=False or until the third
forward pass if static_graph=True. To adjust for this, one option
is to prepend dummy inputs.



ZeroRedundancyOptimizer is experimental and subject to change.



================================================================================

# Distributed Optimizers (Part 15)

add_param_groupparam_group
Add a parameter group to the  ‘s param_groups.
This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to the  as
training progresses.

Parameters
param_group () – specifies the parameters to be optimized and
group-specific optimization options.




This method handles updating the shards on all partitions
but needs to be called on all ranks. Calling this on a subset of
the ranks will cause the training to hang because communication
primitives are called depending on the managed parameters and
expect all the ranks to participate on the same set of parameters.




consolidate_state_dict
Consolidate a list of state_dict s (one per rank) on the target rank.

Parameters
 () – the rank that receives the optimizer states (default: 0).


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.




This needs to be called on all ranks.




join_device
Return default device.



================================================================================

# Distributed Optimizers (Part 16)


Return the ZeRO join hook.
It enables training on uneven inputs by
shadowing the collective communications in the optimizer step.
Gradients must be properly set before this hook is called.

Parameters
 () – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .


This hook does not support any keyword arguments; i.e.  is
unused.



join_process_group
Return process group.



load_state_dictstate_dict
Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.

Parameters
state_dict () – optimizer state; should be an object returned
from a call to state_dict().


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.





state_dict
Return the last global optimizer state known to this rank.

================================================================================

# Distributed Optimizers (Part 17)


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt; or if this method is called without a preceding call
    to consolidate_state_dict().

Return type
[, ]





, 
Perform a single optimizer step and syncs parameters across all ranks.

Parameters
 () – a closure that re-evaluates the model and
returns the loss; optional for most optimizers.


Optional loss depending on the underlying local optimizer.

Return type
[]




Any extra parameters are passed to the base optimizer as-is.

Wrap an arbitrary optim.Optimizer and shards its states across ranks in the group.

The sharing is done as described by .

================================================================================

# Distributed Optimizers (Part 18)

The local optimizer instance in each rank is only
responsible for updating approximately   world_size parameters and
hence only needs to keep   world_size optimizer states. After
parameters are updated locally, each rank will broadcast its parameters to
all other peers to keep all model replicas in the same state.
ZeroRedundancyOptimizer can be used in conjunction with
torch.nn.parallel.DistributedDataParallel to reduce per-rank peak
memory consumption.

ZeroRedundancyOptimizer uses a sorted-greedy algorithm to pack a number
of parameters at each rank. Each parameter belongs to a single rank and is
not divided among ranks. The partition is arbitrary and might not match the
the parameter registration or usage order.

Parameters
 () – an  of torch.Tensor s
or  s giving all parameters, which will be sharded
across ranks.

Keyword Arguments

================================================================================

# Distributed Optimizers (Part 19)

optimizer_class (torch.nn.Optimizer) – the class of the local
optimizer.
process_group (ProcessGroup, optional) – torch.distributed
ProcessGroup (default: dist.group.WORLD initialized by
torch.distributed.init_process_group()).
parameters_as_bucket_view () – if , parameters are
packed into buckets to speed up communication, and param.data
fields point to bucket views at different offsets; if ,
each individual parameter is communicated separately, and each
params.data stays intact (default: ).
overlap_with_ddp () – if ,  is
overlapped with DistributedDataParallel ‘s gradient
synchronization; this requires (1) either a functional optimizer
for the optimizer_class argument or one with a functional
equivalent and (2) registering a DDP communication hook
constructed from one of the functions in ddp_zero_hook.py;
parameters are packed into buckets matching those in
DistributedDataParallel, meaning that the
parameters_as_bucket_view argument is ignored.
If ,  runs disjointly after the backward pass
(per normal).
(default: )
**defaults – any trailing arguments, which are forwarded to the local
optimizer.

================================================================================

# Distributed Optimizers (Part 20)

() – an  of torch.Tensor s
or  s giving all parameters, which will be sharded
across ranks.

================================================================================

# Distributed Optimizers (Part 21)

List:
optimizer_class (torch.nn.Optimizer) – the class of the local
optimizer.
process_group (ProcessGroup, optional) – torch.distributed
ProcessGroup (default: dist.group.WORLD initialized by
torch.distributed.init_process_group()).
parameters_as_bucket_view () – if , parameters are
packed into buckets to speed up communication, and param.data
fields point to bucket views at different offsets; if ,
each individual parameter is communicated separately, and each
params.data stays intact (default: ).
overlap_with_ddp () – if ,  is
overlapped with DistributedDataParallel ‘s gradient
synchronization; this requires (1) either a functional optimizer
for the optimizer_class argument or one with a functional
equivalent and (2) registering a DDP communication hook
constructed from one of the functions in ddp_zero_hook.py;
parameters are packed into buckets matching those in
DistributedDataParallel, meaning that the
parameters_as_bucket_view argument is ignored.
If ,  runs disjointly after the backward pass
(per normal).
(default: )
**defaults – any trailing arguments, which are forwarded to the local
optimizer.

optimizer_class (torch.nn.Optimizer) – the class of the local
optimizer.

================================================================================

# Distributed Optimizers (Part 22)

process_group (ProcessGroup, optional) – torch.distributed
ProcessGroup (default: dist.group.WORLD initialized by
torch.distributed.init_process_group()).

parameters_as_bucket_view () – if , parameters are
packed into buckets to speed up communication, and param.data
fields point to bucket views at different offsets; if ,
each individual parameter is communicated separately, and each
params.data stays intact (default: ).

overlap_with_ddp () – if ,  is
overlapped with DistributedDataParallel ‘s gradient
synchronization; this requires (1) either a functional optimizer
for the optimizer_class argument or one with a functional
equivalent and (2) registering a DDP communication hook
constructed from one of the functions in ddp_zero_hook.py;
parameters are packed into buckets matching those in
DistributedDataParallel, meaning that the
parameters_as_bucket_view argument is ignored.
If ,  runs disjointly after the backward pass
(per normal).
(default: )

**defaults – any trailing arguments, which are forwarded to the local
optimizer.

================================================================================

# Distributed Optimizers (Part 23)

Code example:
torch.distributed.optim  ZeroRedundancyOptimizer
 torch.nn.parallel  DistributedDataParallel  
  Sequential     
   device_ids
  ZeroRedundancyOptimizer
    parameters
    optimizer_class

Currently, ZeroRedundancyOptimizer requires that all of the
passed-in parameters are the same dense type.

If you pass overlap_with_ddp=True, be wary of the following: Given
the way that overlapping DistributedDataParallel with
ZeroRedundancyOptimizer is currently implemented, the first
two or three training iterations do not perform parameter updates in
the optimizer step, depending on if static_graph=False or
static_graph=True, respectively. This is because it needs
information about the gradient bucketing strategy used by
DistributedDataParallel, which is not finalized until the
second forward pass if static_graph=False or until the third
forward pass if static_graph=True. To adjust for this, one option
is to prepend dummy inputs.

ZeroRedundancyOptimizer is experimental and subject to change.

================================================================================

# Distributed Optimizers (Part 24)

add_param_groupparam_group
Add a parameter group to the  ‘s param_groups.
This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to the  as
training progresses.

Parameters
param_group () – specifies the parameters to be optimized and
group-specific optimization options.




This method handles updating the shards on all partitions
but needs to be called on all ranks. Calling this on a subset of
the ranks will cause the training to hang because communication
primitives are called depending on the managed parameters and
expect all the ranks to participate on the same set of parameters.

Add a parameter group to the  ‘s param_groups.

This can be useful when fine tuning a pre-trained network, as frozen
layers can be made trainable and added to the  as
training progresses.

Parameters
param_group () – specifies the parameters to be optimized and
group-specific optimization options.

param_group () – specifies the parameters to be optimized and
group-specific optimization options.

================================================================================

# Distributed Optimizers (Part 25)

This method handles updating the shards on all partitions
but needs to be called on all ranks. Calling this on a subset of
the ranks will cause the training to hang because communication
primitives are called depending on the managed parameters and
expect all the ranks to participate on the same set of parameters.

consolidate_state_dict
Consolidate a list of state_dict s (one per rank) on the target rank.

Parameters
 () – the rank that receives the optimizer states (default: 0).


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.




This needs to be called on all ranks.

Consolidate a list of state_dict s (one per rank) on the target rank.

Parameters
 () – the rank that receives the optimizer states (default: 0).


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.

() – the rank that receives the optimizer states (default: 0).

================================================================================

# Distributed Optimizers (Part 26)

RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.

This needs to be called on all ranks.

join_device
Return default device.

Return default device.

Return the ZeRO join hook.
It enables training on uneven inputs by
shadowing the collective communications in the optimizer step.
Gradients must be properly set before this hook is called.

Parameters
 () – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .


This hook does not support any keyword arguments; i.e.  is
unused.

Return the ZeRO join hook.

It enables training on uneven inputs by
shadowing the collective communications in the optimizer step.

Gradients must be properly set before this hook is called.

Parameters
 () – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .

================================================================================

# Distributed Optimizers (Part 27)

() – a  containing any keyword arguments
to modify the behavior of the join hook at run time; all
 instances sharing the same join context
manager are forwarded the same value for .

This hook does not support any keyword arguments; i.e.  is
unused.

join_process_group
Return process group.

Return process group.

load_state_dictstate_dict
Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.

Parameters
state_dict () – optimizer state; should be an object returned
from a call to state_dict().


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.

Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.

Parameters
state_dict () – optimizer state; should be an object returned
from a call to state_dict().

================================================================================

# Distributed Optimizers (Part 28)


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.

state_dict () – optimizer state; should be an object returned
from a call to state_dict().

RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt.

state_dict
Return the last global optimizer state known to this rank.


RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt; or if this method is called without a preceding call
    to consolidate_state_dict().

Return type
[, ]

Return the last global optimizer state known to this rank.

================================================================================

# Distributed Optimizers (Part 29)

RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt; or if this method is called without a preceding call
    to consolidate_state_dict().

Return type
[, ]

RuntimeError – if overlap_with_ddp=True and this method is
    called before this ZeroRedundancyOptimizer instance
    has been fully initialized, which happens once
    DistributedDataParallel gradient buckets have been
    rebuilt; or if this method is called without a preceding call
    to consolidate_state_dict().

, 
Perform a single optimizer step and syncs parameters across all ranks.

Parameters
 () – a closure that re-evaluates the model and
returns the loss; optional for most optimizers.


Optional loss depending on the underlying local optimizer.

Return type
[]




Any extra parameters are passed to the base optimizer as-is.

Perform a single optimizer step and syncs parameters across all ranks.

Parameters
 () – a closure that re-evaluates the model and
returns the loss; optional for most optimizers.

================================================================================

# Distributed Optimizers (Part 30)


Optional loss depending on the underlying local optimizer.

Return type
[]

() – a closure that re-evaluates the model and
returns the loss; optional for most optimizers.

Optional loss depending on the underlying local optimizer.

Any extra parameters are passed to the base optimizer as-is.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# Get Started

Choose Your Path: Install PyTorch Locally or Launch Instantly on Supported Cloud Platforms

================================================================================

# Get Started - Join PyTorch Foundation

As a member of the PyTorch Foundation, you’ll have access to resources that allow you to be stewards of stable, secure, and long-lasting codebases. You can collaborate on training, local and regional events, open-source developer tooling, academic research, and guides to help new users and contributors have a productive experience.

================================================================================

# Get Started - Production Ready

Transition seamlessly between eager and graph modes with TorchScript, and accelerate the path to production with TorchServe.

================================================================================

# Get Started - Distributed Training

Scalable distributed training and performance optimization in research and production is enabled by the torch.distributed backend.

================================================================================

# Get Started - Robust Ecosystem

A rich ecosystem of tools and libraries extends PyTorch and supports development in computer vision, NLP and more.

================================================================================

# Get Started - Cloud Support

PyTorch is well supported on major cloud platforms, providing frictionless development and easy scaling.

================================================================================

# Get Started - Install PyTorch

Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. This should be suitable for many users. Preview is available if you want the latest, not fully tested and supported, builds that are generated nightly. Please ensure that you have met the prerequisites below (e.g., numpy), depending on your package manager. Anaconda is our recommended package manager since it installs all dependencies. You can also install previous versions of PyTorch. Note that LibTorch is only available for C++.

Latest Stable PyTorch requires Python 3.9 or later. Latest Preview (Nightly) PyTorch  requires Python 3.10 or later.

Code example:
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

================================================================================

# Get Started - Quick Start With Cloud Partners

Get up and running with PyTorch quickly through popular cloud platforms and machine learning services.

================================================================================

# Get Started - Amazon Web Services

List:
PyTorch on AWS
Amazon SageMaker
AWS Deep Learning Containers
AWS Deep Learning AMIs

================================================================================

# Get Started - Google Cloud Platform

List:
Cloud Deep Learning VM Image
Deep Learning Containers

================================================================================

# Get Started - Microsoft Azure

List:
PyTorch on Azure
Azure Machine Learning
Azure Functions

================================================================================

# Get Started - Featured Projects

Explore a rich ecosystem of libraries, tools, and more to support development.

================================================================================

# Get Started - Captum

Captum (“comprehension” in Latin) is an open source, extensible library for model interpretability built on PyTorch.

================================================================================

# Get Started - PyTorch Geometric

PyTorch Geometric is a library for deep learning on irregular input data such as graphs, point clouds, and manifolds.

================================================================================

# Get Started - skorch

skorch is a high-level library for PyTorch that provides full scikit-learn compatibility.

================================================================================

# Get Started - Amazon Advertising

Reduce inference costs by 71% and scale out using PyTorch, TorchServe, and AWS Inferentia.

================================================================================

# Get Started - Salesforce

Pushing the state of the art in NLP and Multi-task learning.

================================================================================

# Get Started - Stanford University

Using PyTorch’s flexibility to efficiently research new algorithmic approaches.

================================================================================

# torch.is_tensor

, 
Returns True if  is a PyTorch tensor.
Note that this function is simply doing isinstance(obj, .
Using that isinstance check is better for typechecking with mypy,
and more explicit - so it’s recommended to use that instead of
.

Parameters
 () – Object to test

Return type
typing_extensions.TypeIs[]

Returns True if  is a PyTorch tensor.

Note that this function is simply doing isinstance(obj, .
Using that isinstance check is better for typechecking with mypy,
and more explicit - so it’s recommended to use that instead of
.

Parameters
 () – Object to test

Return type
typing_extensions.TypeIs[]

typing_extensions.TypeIs[]

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.is_storage

is_storage, 
Returns True if  is a PyTorch storage object.

Parameters
 () – Object to test

Return type
typing_extensions.TypeIs[[TypedStorage, UntypedStorage]]

Returns True if  is a PyTorch storage object.

Parameters
 () – Object to test

Return type
typing_extensions.TypeIs[[TypedStorage, UntypedStorage]]

typing_extensions.TypeIs[[TypedStorage, UntypedStorage]]

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.is_complex

is_complex
Returns True if the data type of  is a complex data type i.e.,
one of torch.complex64, and torch.complex128.

Parameters
 () – the input tensor.

Returns True if the data type of  is a complex data type i.e.,
one of torch.complex64, and torch.complex128.

Parameters
 () – the input tensor.

() – the input tensor.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.is_conj

Returns True if the  is a conjugated tensor, i.e. its conjugate bit is set to .

Parameters
 () – the input tensor.

Returns True if the  is a conjugated tensor, i.e. its conjugate bit is set to .

Parameters
 () – the input tensor.

() – the input tensor.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.is_floating_point

is_floating_point
Returns True if the data type of  is a floating point data type i.e.,
one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.

Parameters
 () – the input tensor.

Returns True if the data type of  is a floating point data type i.e.,
one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.

Parameters
 () – the input tensor.

() – the input tensor.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.is_nonzero (Part 1)

is_nonzero
Returns True if the  is a single element tensor which is not equal to zero
after type conversions.
i.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or
torch.tensor([False]).
Throws a RuntimeError if torch.numel()   (even in case
of sparse tensors).

Parameters
 () – the input tensor.



is_nonzero

is_nonzero

is_nonzero

is_nonzero

is_nonzero  
Traceback (most recent call last):

RuntimeError: bool value of Tensor with more than one value is ambiguous
is_nonzero
Traceback (most recent call last):

RuntimeError: bool value of Tensor with no values is ambiguous

Returns True if the  is a single element tensor which is not equal to zero
after type conversions.
i.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or
torch.tensor([False]).
Throws a RuntimeError if torch.numel()   (even in case
of sparse tensors).

Parameters
 () – the input tensor.

() – the input tensor.

Code example:
is_nonzero

is_nonzero

is_nonzero

is_nonzero

is_nonzero  
Traceback (most recent call last):

RuntimeError: bool value of Tensor with more than one value is ambiguous
is_nonzero
Traceback (most recent call last):

RuntimeError: bool value of Tensor with no values is ambiguous

================================================================================

# torch.is_nonzero (Part 2)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.set_default_dtype (Part 1)

set_default_dtype, 
Sets the default floating point dtype to . Supports floating point dtype
as inputs. Other dtypes will cause torch to raise an exception.
When PyTorch is initialized its default floating point dtype is torch.float32,
and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like
type inference. The default floating point dtype is used to:

Implicitly determine the default complex dtype. When the default floating type is float16,
the default complex dtype is complex32. For float32, the default complex dtype is complex64.
For float64, it is complex128. For bfloat16, an exception will be raised because
there is no corresponding complex type for bfloat16.
Infer the dtype for tensors constructed using Python floats or complex Python
numbers. See examples below.
Determine the result of type promotion between bool and integer tensors and
Python floats and complex Python numbers.


Parameters
 (torch.dtype) – the floating point dtype to make the default.



================================================================================

# torch.set_default_dtype (Part 2)

# initial default for floating point is torch.float32
# Python floats are interpreted as float32
 
torch.float32
# initial default for floating point is torch.complex64
# Complex Python numbers are interpreted as complex64
 
torch.complex64


set_default_dtype
# Python floats are now interpreted as float64
   # a new floating point tensor
torch.float64
# Complex Python numbers are now interpreted as complex128
   # a new complex tensor
torch.complex128


set_default_dtype
# Python floats are now interpreted as float16
   # a new floating point tensor
torch.float16
# Complex Python numbers are now interpreted as complex128
   # a new complex tensor
torch.complex32

Sets the default floating point dtype to . Supports floating point dtype
as inputs. Other dtypes will cause torch to raise an exception.

When PyTorch is initialized its default floating point dtype is torch.float32,
and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like
type inference. The default floating point dtype is used to:

================================================================================

# torch.set_default_dtype (Part 3)

List:
Implicitly determine the default complex dtype. When the default floating type is float16,
the default complex dtype is complex32. For float32, the default complex dtype is complex64.
For float64, it is complex128. For bfloat16, an exception will be raised because
there is no corresponding complex type for bfloat16.
Infer the dtype for tensors constructed using Python floats or complex Python
numbers. See examples below.
Determine the result of type promotion between bool and integer tensors and
Python floats and complex Python numbers.

Implicitly determine the default complex dtype. When the default floating type is float16,
the default complex dtype is complex32. For float32, the default complex dtype is complex64.
For float64, it is complex128. For bfloat16, an exception will be raised because
there is no corresponding complex type for bfloat16.

Infer the dtype for tensors constructed using Python floats or complex Python
numbers. See examples below.

Determine the result of type promotion between bool and integer tensors and
Python floats and complex Python numbers.

Parameters
 (torch.dtype) – the floating point dtype to make the default.

================================================================================

# torch.set_default_dtype (Part 4)

(torch.dtype) – the floating point dtype to make the default.

Code example:
# initial default for floating point is torch.float32
# Python floats are interpreted as float32
 
torch.float32
# initial default for floating point is torch.complex64
# Complex Python numbers are interpreted as complex64
 
torch.complex64

Code example:
set_default_dtype
# Python floats are now interpreted as float64
   # a new floating point tensor
torch.float64
# Complex Python numbers are now interpreted as complex128
   # a new complex tensor
torch.complex128

Code example:
set_default_dtype
# Python floats are now interpreted as float16
   # a new floating point tensor
torch.float16
# Complex Python numbers are now interpreted as complex128
   # a new complex tensor
torch.complex32

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.get_default_dtype

get_default_dtype  torch.dtype
Get the current default floating point torch.dtype.

get_default_dtype  # initial default for floating point is torch.float32
torch.float32
set_default_dtype
get_default_dtype  # default is now changed to torch.float64
torch.float64

Get the current default floating point torch.dtype.

Code example:
get_default_dtype  # initial default for floating point is torch.float32
torch.float32
set_default_dtype
get_default_dtype  # default is now changed to torch.float64
torch.float64

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.set_default_device (Part 1)

set_default_device
Sets the default torch.Tensor to be allocated on .  This
does not affect factory function calls which are called with an explicit
 argument.  Factory calls will be performed as if they
were passed  as an argument.
To only temporarily change the default device instead of setting it
globally, use  torch.device(device): instead.
The default device is initially .  If you set the default tensor
device to another device (e.g., ) without a device index, tensors
will be allocated on whatever the current device for the device type,
even after torch.cuda.set_device() is called.


This function imposes a slight performance cost on every Python
call to the torch API (not just factory functions).  If this
is causing problems for you, please comment on
pytorch/pytorch#92701



This doesn’t affect functions that create tensors that share the same memory as the input, like:
torch.from_numpy() and torch.frombuffer()


Parameters
 () – the device to set as default



================================================================================

# torch.set_default_device (Part 2)

get_default_device
device(type='cpu')
set_default_device  # current device is 0
get_default_device
device(type='cuda', index=0)
set_default_device
set_device  # current device is 1
get_default_device
device(type='cuda', index=1)
set_default_device
get_default_device
device(type='cuda', index=1)

Sets the default torch.Tensor to be allocated on .  This
does not affect factory function calls which are called with an explicit
 argument.  Factory calls will be performed as if they
were passed  as an argument.

To only temporarily change the default device instead of setting it
globally, use  torch.device(device): instead.

The default device is initially .  If you set the default tensor
device to another device (e.g., ) without a device index, tensors
will be allocated on whatever the current device for the device type,
even after torch.cuda.set_device() is called.

This function imposes a slight performance cost on every Python
call to the torch API (not just factory functions).  If this
is causing problems for you, please comment on
pytorch/pytorch#92701

This doesn’t affect functions that create tensors that share the same memory as the input, like:
torch.from_numpy() and torch.frombuffer()

================================================================================

# torch.set_default_device (Part 3)

Parameters
 () – the device to set as default

() – the device to set as default

Code example:
get_default_device
device(type='cpu')
set_default_device  # current device is 0
get_default_device
device(type='cuda', index=0)
set_default_device
set_device  # current device is 1
get_default_device
device(type='cuda', index=1)
set_default_device
get_default_device
device(type='cuda', index=1)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.get_default_device

get_default_device
Gets the default torch.Tensor to be allocated on 

Return type

Gets the default torch.Tensor to be allocated on

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.set_default_tensor_type (Part 1)

set_default_tensor_type, 


This function is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and
torch.set_default_device() as alternatives.

Sets the default torch.Tensor type to floating point tensor type
. This type will also be used as default floating point type for
type inference in torch.tensor().
The default floating point tensor type is initially torch.FloatTensor.

Parameters
 () – the floating point tensor type or its name



     # initial default for floating point is torch.float32
torch.float32
set_default_tensor_typeDoubleTensor
     # a new floating point tensor
torch.float64

This function is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and
torch.set_default_device() as alternatives.

Sets the default torch.Tensor type to floating point tensor type
. This type will also be used as default floating point type for
type inference in torch.tensor().

The default floating point tensor type is initially torch.FloatTensor.

Parameters
 () – the floating point tensor type or its name

() – the floating point tensor type or its name

================================================================================

# torch.set_default_tensor_type (Part 2)

Code example:
# initial default for floating point is torch.float32
torch.float32
set_default_tensor_typeDoubleTensor
     # a new floating point tensor
torch.float64

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.numel

Returns the total number of elements in the  tensor.

Parameters
 () – the input tensor.

Returns the total number of elements in the  tensor.

Parameters
 () – the input tensor.

() – the input tensor.

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.set_printoptions (Part 1)

set_printoptions, , , , , 
Set options for printing. Items shamelessly taken from NumPy

Parameters

 – Number of digits of precision for floating point output
(default = 4).
 – Total number of array elements which trigger summarization
rather than full  (default = 1000).
 – Number of array items in summary at beginning and end of
each dimension (default = 3).
 – The number of characters per line for the purpose of
inserting line breaks (default = 80). Thresholded matrices will
ignore this parameter.
 – Sane defaults for pretty printing. Can override with any of
the above options. (any one of , , )
 – Enable (True) or disable (False) scientific notation. If
None (default) is specified, the value is defined by
torch._tensor_str._Formatter. This value is automatically chosen
by the framework.




# Limit the precision of elements
set_printoptions

tensor([1.12])
# Limit the number of elements shown
set_printoptions

tensor([0, 1, 2, ..., 7, 8, 9])
# Restore defaults
set_printoptions

tensor([1.1235])

tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Set options for printing. Items shamelessly taken from NumPy

Parameters

================================================================================

# torch.set_printoptions (Part 2)

 – Number of digits of precision for floating point output
(default = 4).
 – Total number of array elements which trigger summarization
rather than full  (default = 1000).
 – Number of array items in summary at beginning and end of
each dimension (default = 3).
 – The number of characters per line for the purpose of
inserting line breaks (default = 80). Thresholded matrices will
ignore this parameter.
 – Sane defaults for pretty printing. Can override with any of
the above options. (any one of , , )
 – Enable (True) or disable (False) scientific notation. If
None (default) is specified, the value is defined by
torch._tensor_str._Formatter. This value is automatically chosen
by the framework.

================================================================================

# torch.set_printoptions (Part 3)

List:
– Number of digits of precision for floating point output
(default = 4).
 – Total number of array elements which trigger summarization
rather than full  (default = 1000).
 – Number of array items in summary at beginning and end of
each dimension (default = 3).
 – The number of characters per line for the purpose of
inserting line breaks (default = 80). Thresholded matrices will
ignore this parameter.
 – Sane defaults for pretty printing. Can override with any of
the above options. (any one of , , )
 – Enable (True) or disable (False) scientific notation. If
None (default) is specified, the value is defined by
torch._tensor_str._Formatter. This value is automatically chosen
by the framework.

– Number of digits of precision for floating point output
(default = 4).

– Total number of array elements which trigger summarization
rather than full  (default = 1000).

– Number of array items in summary at beginning and end of
each dimension (default = 3).

– The number of characters per line for the purpose of
inserting line breaks (default = 80). Thresholded matrices will
ignore this parameter.

– Sane defaults for pretty printing. Can override with any of
the above options. (any one of , , )

================================================================================

# torch.set_printoptions (Part 4)

– Enable (True) or disable (False) scientific notation. If
None (default) is specified, the value is defined by
torch._tensor_str._Formatter. This value is automatically chosen
by the framework.

Code example:
# Limit the precision of elements
set_printoptions

tensor([1.12])
# Limit the number of elements shown
set_printoptions

tensor([0, 1, 2, ..., 7, 8, 9])
# Restore defaults
set_printoptions

tensor([1.1235])

tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.set_flush_denormal

set_flush_denormal  
Disables denormal floating numbers on CPU.
Returns  if your system supports flushing denormal numbers and it
successfully configures flush denormal mode.  set_flush_denormal()
is supported on x86 architectures supporting SSE3 and AArch64 architecture.

Parameters
 () – Controls whether to enable flush denormal mode or not



set_flush_denormal

 
tensor([ 0.], dtype=torch.float64)
set_flush_denormal

 
tensor(9.88131e-324 *
       [ 1.0000], dtype=torch.float64)

Disables denormal floating numbers on CPU.

Returns  if your system supports flushing denormal numbers and it
successfully configures flush denormal mode.  set_flush_denormal()
is supported on x86 architectures supporting SSE3 and AArch64 architecture.

Parameters
 () – Controls whether to enable flush denormal mode or not

() – Controls whether to enable flush denormal mode or not

Code example:
set_flush_denormal

 
tensor([ 0.], dtype=torch.float64)
set_flush_denormal

 
tensor(9.88131e-324 *
       [ 1.0000], dtype=torch.float64)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.tensor (Part 1)

, , , , requires_grad, pin_memory  
Constructs a tensor with no autograd history (also known as a “leaf tensor”, see Autograd mechanics) by copying .


When working with tensors prefer using torch.Tensor.clone(),
torch.Tensor.detach(), and torch.Tensor.requires_grad_() for
readability. Letting  be a tensor, torch.tensor(t) is equivalent to
t.detach().clone(), and torch.tensor(t, requires_grad=True)
is equivalent to t.detach().clone().requires_grad_(True).



torch.as_tensor() preserves autograd history and avoids copies where possible.
torch.from_numpy() creates a tensor that shares storage with a NumPy array.


Parameters
 (array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.

Keyword Arguments

================================================================================

# torch.tensor (Part 2)

 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .
 (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .




     
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

   # Type inference on data
tensor([ 0,  1])

  
             
               # creates a double tensor on a CUDA device
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

  # Create a zero-dimensional (scalar) tensor
tensor(3.1416)

  # Create an empty tensor (of size (0,))
tensor([])

Constructs a tensor with no autograd history (also known as a “leaf tensor”, see Autograd mechanics) by copying .

================================================================================

# torch.tensor (Part 3)

When working with tensors prefer using torch.Tensor.clone(),
torch.Tensor.detach(), and torch.Tensor.requires_grad_() for
readability. Letting  be a tensor, torch.tensor(t) is equivalent to
t.detach().clone(), and torch.tensor(t, requires_grad=True)
is equivalent to t.detach().clone().requires_grad_(True).

torch.as_tensor() preserves autograd history and avoids copies where possible.
torch.from_numpy() creates a tensor that shares storage with a NumPy array.

Parameters
 (array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.

Keyword Arguments

 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .
 (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

================================================================================

# torch.tensor (Part 4)

(array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.

List:
(torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .
 (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

(torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .

(torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.

requires_grad () – If autograd should record operations on the
returned tensor. Default: .

pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

================================================================================

# torch.tensor (Part 5)

Code example:
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

   # Type inference on data
tensor([ 0,  1])

  
             
               # creates a double tensor on a CUDA device
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

  # Create a zero-dimensional (scalar) tensor
tensor(3.1416)

  # Create an empty tensor (of size (0,))
tensor([])

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.sparse_coo_tensor (Part 1)

sparse_coo_tensor, , , , , , pin_memory, requires_grad, check_invariants, is_coalesced  
Constructs a sparse tensor in COO(rdinate) format with specified values at the given
.


This function returns an uncoalesced tensor when is_coalesced is
unspecified or .



If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.


Parameters

 (array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types. Will be cast to a torch.LongTensor
internally. The indices are the coordinates of the non-zero values in the matrix, and thus
should be two-dimensional where the first dimension is the number of tensor dimensions and
the second dimension is the number of non-zero values.
 (array_like) – Initial values for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.
 (list, tuple, or torch.Size, optional) – Size of the sparse tensor. If not
provided the size will be inferred as the minimum size big enough to hold all non-zero
elements.


Keyword Arguments

================================================================================

# torch.sparse_coo_tensor (Part 2)

 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, infers data type from .
 (torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(see torch.set_default_device()).  will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.
is_coalesced () – When``True``, the caller is
responsible for providing tensor indices that correspond to a
coalesced tensor.  If the check_invariants flag is
False, no error will be raised if the prerequisites are not
met and this will lead to silently incorrect results. To force
coalescion please use coalesce() on the resulting
Tensor.
Default: None: except for trivial cases (e.g. nnz < 2) the
resulting Tensor has is_coalesced set to .



================================================================================

# torch.sparse_coo_tensor (Part 3)


    
                    
     
sparse_coo_tensor   
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)

sparse_coo_tensor   # Shape inference
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)

sparse_coo_tensor   
                        
                        
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,
       layout=torch.sparse_coo)

# Create an empty sparse tensor with the following invariants:
#   1. sparse_dim + dense_dim = len(SparseTensor.shape)
#   2. SparseTensor._indices().shape = (sparse_dim, nnz)
#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])

================================================================================

# torch.sparse_coo_tensor (Part 4)

# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
  sparse_coo_tensor   
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0,)),
       size=(1,), nnz=0, layout=torch.sparse_coo)

# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
# sparse_dim = 1
  sparse_coo_tensor     
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0, 2)),
       size=(1, 2), nnz=0, layout=torch.sparse_coo)

Constructs a sparse tensor in COO(rdinate) format with specified values at the given
.

This function returns an uncoalesced tensor when is_coalesced is
unspecified or .

If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.

Parameters

================================================================================

# torch.sparse_coo_tensor (Part 5)

 (array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types. Will be cast to a torch.LongTensor
internally. The indices are the coordinates of the non-zero values in the matrix, and thus
should be two-dimensional where the first dimension is the number of tensor dimensions and
the second dimension is the number of non-zero values.
 (array_like) – Initial values for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.
 (list, tuple, or torch.Size, optional) – Size of the sparse tensor. If not
provided the size will be inferred as the minimum size big enough to hold all non-zero
elements.


Keyword Arguments

================================================================================

# torch.sparse_coo_tensor (Part 6)

 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, infers data type from .
 (torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(see torch.set_default_device()).  will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.
is_coalesced () – When``True``, the caller is
responsible for providing tensor indices that correspond to a
coalesced tensor.  If the check_invariants flag is
False, no error will be raised if the prerequisites are not
met and this will lead to silently incorrect results. To force
coalescion please use coalesce() on the resulting
Tensor.
Default: None: except for trivial cases (e.g. nnz < 2) the
resulting Tensor has is_coalesced set to .

================================================================================

# torch.sparse_coo_tensor (Part 7)

List:
(array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types. Will be cast to a torch.LongTensor
internally. The indices are the coordinates of the non-zero values in the matrix, and thus
should be two-dimensional where the first dimension is the number of tensor dimensions and
the second dimension is the number of non-zero values.
 (array_like) – Initial values for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.
 (list, tuple, or torch.Size, optional) – Size of the sparse tensor. If not
provided the size will be inferred as the minimum size big enough to hold all non-zero
elements.

(array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types. Will be cast to a torch.LongTensor
internally. The indices are the coordinates of the non-zero values in the matrix, and thus
should be two-dimensional where the first dimension is the number of tensor dimensions and
the second dimension is the number of non-zero values.

(array_like) – Initial values for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.

================================================================================

# torch.sparse_coo_tensor (Part 8)

(list, tuple, or torch.Size, optional) – Size of the sparse tensor. If not
provided the size will be inferred as the minimum size big enough to hold all non-zero
elements.

================================================================================

# torch.sparse_coo_tensor (Part 9)

List:
(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, infers data type from .
 (torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(see torch.set_default_device()).  will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.
is_coalesced () – When``True``, the caller is
responsible for providing tensor indices that correspond to a
coalesced tensor.  If the check_invariants flag is
False, no error will be raised if the prerequisites are not
met and this will lead to silently incorrect results. To force
coalescion please use coalesce() on the resulting
Tensor.
Default: None: except for trivial cases (e.g. nnz < 2) the
resulting Tensor has is_coalesced set to .

================================================================================

# torch.sparse_coo_tensor (Part 10)

(torch.dtype, optional) – the desired data type of returned tensor.
Default: if None, infers data type from .

(torch.device, optional) – the desired device of returned tensor.
Default: if None, uses the current device for the default tensor type
(see torch.set_default_device()).  will be the CPU
for CPU tensor types and the current CUDA device for CUDA tensor types.

pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

requires_grad () – If autograd should record operations on the
returned tensor. Default: .

check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

is_coalesced () – When``True``, the caller is
responsible for providing tensor indices that correspond to a
coalesced tensor.  If the check_invariants flag is
False, no error will be raised if the prerequisites are not
met and this will lead to silently incorrect results. To force
coalescion please use coalesce() on the resulting
Tensor.
Default: None: except for trivial cases (e.g. nnz < 2) the
resulting Tensor has is_coalesced set to .

================================================================================

# torch.sparse_coo_tensor (Part 11)

Code example:
sparse_coo_tensor   
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 4), nnz=3, layout=torch.sparse_coo)

sparse_coo_tensor   # Shape inference
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       size=(2, 3), nnz=3, layout=torch.sparse_coo)

sparse_coo_tensor   
                        
                        
tensor(indices=tensor([[0, 1, 1],
                       [2, 0, 2]]),
       values=tensor([3., 4., 5.]),
       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,
       layout=torch.sparse_coo)

# Create an empty sparse tensor with the following invariants:
#   1. sparse_dim + dense_dim = len(SparseTensor.shape)
#   2. SparseTensor._indices().shape = (sparse_dim, nnz)
#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])

================================================================================

# torch.sparse_coo_tensor (Part 12)

# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and
# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))
  sparse_coo_tensor   
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0,)),
       size=(1,), nnz=0, layout=torch.sparse_coo)

# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and
# sparse_dim = 1
  sparse_coo_tensor     
tensor(indices=tensor([], size=(1, 0)),
       values=tensor([], size=(0, 2)),
       size=(1, 2), nnz=0, layout=torch.sparse_coo)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.sparse_csr_tensor (Part 1)

sparse_csr_tensorcrow_indices, col_indices, , , , , , pin_memory, requires_grad, check_invariants  
Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified
values at the given crow_indices and col_indices. Sparse matrix multiplication operations
in CSR format are typically faster than that for sparse tensors in COO format. Make you have a look
at the note on the data type of the indices.


If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.


Parameters

================================================================================

# torch.sparse_csr_tensor (Part 2)

crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and col_indices depending on where the given row
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
row.
col_indices (array_like) – Column co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length
as values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.


Keyword Arguments

================================================================================

# torch.sparse_csr_tensor (Part 3)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.




crow_indices    
col_indices     
     
sparse_csr_tensorcrow_indices 
                        col_indices 
                         
tensor(crow_indices=tensor([0, 2, 4]),
       col_indices=tensor([0, 1, 0, 1]),
       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
       dtype=torch.float64, layout=torch.sparse_csr)

================================================================================

# torch.sparse_csr_tensor (Part 4)

Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified
values at the given crow_indices and col_indices. Sparse matrix multiplication operations
in CSR format are typically faster than that for sparse tensors in COO format. Make you have a look
at the note on the data type of the indices.

If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.

Parameters

================================================================================

# torch.sparse_csr_tensor (Part 5)

crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and col_indices depending on where the given row
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
row.
col_indices (array_like) – Column co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length
as values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.


Keyword Arguments

================================================================================

# torch.sparse_csr_tensor (Part 6)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

================================================================================

# torch.sparse_csr_tensor (Part 7)

List:
crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and col_indices depending on where the given row
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
row.
col_indices (array_like) – Column co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length
as values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.

================================================================================

# torch.sparse_csr_tensor (Part 8)

crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and col_indices depending on where the given row
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
row.

col_indices (array_like) – Column co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length
as values.

(array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.

(list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.

================================================================================

# torch.sparse_csr_tensor (Part 9)

List:
(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.

(torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.

================================================================================

# torch.sparse_csr_tensor (Part 10)

pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

requires_grad () – If autograd should record operations on the
returned tensor. Default: .

check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

Code example:
crow_indices    
col_indices     
     
sparse_csr_tensorcrow_indices 
                        col_indices 
                         
tensor(crow_indices=tensor([0, 2, 4]),
       col_indices=tensor([0, 1, 0, 1]),
       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
       dtype=torch.float64, layout=torch.sparse_csr)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.sparse_csc_tensor (Part 1)

sparse_csc_tensorccol_indices, row_indices, , , , , , pin_memory, requires_grad, check_invariants  
Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given
ccol_indices and row_indices. Sparse matrix
multiplication operations in CSC format are typically faster than that
for sparse tensors in COO format. Make you have a look at the
note on the data type of the indices.


If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.


Parameters

================================================================================

# torch.sparse_csc_tensor (Part 2)

ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and row_indices depending on where the given column
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
column.
row_indices (array_like) – Row co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length as
values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.


Keyword Arguments

================================================================================

# torch.sparse_csc_tensor (Part 3)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.




ccol_indices    
row_indices     
     
sparse_csc_tensorccol_indices 
                        row_indices 
                         
tensor(ccol_indices=tensor([0, 2, 4]),
       row_indices=tensor([0, 1, 0, 1]),
       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
       dtype=torch.float64, layout=torch.sparse_csc)

================================================================================

# torch.sparse_csc_tensor (Part 4)

Constructs a sparse tensor in CSC (Compressed Sparse Column) with specified values at the given
ccol_indices and row_indices. Sparse matrix
multiplication operations in CSC format are typically faster than that
for sparse tensors in COO format. Make you have a look at the
note on the data type of the indices.

If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.

Parameters

================================================================================

# torch.sparse_csc_tensor (Part 5)

ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and row_indices depending on where the given column
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
column.
row_indices (array_like) – Row co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length as
values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.


Keyword Arguments

================================================================================

# torch.sparse_csc_tensor (Part 6)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

================================================================================

# torch.sparse_csc_tensor (Part 7)

List:
ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and row_indices depending on where the given column
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
column.
row_indices (array_like) – Row co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length as
values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.

================================================================================

# torch.sparse_csc_tensor (Part 8)

ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize,   .  The last element of each batch
is the number of non-zeros. This tensor encodes the index in
values and row_indices depending on where the given column
starts. Each successive number in the tensor subtracted by the
number before it denotes the number of elements in a given
column.

row_indices (array_like) – Row co-ordinates of each element in
values. (B+1)-dimensional tensor with the same length as
values.

(array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1+K)-dimensional tensor where  is the number
of dense dimensions.

(list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   *densesize). If
not provided, the size will be inferred as the minimum size
big enough to hold all non-zero elements.

================================================================================

# torch.sparse_csc_tensor (Part 9)

List:
(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.

(torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.

================================================================================

# torch.sparse_csc_tensor (Part 10)

pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

requires_grad () – If autograd should record operations on the
returned tensor. Default: .

check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

Code example:
ccol_indices    
row_indices     
     
sparse_csc_tensorccol_indices 
                        row_indices 
                         
tensor(ccol_indices=tensor([0, 2, 4]),
       row_indices=tensor([0, 1, 0, 1]),
       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
       dtype=torch.float64, layout=torch.sparse_csc)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.sparse_bsr_tensor (Part 1)

sparse_bsr_tensorcrow_indices, col_indices, , , , , , pin_memory, requires_grad, check_invariants  
Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given
crow_indices and col_indices. Sparse matrix
multiplication operations in BSR format are typically faster than that
for sparse tensors in COO format. Make you have a look at the
note on the data type of the indices.


If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.


Parameters

================================================================================

# torch.sparse_bsr_tensor (Part 2)

crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, nrowblocks  .  The last element of each
batch is the number of non-zeros. This tensor encodes the
block index in values and col_indices depending on where the
given row block starts. Each successive number in the tensor
subtracted by the number before it denotes the number of
blocks in a given row.
col_indices (array_like) – Column block co-ordinates of each block
in values. (B+1)-dimensional tensor with the same length as
values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) where  
values.shape[1:3]. If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.


Keyword Arguments

================================================================================

# torch.sparse_bsr_tensor (Part 3)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.




crow_indices    
col_indices   
         
sparse_bsr_tensorcrow_indices 
                        col_indices 
                         
tensor(crow_indices=tensor([0, 1, 2]),
       col_indices=tensor([0, 1]),
       values=tensor([[[1., 2.],
                       [3., 4.]],
                      [[5., 6.],
                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,
       layout=torch.sparse_bsr)

================================================================================

# torch.sparse_bsr_tensor (Part 4)

Constructs a sparse tensor in BSR (Block Compressed Sparse Row)) with specified 2-dimensional blocks at the given
crow_indices and col_indices. Sparse matrix
multiplication operations in BSR format are typically faster than that
for sparse tensors in COO format. Make you have a look at the
note on the data type of the indices.

If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.

Parameters

================================================================================

# torch.sparse_bsr_tensor (Part 5)

crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, nrowblocks  .  The last element of each
batch is the number of non-zeros. This tensor encodes the
block index in values and col_indices depending on where the
given row block starts. Each successive number in the tensor
subtracted by the number before it denotes the number of
blocks in a given row.
col_indices (array_like) – Column block co-ordinates of each block
in values. (B+1)-dimensional tensor with the same length as
values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) where  
values.shape[1:3]. If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.


Keyword Arguments

================================================================================

# torch.sparse_bsr_tensor (Part 6)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

================================================================================

# torch.sparse_bsr_tensor (Part 7)

List:
crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, nrowblocks  .  The last element of each
batch is the number of non-zeros. This tensor encodes the
block index in values and col_indices depending on where the
given row block starts. Each successive number in the tensor
subtracted by the number before it denotes the number of
blocks in a given row.
col_indices (array_like) – Column block co-ordinates of each block
in values. (B+1)-dimensional tensor with the same length as
values.
 (array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) where  
values.shape[1:3]. If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.

================================================================================

# torch.sparse_bsr_tensor (Part 8)

crow_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, nrowblocks  .  The last element of each
batch is the number of non-zeros. This tensor encodes the
block index in values and col_indices depending on where the
given row block starts. Each successive number in the tensor
subtracted by the number before it denotes the number of
blocks in a given row.

col_indices (array_like) – Column block co-ordinates of each block
in values. (B+1)-dimensional tensor with the same length as
values.

(array_list) – Initial values for the tensor. Can be a list,
tuple, NumPy , scalar, and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.

(list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) where  
values.shape[1:3]. If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.

================================================================================

# torch.sparse_bsr_tensor (Part 9)

List:
(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.

(torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.

================================================================================

# torch.sparse_bsr_tensor (Part 10)

pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

requires_grad () – If autograd should record operations on the
returned tensor. Default: .

check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

Code example:
crow_indices    
col_indices   
         
sparse_bsr_tensorcrow_indices 
                        col_indices 
                         
tensor(crow_indices=tensor([0, 1, 2]),
       col_indices=tensor([0, 1]),
       values=tensor([[[1., 2.],
                       [3., 4.]],
                      [[5., 6.],
                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,
       layout=torch.sparse_bsr)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.sparse_bsc_tensor (Part 1)

sparse_bsc_tensorccol_indices, row_indices, , , , , , pin_memory, requires_grad, check_invariants  
Constructs a sparse tensor in BSC (Block Compressed Sparse
Column)) with specified 2-dimensional blocks at the
given ccol_indices and row_indices. Sparse matrix
multiplication operations in BSC format are typically faster than that
for sparse tensors in COO format. Make you have a look at the
note on the data type of the indices.


If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.


Parameters

================================================================================

# torch.sparse_bsc_tensor (Part 2)

ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, ncolblocks  . The last element of each
batch is the number of non-zeros. This tensor encodes the
index in values and row_indices depending on where the given
column starts. Each successive number in the tensor subtracted
by the number before it denotes the number of elements in a
given column.
row_indices (array_like) – Row block co-ordinates of each block in
values. (B+1)-dimensional tensor with the same length
as values.
 (array_list) – Initial blocks for the tensor. Can be a list,
tuple, NumPy , and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.


Keyword Arguments

================================================================================

# torch.sparse_bsc_tensor (Part 3)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.




ccol_indices    
row_indices   
         
sparse_bsc_tensorccol_indices 
                        row_indices 
                         
tensor(ccol_indices=tensor([0, 1, 2]),
       row_indices=tensor([0, 1]),
       values=tensor([[[1., 2.],
                       [3., 4.]],
                      [[5., 6.],
                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,
       layout=torch.sparse_bsc)

================================================================================

# torch.sparse_bsc_tensor (Part 4)

Constructs a sparse tensor in BSC (Block Compressed Sparse
Column)) with specified 2-dimensional blocks at the
given ccol_indices and row_indices. Sparse matrix
multiplication operations in BSC format are typically faster than that
for sparse tensors in COO format. Make you have a look at the
note on the data type of the indices.

If the  argument is not specified the device of the given
 and indices tensor(s) must match. If, however, the
argument is specified the input Tensors will be converted to the
given device and in turn determine the device of the constructed
sparse tensor.

Parameters

================================================================================

# torch.sparse_bsc_tensor (Part 5)

ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, ncolblocks  . The last element of each
batch is the number of non-zeros. This tensor encodes the
index in values and row_indices depending on where the given
column starts. Each successive number in the tensor subtracted
by the number before it denotes the number of elements in a
given column.
row_indices (array_like) – Row block co-ordinates of each block in
values. (B+1)-dimensional tensor with the same length
as values.
 (array_list) – Initial blocks for the tensor. Can be a list,
tuple, NumPy , and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.


Keyword Arguments

================================================================================

# torch.sparse_bsc_tensor (Part 6)

 (torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

================================================================================

# torch.sparse_bsc_tensor (Part 7)

List:
ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, ncolblocks  . The last element of each
batch is the number of non-zeros. This tensor encodes the
index in values and row_indices depending on where the given
column starts. Each successive number in the tensor subtracted
by the number before it denotes the number of elements in a
given column.
row_indices (array_like) – Row block co-ordinates of each block in
values. (B+1)-dimensional tensor with the same length
as values.
 (array_list) – Initial blocks for the tensor. Can be a list,
tuple, NumPy , and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.
 (list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.

================================================================================

# torch.sparse_bsc_tensor (Part 8)

ccol_indices (array_like) – (B+1)-dimensional array of size
(*batchsize, ncolblocks  . The last element of each
batch is the number of non-zeros. This tensor encodes the
index in values and row_indices depending on where the given
column starts. Each successive number in the tensor subtracted
by the number before it denotes the number of elements in a
given column.

row_indices (array_like) – Row block co-ordinates of each block in
values. (B+1)-dimensional tensor with the same length
as values.

(array_list) – Initial blocks for the tensor. Can be a list,
tuple, NumPy , and other types that
represents a (1 + 2 + K)-dimensional tensor where  is the
number of dense dimensions.

(list, tuple, torch.Size, optional) – Size of the
sparse tensor: (*batchsize,   blocksize[0],  
blocksize[1], *densesize) If not provided, the size will be
inferred as the minimum size big enough to hold all non-zero
blocks.

================================================================================

# torch.sparse_bsc_tensor (Part 9)

List:
(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.
 (torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.
pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .
requires_grad () – If autograd should record operations on the
returned tensor. Default: .
check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

(torch.dtype, optional) – the desired data type of
returned tensor.  Default: if None, infers data type from
.

(torch.device, optional) – the desired device of
returned tensor.  Default: if None, uses the current device
for the default tensor type (see
torch.set_default_device()).  will be
the CPU for CPU tensor types and the current CUDA device for
CUDA tensor types.

================================================================================

# torch.sparse_bsc_tensor (Part 10)

pin_memory () – If set, returned tensor would be allocated in
the pinned memory. Works only for CPU tensors. Default: .

requires_grad () – If autograd should record operations on the
returned tensor. Default: .

check_invariants () – If sparse tensor invariants are checked.
Default: as returned by torch.sparse.check_sparse_tensor_invariants.is_enabled(),
initially False.

Code example:
ccol_indices    
row_indices   
         
sparse_bsc_tensorccol_indices 
                        row_indices 
                         
tensor(ccol_indices=tensor([0, 1, 2]),
       row_indices=tensor([0, 1]),
       values=tensor([[[1., 2.],
                       [3., 4.]],
                      [[5., 6.],
                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,
       layout=torch.sparse_bsc)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.asarray (Part 1)

, , , DeviceLikeType, , requires_grad  
Converts  to a tensor.
 can be one of:


a NumPy array or a NumPy scalar
a DLPack capsule
an object that implements Python’s buffer protocol

a sequence of scalars

================================================================================

# torch.asarray (Part 2)

When  is a tensor, NumPy array, or DLPack capsule the returned tensor will,
by default, not require a gradient, have the same datatype as , be on the
same device, and share memory with it. These properties can be controlled with the
, , , and requires_grad keyword arguments.
If the returned tensor is of a different datatype, on a different device, or a copy is
requested then it will not share its memory with . If requires_grad
is  then the returned tensor will require a gradient, and if  is
also a tensor with an autograd history then the returned tensor will have the same history.
When  is not a tensor, NumPy array, or DLPack capsule but implements Python’s
buffer protocol then the buffer is interpreted as an array of bytes grouped according to
the size of the datatype passed to the  keyword argument. (If no datatype is
passed then the default floating point datatype is used, instead.) The returned tensor
will have the specified datatype (or default floating point datatype if none is specified)
and, by default, be on the CPU device and share memory with the buffer.
When  is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on
the CPU and that doesn’t share its memory (i.e. ). By default datatype will
be the PyTorch datatype corresponding to the NumPy’s scalar’s datatype.
When  is none of the above but a scalar, or a sequence of scalars then the
returned tensor will, by default, infer its datatype from the scalar values, be on the
current default device, and not share its memory.

================================================================================

# torch.asarray (Part 3)


torch.tensor() creates a tensor that always copies the data from the input object.
torch.from_numpy() creates a tensor that always shares memory from NumPy arrays.
torch.frombuffer() creates a tensor that always shares memory from objects that
implement the buffer protocol.
torch.from_dlpack() creates a tensor that always shares memory from
DLPack capsules.


Parameters
 () – a tensor, NumPy array, DLPack Capsule, object that implements Python’s
buffer protocol, scalar, or sequence of scalars.

Keyword Arguments

================================================================================

# torch.asarray (Part 4)

 (torch.dtype, optional) – the datatype of the returned tensor.
Default: , which causes the datatype of the returned tensor to be
inferred from .
 () – controls whether the returned tensor shares memory with .
Default: , which causes the returned tensor to share memory with 
whenever possible. If  then the returned tensor does not share its memory.
If  then the returned tensor shares its memory with  and an
error is thrown if it cannot.
 (torch.device, optional) – the device of the returned tensor.
Default: , which causes the device of  to be used. Or, if
 is a Python sequence, the current default device will be used.
requires_grad () – whether the returned tensor requires grad.
Default: , which causes the returned tensor not to require a gradient.
If , then the returned tensor will require a gradient, and if 
is also a tensor with an autograd history then the returned tensor will have
the same history.




    
# Shares memory with tensor 'a'
  
  

# Forces memory copy
   
  


     requires_grad
    

tensor([3., 4., 5.], grad_fn=<AddBackward0>)
# Shares memory with tensor 'b', with no grad
  

tensor([3., 4., 5.])
# Shares memory with tensor 'b', retaining autograd history
   requires_grad

================================================================================

# torch.asarray (Part 5)

tensor([3., 4., 5.], grad_fn=<AddBackward0>)

    
# Shares memory with array 'array'
  
__array_interface__  

# Copies memory due to dtype mismatch
   
__array_interface__  


  

tensor(0.5000, dtype=torch.float64)

Converts  to a tensor.

List:
a NumPy array or a NumPy scalar
a DLPack capsule
an object that implements Python’s buffer protocol

a sequence of scalars

a NumPy array or a NumPy scalar

an object that implements Python’s buffer protocol

a sequence of scalars

When  is a tensor, NumPy array, or DLPack capsule the returned tensor will,
by default, not require a gradient, have the same datatype as , be on the
same device, and share memory with it. These properties can be controlled with the
, , , and requires_grad keyword arguments.
If the returned tensor is of a different datatype, on a different device, or a copy is
requested then it will not share its memory with . If requires_grad
is  then the returned tensor will require a gradient, and if  is
also a tensor with an autograd history then the returned tensor will have the same history.

================================================================================

# torch.asarray (Part 6)

When  is not a tensor, NumPy array, or DLPack capsule but implements Python’s
buffer protocol then the buffer is interpreted as an array of bytes grouped according to
the size of the datatype passed to the  keyword argument. (If no datatype is
passed then the default floating point datatype is used, instead.) The returned tensor
will have the specified datatype (or default floating point datatype if none is specified)
and, by default, be on the CPU device and share memory with the buffer.

When  is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on
the CPU and that doesn’t share its memory (i.e. ). By default datatype will
be the PyTorch datatype corresponding to the NumPy’s scalar’s datatype.

When  is none of the above but a scalar, or a sequence of scalars then the
returned tensor will, by default, infer its datatype from the scalar values, be on the
current default device, and not share its memory.

================================================================================

# torch.asarray (Part 7)

torch.tensor() creates a tensor that always copies the data from the input object.
torch.from_numpy() creates a tensor that always shares memory from NumPy arrays.
torch.frombuffer() creates a tensor that always shares memory from objects that
implement the buffer protocol.
torch.from_dlpack() creates a tensor that always shares memory from
DLPack capsules.

Parameters
 () – a tensor, NumPy array, DLPack Capsule, object that implements Python’s
buffer protocol, scalar, or sequence of scalars.

Keyword Arguments

================================================================================

# torch.asarray (Part 8)

 (torch.dtype, optional) – the datatype of the returned tensor.
Default: , which causes the datatype of the returned tensor to be
inferred from .
 () – controls whether the returned tensor shares memory with .
Default: , which causes the returned tensor to share memory with 
whenever possible. If  then the returned tensor does not share its memory.
If  then the returned tensor shares its memory with  and an
error is thrown if it cannot.
 (torch.device, optional) – the device of the returned tensor.
Default: , which causes the device of  to be used. Or, if
 is a Python sequence, the current default device will be used.
requires_grad () – whether the returned tensor requires grad.
Default: , which causes the returned tensor not to require a gradient.
If , then the returned tensor will require a gradient, and if 
is also a tensor with an autograd history then the returned tensor will have
the same history.

() – a tensor, NumPy array, DLPack Capsule, object that implements Python’s
buffer protocol, scalar, or sequence of scalars.

================================================================================

# torch.asarray (Part 9)

List:
(torch.dtype, optional) – the datatype of the returned tensor.
Default: , which causes the datatype of the returned tensor to be
inferred from .
 () – controls whether the returned tensor shares memory with .
Default: , which causes the returned tensor to share memory with 
whenever possible. If  then the returned tensor does not share its memory.
If  then the returned tensor shares its memory with  and an
error is thrown if it cannot.
 (torch.device, optional) – the device of the returned tensor.
Default: , which causes the device of  to be used. Or, if
 is a Python sequence, the current default device will be used.
requires_grad () – whether the returned tensor requires grad.
Default: , which causes the returned tensor not to require a gradient.
If , then the returned tensor will require a gradient, and if 
is also a tensor with an autograd history then the returned tensor will have
the same history.

(torch.dtype, optional) – the datatype of the returned tensor.
Default: , which causes the datatype of the returned tensor to be
inferred from .

================================================================================

# torch.asarray (Part 10)

() – controls whether the returned tensor shares memory with .
Default: , which causes the returned tensor to share memory with 
whenever possible. If  then the returned tensor does not share its memory.
If  then the returned tensor shares its memory with  and an
error is thrown if it cannot.

(torch.device, optional) – the device of the returned tensor.
Default: , which causes the device of  to be used. Or, if
 is a Python sequence, the current default device will be used.

requires_grad () – whether the returned tensor requires grad.
Default: , which causes the returned tensor not to require a gradient.
If , then the returned tensor will require a gradient, and if 
is also a tensor with an autograd history then the returned tensor will have
the same history.

Code example:
# Shares memory with tensor 'a'
  
  

# Forces memory copy
   
  


     requires_grad
    

tensor([3., 4., 5.], grad_fn=<AddBackward0>)
# Shares memory with tensor 'b', with no grad
  

tensor([3., 4., 5.])
# Shares memory with tensor 'b', retaining autograd history
   requires_grad

tensor([3., 4., 5.], grad_fn=<AddBackward0>)

    
# Shares memory with array 'array'
  
__array_interface__  

================================================================================

# torch.asarray (Part 11)

# Copies memory due to dtype mismatch
   
__array_interface__  


  

tensor(0.5000, dtype=torch.float64)

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.as_tensor (Part 1)

Optional[dtype]    Optional[DeviceLikeType]  
Converts  into a tensor, sharing data and preserving autograd
history if possible.
If  is already a tensor with the requested dtype and device
then  itself is returned, but if  is a
tensor with a different dtype or device then it’s copied as if using
data.to(dtype=dtype, device=device).
If  is a NumPy array (an ndarray) with the same dtype and device then a
tensor is constructed using torch.from_numpy().
If  is a CuPy array, the returned tensor will be located on the same device as the CuPy array unless
specifically overwritten by  or a default device.


torch.tensor() never shares its data and creates a new “leaf tensor” (see Autograd mechanics).


Parameters

 (array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.
 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .
 (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.




    
  

tensor([ 1,  2,  3])
  

array([-1,  2,  3])

    
   

================================================================================

# torch.as_tensor (Part 2)

tensor([ 1,  2,  3])
  

array([1,  2,  3])

Converts  into a tensor, sharing data and preserving autograd
history if possible.

If  is already a tensor with the requested dtype and device
then  itself is returned, but if  is a
tensor with a different dtype or device then it’s copied as if using
data.to(dtype=dtype, device=device).

If  is a NumPy array (an ndarray) with the same dtype and device then a
tensor is constructed using torch.from_numpy().

If  is a CuPy array, the returned tensor will be located on the same device as the CuPy array unless
specifically overwritten by  or a default device.

torch.tensor() never shares its data and creates a new “leaf tensor” (see Autograd mechanics).

Parameters

 (array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.
 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .
 (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.

================================================================================

# torch.as_tensor (Part 3)

List:
(array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.
 (torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .
 (torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.

(array_like) – Initial data for the tensor. Can be a list, tuple,
NumPy , scalar, and other types.

(torch.dtype, optional) – the desired data type of returned tensor.
Default: if , infers data type from .

(torch.device, optional) – the device of the constructed tensor. If None and data is a tensor
then the device of data is used. If None and data is not a tensor then
the result tensor is constructed on the current device.

Code example:
tensor([ 1,  2,  3])
  

array([-1,  2,  3])

    
   

tensor([ 1,  2,  3])
  

array([1,  2,  3])

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

# torch.as_strided (Part 1)

as_strided, , , storage_offset  
Create a view of an existing torch.Tensor  with specified
,  and storage_offset.


Prefer using other view functions, like torch.Tensor.view() or
torch.Tensor.expand(), to setting a view’s strides manually with
as_strided, as this function will throw an error on non-standard Pytorch
backends (that do not have a concept of stride) and the result will depend
on the current layout in memory. The constructed view must only refer to
elements within the Tensor’s storage or a runtime error will be thrown.
If the generated view is “overlapped” (with multiple indices referring to
the same element in memory), the behavior of inplace operations on this view
is undefined (and might not throw runtime errors).


Parameters

 () – the input tensor.
 () – the shape of the output tensor
 () – the stride of the output tensor
storage_offset () – the offset in the underlying storage of the output tensor.
If , the storage_offset of the output tensor will match the input tensor.




   

tensor([[ 0.9039,  0.6291,  1.0795],
        [ 0.1586,  2.1939, -0.4900],
        [-0.1909, -0.7503,  1.9355]])
  as_strided    

================================================================================

# torch.as_strided (Part 2)

tensor([[0.9039, 1.0795],
        [0.6291, 0.1586]])
  as_strided     
tensor([[0.6291, 0.1586],
        [1.0795, 2.1939]])

Create a view of an existing torch.Tensor  with specified
,  and storage_offset.

Prefer using other view functions, like torch.Tensor.view() or
torch.Tensor.expand(), to setting a view’s strides manually with
as_strided, as this function will throw an error on non-standard Pytorch
backends (that do not have a concept of stride) and the result will depend
on the current layout in memory. The constructed view must only refer to
elements within the Tensor’s storage or a runtime error will be thrown.
If the generated view is “overlapped” (with multiple indices referring to
the same element in memory), the behavior of inplace operations on this view
is undefined (and might not throw runtime errors).

Parameters

 () – the input tensor.
 () – the shape of the output tensor
 () – the stride of the output tensor
storage_offset () – the offset in the underlying storage of the output tensor.
If , the storage_offset of the output tensor will match the input tensor.

================================================================================

# torch.as_strided (Part 3)

List:
() – the input tensor.
 () – the shape of the output tensor
 () – the stride of the output tensor
storage_offset () – the offset in the underlying storage of the output tensor.
If , the storage_offset of the output tensor will match the input tensor.

() – the input tensor.

() – the shape of the output tensor

() – the stride of the output tensor

storage_offset () – the offset in the underlying storage of the output tensor.
If , the storage_offset of the output tensor will match the input tensor.

Code example:
tensor([[ 0.9039,  0.6291,  1.0795],
        [ 0.1586,  2.1939, -0.4900],
        [-0.1909, -0.7503,  1.9355]])
  as_strided    

tensor([[0.9039, 1.0795],
        [0.6291, 0.1586]])
  as_strided     
tensor([[0.6291, 0.1586],
        [1.0795, 2.1939]])

List:
TorchCodec
torchvision
ExecuTorch
PyTorch on XLA Devices

================================================================================

