# Async Web Scraper API

High-performance async web scraper optimized for speed and efficiency.

## Class: AsyncWebScraper

Asynchronous web scraper with concurrent processing, smart caching, and performance optimization.

### Constructor

```python
AsyncWebScraper(config: Optional[ScrapingConfig] = None)
```

**Parameters:**
- `config` (ScrapingConfig, optional): Configuration object. Uses defaults if None.

### Configuration Classes

#### ScrapingConfig

```python
@dataclass
class ScrapingConfig:
    concurrent_limit: int = 4          # Max concurrent requests
    requests_per_second: float = 6.0   # Rate limiting
    timeout: float = 15.0              # Request timeout
    max_pages: int = 30                # Maximum pages to scrape
    max_depth: int = 2                 # Maximum crawling depth
    same_domain_only: bool = True      # Stay within domain
    respect_robots_txt: bool = True    # Honor robots.txt
    user_agent: str = "..."            # User agent string
    retry_attempts: int = 3            # Retry failed requests
    retry_delay: float = 1.0           # Delay between retries
```

#### PerformanceMetrics

```python
@dataclass
class PerformanceMetrics:
    start_time: float
    urls_discovered: int = 0
    urls_processed: int = 0
    urls_failed: int = 0
    total_requests: int = 0
    cache_hits: int = 0
    total_bytes_downloaded: int = 0
```

### Methods

#### scrape_website()

```python
async scrape_website(
    start_urls: List[str],
    output_file: str = "data/website_docs.json"
) -> Tuple[bool, PerformanceMetrics]
```

Main async scraping method with performance optimization.

**Parameters:**
- `start_urls` (List[str]): List of starting URLs
- `output_file` (str, optional): Output file path

**Returns:**
- `Tuple[bool, PerformanceMetrics]`: Success status and performance metrics

**Example:**
```python
import asyncio
from src.async_web_scraper import AsyncWebScraper, ScrapingConfig

async def main():
    config = ScrapingConfig(
        concurrent_limit=6,
        max_pages=50,
        requests_per_second=8.0
    )

    scraper = AsyncWebScraper(config)
    success, metrics = await scraper.scrape_website([
        "https://docs.python.org/3/"
    ])

    print(f"Processed {metrics.urls_processed} pages")
    print(f"Cache hits: {metrics.cache_hits}")

asyncio.run(main())
```

#### scrape_page()

```python
async scrape_page(
    session: aiohttp.ClientSession,
    url: str
) -> Optional[Dict[str, Any]]
```

Scrape a single page with error handling and retries.

**Parameters:**
- `session` (aiohttp.ClientSession): HTTP session
- `url` (str): URL to scrape

**Returns:**
- `Optional[Dict[str, Any]]`: Page data or None if failed

#### extract_content()

```python
def extract_content(self, soup: BeautifulSoup, url: str) -> List[Dict[str, Any]]
```

Extract structured content from HTML.

**Parameters:**
- `soup` (BeautifulSoup): Parsed HTML
- `url` (str): Source URL

**Returns:**
- `List[Dict[str, Any]]`: List of content chunks with metadata

#### discover_urls()

```python
def discover_urls(self, soup: BeautifulSoup, base_url: str, current_depth: int) -> Set[str]
```

Discover new URLs from page content.

**Parameters:**
- `soup` (BeautifulSoup): Parsed HTML
- `base_url` (str): Base URL for relative links
- `current_depth` (int): Current crawling depth

**Returns:**
- `Set[str]`: Set of discovered URLs

### Performance Features

#### Smart Caching
- Content-based cache keys using SHA-256 hashes
- Automatic cache invalidation for dynamic content
- Memory-efficient storage of processed data

#### Rate Limiting
- Configurable requests per second
- Distributed across concurrent workers
- Adaptive delays based on server response

#### Concurrent Processing
- Configurable concurrency limits
- Session pooling for connection reuse
- Efficient resource management

#### Error Handling
- Automatic retries with exponential backoff
- Graceful degradation on network errors
- Comprehensive error logging

### Example Usage

#### Basic Async Scraping
```python
import asyncio
from src.async_web_scraper import AsyncWebScraper

async def scrape_docs():
    scraper = AsyncWebScraper()
    success, metrics = await scraper.scrape_website([
        "https://fastapi.tiangolo.com/"
    ])

    if success:
        print(f"âœ… Scraped {metrics.urls_processed} pages")
        print(f"ðŸ“Š Total bytes: {metrics.total_bytes_downloaded:,}")
        print(f"âš¡ Cache hits: {metrics.cache_hits}")

    return success

# Run the scraper
success = asyncio.run(scrape_docs())
```

#### High-Performance Configuration
```python
from src.async_web_scraper import AsyncWebScraper, ScrapingConfig

# Configure for maximum performance
config = ScrapingConfig(
    concurrent_limit=8,           # More concurrent requests
    requests_per_second=10.0,     # Higher rate limit
    max_pages=100,                # More pages
    timeout=20.0,                 # Longer timeout
    retry_attempts=5              # More retries
)

scraper = AsyncWebScraper(config)
success, metrics = asyncio.run(
    scraper.scrape_website(["https://docs.python.org/"])
)
```

#### Integration with RAG System
```python
from src.rag_system import RAGSystem
from src.async_web_scraper import AsyncWebScraper, ScrapingConfig

async def fast_rag_setup():
    # Fast scraping with async scraper
    config = ScrapingConfig(concurrent_limit=6, max_pages=50)
    scraper = AsyncWebScraper(config)

    success, metrics = await scraper.scrape_website([
        "https://docs.python.org/3/"
    ], "data/python_docs.json")

    if success:
        # Load into RAG system
        rag = RAGSystem()
        rag.load_data("data/python_docs.json")

        # Ready for queries
        result = rag.demo_query("Python functions", top_k=3)
        return result

result = asyncio.run(fast_rag_setup())
```

### Performance Benchmarks

Typical performance improvements over synchronous scraping:

- **Speed**: 3-5x faster with concurrent processing
- **Efficiency**: 60% reduction in total time for large sites
- **Cache hit rate**: 40-60% for repeated scraping
- **Memory usage**: Optimized for large-scale processing

### Best Practices

1. **Concurrency**: Start with 4-6 concurrent requests
2. **Rate limiting**: Respect server limits (6-10 requests/second)
3. **Timeouts**: Use 15-20 second timeouts for reliability
4. **Retries**: Configure 3-5 retry attempts for robustness
5. **Caching**: Enable caching for repeated scraping
6. **Monitoring**: Check performance metrics for optimization