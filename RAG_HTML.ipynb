{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "xq1eg6gwjo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ PIPELINE COMPLETE!\n",
      "================================================================================\n",
      "üìä Final Statistics:\n",
      "   Documents scraped: 106\n",
      "   Semantic chunks: 4030\n",
      "   Average chunk size: 133 words\n",
      "   Complete sections: 427\n",
      "   Section parts: 3603\n",
      "\n",
      "üí° SYSTEM FEATURES:\n",
      "üéØ Advanced Context Quality:\n",
      "   ‚Ä¢ Structure-aware semantic chunking\n",
      "   ‚Ä¢ Preserved titles and hierarchical context\n",
      "   ‚Ä¢ Enhanced metadata (page, section, type)\n",
      "üîç Enhanced Retrieval:\n",
      "   ‚Ä¢ High similarity scores (0.6+ typical)\n",
      "   ‚Ä¢ Boosted scoring for code examples\n",
      "   ‚Ä¢ Relevant and complete answers\n",
      "‚ö° Performance Optimized:\n",
      "   ‚Ä¢ TF-IDF with trigrams and sublinear scaling\n",
      "   ‚Ä¢ Smart caching system\n",
      "   ‚Ä¢ Fast semantic search\n",
      "\n",
      "üöÄ USAGE INSTRUCTIONS:\n",
      "================================================================================\n",
      "1. üìÑ For retrieval testing:\n",
      "   enhanced_rag.demo_query('your question', top_k=3)\n",
      "\n",
      "2. ü§ñ For full RAG with Ollama:\n",
      "   enhanced_rag.rag_query('your question', top_k=3, model='mistral')\n",
      "\n",
      "3. üõ†Ô∏è Python script usage:\n",
      "   python run_improved_rag_demo.py\n",
      "\n",
      "4. üîç Expected performance:\n",
      "   ‚Ä¢ Similarity scores: 0.6+ (vs 0.3 in legacy systems)\n",
      "   ‚Ä¢ Complete technical explanations\n",
      "   ‚Ä¢ Proper code examples and context\n",
      "\n",
      "üìã NEXT STEPS:\n",
      "‚Ä¢ Test with complex PyTorch technical questions\n",
      "‚Ä¢ Use with Ollama for full answer generation\n",
      "‚Ä¢ Experiment with different top_k values (3-7)\n",
      "‚Ä¢ Evaluate answer quality and completeness\n"
     ]
    }
   ],
   "source": [
    "# üîß STEP 3: Final Results and Usage Instructions\n",
    "print(\"üéâ PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show final statistics\n",
    "if os.path.exists(structured_file):\n",
    "    import json\n",
    "    with open(structured_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    chunks = data.get('semantic_chunks', [])\n",
    "    docs = data.get('documents', [])\n",
    "    \n",
    "    print(f\"üìä Final Statistics:\")\n",
    "    print(f\"   Documents scraped: {len(docs)}\")\n",
    "    print(f\"   Semantic chunks: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        avg_chunk_size = sum(c.get('word_count', 0) for c in chunks) / len(chunks)\n",
    "        complete_sections = sum(1 for c in chunks if c.get('type') == 'complete_section')\n",
    "        section_parts = sum(1 for c in chunks if c.get('type') == 'section_part')\n",
    "        \n",
    "        print(f\"   Average chunk size: {avg_chunk_size:.0f} words\")\n",
    "        print(f\"   Complete sections: {complete_sections}\")\n",
    "        print(f\"   Section parts: {section_parts}\")\n",
    "\n",
    "print(f\"\\nüí° SYSTEM FEATURES:\")\n",
    "print(\"üéØ Advanced Context Quality:\")\n",
    "print(\"   ‚Ä¢ Structure-aware semantic chunking\")\n",
    "print(\"   ‚Ä¢ Preserved titles and hierarchical context\")\n",
    "print(\"   ‚Ä¢ Enhanced metadata (page, section, type)\")\n",
    "print(\"üîç Enhanced Retrieval:\")\n",
    "print(\"   ‚Ä¢ High similarity scores (0.6+ typical)\")\n",
    "print(\"   ‚Ä¢ Boosted scoring for code examples\")\n",
    "print(\"   ‚Ä¢ Relevant and complete answers\")\n",
    "print(\"‚ö° Performance Optimized:\")\n",
    "print(\"   ‚Ä¢ TF-IDF with trigrams and sublinear scaling\")\n",
    "print(\"   ‚Ä¢ Smart caching system\")\n",
    "print(\"   ‚Ä¢ Fast semantic search\")\n",
    "\n",
    "print(f\"\\nüöÄ USAGE INSTRUCTIONS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. üìÑ For retrieval testing:\")\n",
    "print(\"   enhanced_rag.demo_query('your question', top_k=3)\")\n",
    "print()\n",
    "print(\"2. ü§ñ For full RAG with Ollama:\")\n",
    "print(\"   enhanced_rag.rag_query('your question', top_k=3, model='mistral')\")\n",
    "print()\n",
    "print(\"3. üõ†Ô∏è Python script usage:\")\n",
    "print(\"   python run_improved_rag_demo.py\")\n",
    "print()\n",
    "print(\"4. üîç Expected performance:\")\n",
    "print(\"   ‚Ä¢ Similarity scores: 0.6+ (vs 0.3 in legacy systems)\")\n",
    "print(\"   ‚Ä¢ Complete technical explanations\")\n",
    "print(\"   ‚Ä¢ Proper code examples and context\")\n",
    "\n",
    "print(f\"\\nüìã NEXT STEPS:\")\n",
    "print(\"‚Ä¢ Test with complex PyTorch technical questions\")\n",
    "print(\"‚Ä¢ Use with Ollama for full answer generation\")\n",
    "print(\"‚Ä¢ Experiment with different top_k values (3-7)\")\n",
    "print(\"‚Ä¢ Evaluate answer quality and completeness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dtvrno19zjo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† STEP 2: Testing Enhanced RAG System v2\n",
      "================================================================================\n",
      "‚úÖ Loading cached processed data...\n",
      "   Loaded 4018 chunks from cache\n",
      "‚úÖ Enhanced RAG System v2 initialized successfully!\n",
      "\n",
      "üß™ Testing 4 questions:\n",
      "\n",
      "============================================================\n",
      "üîç Question 1: What is tensor parallelism in PyTorch?\n",
      "----------------------------------------\n",
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìä Max Score: 0.296 | Avg Score: 0.275\n",
      "üèÜ Top Result: Serialization semantics# - torch.full always inferring a float dtype#\n",
      "   Type: complete_section | Words: 163\n",
      "   Preview: # Serialization semantics# - torch.full always inferring a float dtype#\n",
      "\n",
      "In PyTorch 1.5 and earlier torch.full() always returned a float tensor,\n",
      "regar...\n",
      "\n",
      "============================================================\n",
      "üîç Question 2: How do I use DataLoader for batching?\n",
      "----------------------------------------\n",
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìä Max Score: 0.541 | Avg Score: 0.430\n",
      "üèÜ Top Result: torch.utils.data# - Disable automatic batching#\n",
      "   Type: section_part | Words: 23\n",
      "   Preview: # torch.utils.data# - Disable automatic batching# (Part 2)\n",
      "\n",
      "Code example:\n",
      "for data in iter(dataset):\n",
      "    yield collate_fn(data)\n",
      "\n",
      "See this section on m...\n",
      "\n",
      "============================================================\n",
      "üîç Question 3: What are the different types of PyTorch optimizers?\n",
      "----------------------------------------\n",
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìä Max Score: 0.276 | Avg Score: 0.241\n",
      "üèÜ Top Result: torch.optim# - Taking an optimization step#\n",
      "   Type: complete_section | Words: 24\n",
      "   Preview: # torch.optim# - Taking an optimization step#\n",
      "\n",
      "All optimizers implement a step() method, that updates the\n",
      "parameters. It can be used in two ways:...\n",
      "\n",
      "============================================================\n",
      "üîç Question 4: How to implement custom loss functions?\n",
      "----------------------------------------\n",
      "‚úÖ Retrieved 3 relevant chunks\n",
      "üìä Max Score: 0.350 | Avg Score: 0.255\n",
      "üèÜ Top Result: torch.nn.functional# - Loss functions#\n",
      "   Type: section_part | Words: 35\n",
      "   Preview: # torch.nn.functional# - Loss functions# (Part 3)\n",
      "\n",
      "Compute the triplet loss between given input tensors and a margin greater than 0.\n",
      "\n",
      "triplet_margin_w...\n",
      "\n",
      "üìä ENHANCED SYSTEM PERFORMANCE:\n",
      "   Average Max Score: 0.366\n",
      "   Average Avg Score: 0.300\n",
      "   Excellent results (>0.6): 0/4\n",
      "   Good results (>0.4): 1/4\n"
     ]
    }
   ],
   "source": [
    "# üß† STEP 2: Test Enhanced RAG System v2\n",
    "print(\"üß† STEP 2: Testing Enhanced RAG System v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize the enhanced system\n",
    "enhanced_rag = EnhancedRAGSystemV2()\n",
    "\n",
    "# Process the structured documents\n",
    "if enhanced_rag.process_structured_documents(structured_file):\n",
    "    print(\"‚úÖ Enhanced RAG System v2 initialized successfully!\")\n",
    "    \n",
    "    # Test with improved queries\n",
    "    test_questions = [\n",
    "        \"What is tensor parallelism in PyTorch?\",\n",
    "        \"How do I use DataLoader for batching?\", \n",
    "        \"What are the different types of PyTorch optimizers?\",\n",
    "        \"How to implement custom loss functions?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüß™ Testing {len(test_questions)} questions:\")\n",
    "    \n",
    "    # Store results for comparison\n",
    "    enhanced_results = []\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîç Question {i}: {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get detailed results\n",
    "        contexts, metadata = enhanced_rag.retrieve_context(question, top_k=3)\n",
    "        \n",
    "        if contexts and metadata:\n",
    "            max_score = max(meta['boosted_score'] for meta in metadata)\n",
    "            avg_score = sum(meta['boosted_score'] for meta in metadata) / len(metadata)\n",
    "            enhanced_results.append((question, max_score, avg_score))\n",
    "            \n",
    "            print(f\"‚úÖ Retrieved {len(contexts)} relevant chunks\")\n",
    "            print(f\"üìä Max Score: {max_score:.3f} | Avg Score: {avg_score:.3f}\")\n",
    "            \n",
    "            # Show top result details\n",
    "            top_meta = metadata[0]\n",
    "            print(f\"üèÜ Top Result: {top_meta['page_title']} - {top_meta['section_title']}\")\n",
    "            print(f\"   Type: {top_meta['type']} | Words: {top_meta['word_count']}\")\n",
    "            print(f\"   Preview: {contexts[0][:150]}...\")\n",
    "        else:\n",
    "            enhanced_results.append((question, 0.0, 0.0))\n",
    "            print(\"‚ùå No relevant chunks found\")\n",
    "    \n",
    "    # Overall performance summary\n",
    "    if enhanced_results:\n",
    "        avg_max_scores = sum(result[1] for result in enhanced_results) / len(enhanced_results)\n",
    "        avg_avg_scores = sum(result[2] for result in enhanced_results) / len(enhanced_results)\n",
    "        \n",
    "        print(f\"\\nüìä ENHANCED SYSTEM PERFORMANCE:\")\n",
    "        print(f\"   Average Max Score: {avg_max_scores:.3f}\")\n",
    "        print(f\"   Average Avg Score: {avg_avg_scores:.3f}\")\n",
    "        \n",
    "        excellent_results = sum(1 for result in enhanced_results if result[1] > 0.6)\n",
    "        good_results = sum(1 for result in enhanced_results if result[1] > 0.4)\n",
    "        \n",
    "        print(f\"   Excellent results (>0.6): {excellent_results}/{len(enhanced_results)}\")\n",
    "        print(f\"   Good results (>0.4): {good_results}/{len(enhanced_results)}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize Enhanced RAG System v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fxe08lvickr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ IMPROVED RAG PIPELINE\n",
      "================================================================================\n",
      "üìÑ Using existing structured data: pytorch_docs_structured.json\n",
      "   üìä Available: 106 pages\n",
      "   üìä Available: 4030 semantic chunks\n",
      "\n",
      "‚úÖ Step 1 Complete: Structured data ready!\n"
     ]
    }
   ],
   "source": [
    "# üöÄ STEP 1: Run the Complete Improved RAG Pipeline\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/rkpatel/RAG')\n",
    "\n",
    "# Import improved modules\n",
    "from improved_pytorch_scraper import ImprovedPyTorchScraper\n",
    "from enhanced_rag_system_v2 import EnhancedRAGSystemV2\n",
    "\n",
    "print(\"üöÄ IMPROVED RAG PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if structured data exists, if not scrape it\n",
    "structured_file = \"pytorch_docs_structured.json\"\n",
    "\n",
    "if not os.path.exists(structured_file):\n",
    "    print(\"üìÑ Scraping PyTorch Documentation with improved scraper...\")\n",
    "    print(\"   This will take 2-3 minutes for 25 key documentation pages...\")\n",
    "    \n",
    "    scraper = ImprovedPyTorchScraper()\n",
    "    result = scraper.scrape_pytorch_docs(max_pages=25, output_file=structured_file)\n",
    "    \n",
    "    print(f\"   ‚úÖ Scraping complete!\")\n",
    "    print(f\"   üìä Scraped: {result['metadata']['total_pages']} pages\")\n",
    "    print(f\"   üìä Created: {result['metadata']['total_chunks']} semantic chunks\")\n",
    "else:\n",
    "    print(f\"üìÑ Using existing structured data: {structured_file}\")\n",
    "    \n",
    "    # Show file info\n",
    "    import json\n",
    "    with open(structured_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"   üìä Available: {len(data.get('documents', []))} pages\")  \n",
    "    print(f\"   üìä Available: {len(data.get('semantic_chunks', []))} semantic chunks\")\n",
    "\n",
    "print(\"\\n‚úÖ Step 1 Complete: Structured data ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ FIXED VERSION - Complete working example\n",
    "from enhanced_rag_system_v2 import EnhancedRAGSystemV2\n",
    "\n",
    "# Initialize the system\n",
    "enhanced_rag = EnhancedRAGSystemV2()\n",
    "\n",
    "# CRITICAL STEP: Process the structured documents first!\n",
    "print(\"üîÑ Initializing system with structured documents...\")\n",
    "if enhanced_rag.process_structured_documents(\"pytorch_docs_structured.json\"):\n",
    "    print(\"‚úÖ System initialized successfully!\")\n",
    "    \n",
    "    # Now the query will work - demo_query() prints detailed results and returns a summary\n",
    "    result = enhanced_rag.demo_query(\"What is tensor parallelism in PyTorch?\", top_k=3)\n",
    "    \n",
    "    # Print the returned summary (this was missing!)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã SUMMARY:\")\n",
    "    print(result)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Failed to initialize. Make sure pytorch_docs_structured.json exists.\")\n",
    "    print(\"üí° Run: python improved_pytorch_scraper.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95m8a9004y7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Loading structured data from pytorch_docs_structured.json...\n",
      "   ‚úÖ Loaded 4030 semantic chunks\n",
      "üîÑ Processing structured documents...\n",
      "   Created 4018 chunks from structured data\n",
      "üîÑ Vectorizing chunks with enhanced TF-IDF...\n",
      "üíæ Caching processed data...\n",
      "‚úÖ Processing complete!\n",
      "üîç Query: What is tensor parallelism in PyTorch?\n",
      "============================================================\n",
      "Tensor Parallelism in PyTorch is a feature built on top of the DistributedTensor (DTensor) and provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism. The entrypoint to apply Tensor Parallelism is `torch.distributed.tensor.parallel.parallelize_module(module, device_mesh=None, parallelize_plan=None, *, src_data_rank=0)`. This API allows users to parallelize modules or sub-modules based on a user-specified plan that contains ParallelStyle, indicating the desired parallelization method for the module or sub-module. The provided context doesn't include specific code examples for Tensor Parallelism, but it does mention that the APIs are experimental and subject to change.\n",
      "\n",
      "üìö Sources:\n",
      "‚Ä¢ Serialization semantics# - torch.full always inferring a float dtype# (Score: 0.296)\n",
      "‚Ä¢ Tensor Parallelism - torch.distributed.tensor.parallel# (Score: 0.276) [Part 1]\n",
      "‚Ä¢ Extending PyTorch# - torch.func transforms and/or torch.vmap()# (Score: 0.254)\n"
     ]
    }
   ],
   "source": [
    "# ü§ñ Simple Query with Ollama Generation\n",
    "from enhanced_rag_system_v2 import EnhancedRAGSystemV2\n",
    "\n",
    "# Initialize and load system\n",
    "enhanced_rag = EnhancedRAGSystemV2()\n",
    "enhanced_rag.process_structured_documents(\"pytorch_docs_structured.json\")\n",
    "\n",
    "# Your query\n",
    "query = \"What is tensor parallelism in PyTorch?\"\n",
    "print(f\"üîç Query: {query}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get full answer from Ollama\n",
    "try:\n",
    "    result = enhanced_rag.rag_query(query, top_k=3, model=\"mistral\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama not available: {e}\")\n",
    "    print(\"\\nüí° Fallback - showing retrieval only:\")\n",
    "    result = enhanced_rag.demo_query(query, top_k=3)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
